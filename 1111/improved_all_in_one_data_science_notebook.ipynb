{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03547ac7",
   "metadata": {},
   "source": [
    "# Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b1a7c0",
   "metadata": {},
   "source": [
    "## [Introduction](#Introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b95817e",
   "metadata": {},
   "source": [
    "## [Data Science Foundations](#Data-Science-Foundations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2bc90b",
   "metadata": {},
   "source": [
    "## [Data Processing and Manipulation](#Data-Processing-and-Manipulation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73001477",
   "metadata": {},
   "source": [
    "## [Exploratory Data Analysis (EDA)](#Exploratory-Data-Analysis-(EDA))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d3b752",
   "metadata": {},
   "source": [
    "## [Machine Learning](#Machine-Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fe97da",
   "metadata": {},
   "source": [
    "## [Deep Learning](#Deep-Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69e199c",
   "metadata": {},
   "source": [
    "## [Special Topics in Data Science](#Special-Topics-in-Data-Science)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2666e2",
   "metadata": {},
   "source": [
    "## [Best Practices and Advanced Techniques](#Best-Practices-and-Advanced-Techniques)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500f0247",
   "metadata": {},
   "source": [
    "## [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef558df",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "<a id='Introduction'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f2d8c8",
   "metadata": {},
   "source": [
    "# Data Science Foundations\n",
    "<a id='Data-Science-Foundations'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5471ff27",
   "metadata": {},
   "source": [
    "## Markdown\n",
    "#### Hello python\n",
    "\n",
    "\n",
    "$y = 3x + 2$\n",
    "\n",
    "```python\n",
    "a = 10\n",
    "print(a)\n",
    "```\n",
    "\n",
    "> Quote \"Python is fun\"\n",
    "\n",
    "[파이썬 공식 페이지](https://www.python.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6775cf",
   "metadata": {},
   "source": [
    "## While, For 반복문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219441ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'korea': 'seoul', 'japan': 'tokyo', 'canada': 'ottawa'}\n",
    "for k in a:\n",
    "    print(k, a[k])\n",
    "    \n",
    "for value in a.values():\n",
    "    print(value)\n",
    "\n",
    "for a, b in a.items():\n",
    "    print(a, b)\n",
    "\n",
    "# itmes()는 튜플을 원소로 갖는 리스트를 반환\n",
    "# 튜플인 경우 \"a, b\"처럼 두 개 이상의 변수를 한 번에 받아올 수 있음\n",
    "\n",
    "a = [1, 2, 4, 3, 5]\n",
    "for i, val in enumerate(a):\n",
    "    if i > 3:\n",
    "        print(i, val)\n",
    "        \n",
    "list(enumerate(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e4fbb2",
   "metadata": {},
   "source": [
    "### if & for 연습문제\n",
    "   \n",
    "  1. 구구단을 2 - 9단까지 출력하시오.\n",
    "  2. 1 - 100까지 정수 중 2의 배수 또는 11의 배수를 모두 출력하시오. \n",
    "  4. a = [22, 1, 3, 4, 7, 98, 21, 55, 87, 99, 19, 20, 45] 에서 최대값과 최소값을 찾으시오. (sorted, sort 사용 금지)\n",
    "  5. a = [22, 1, 3, 4, 7, 98, 21, 55, 87, 99, 19, 20, 45] 에서 평균을 구하세요.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab70b417",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "for i in a:\n",
    "    print(\"<\", i, \"단입니다 >\")\n",
    "    for j in a:\n",
    "        print(i, 'x', j, '=', i*j)\n",
    "    print(\"\")\n",
    "\n",
    "x = 2\n",
    "while x <= 9:\n",
    "    y = 1\n",
    "    while y <= 9:\n",
    "        print(x, 'x', y, '=', x*y)\n",
    "        y += 1\n",
    "    x += 1\n",
    "\n",
    "print(list(range(2,101,2)))\n",
    "print(list(range(11,101,11)))\n",
    "\n",
    "nums = list(range(1,101))\n",
    "for x in nums:\n",
    "    if x % 2 ==0 or x % 11 == 0:\n",
    "        print(x)\n",
    "\n",
    "# sort 사용할 경우\n",
    "a = [22, 1, 3, 4, 7, 98, 21, 55, 87, 99, 19, 20, 45]\n",
    "\n",
    "a.sort()\n",
    "a[0], a[-1]\n",
    "\n",
    "a = [22, 1, 3, 4, 7, 98, 21, 55, 87, 99, 19, 20, 45]\n",
    "\n",
    "_min = a[0]\n",
    "_max = a[0]\n",
    "\n",
    "for x in a:    # for x in a[1:] 이라고 해도 됌\n",
    "    if x < _min:\n",
    "        _min = x\n",
    "    if x > _max:\n",
    "        _max = x\n",
    "print(_min, _max)\n",
    "\n",
    "a = [22, 1, 3, 4, 7, 98, 21, 55, 87, 99, 19, 20, 45]\n",
    "\n",
    "_sum = 0\n",
    "for x in a:\n",
    "    _sum += x\n",
    "\n",
    "print(_sum / len(a))\n",
    "\n",
    "a = [22, 1, 3, 4, 7, 98, 21, 55, 87, 99, 19, 20, 45]\n",
    "\n",
    "i = 0\n",
    "_sum = 0\n",
    "while i < len(a):\n",
    "    _sum += a[i]\n",
    "    i += 1\n",
    "    \n",
    "print(_sum / len(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3effa5c",
   "metadata": {},
   "source": [
    "## 함수 기본 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0414c10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(x, y=10, z=5):\n",
    "    a = x + y + z\n",
    "    return a\n",
    "\n",
    "d = add(10, 20, 30)\n",
    "print(d)\n",
    "\n",
    "# def add(x=10, y=20, z) 는 불가능함!!\n",
    "# 기본 파라미터 뒤에 일반 파라미터가 올 수 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b436d5a",
   "metadata": {},
   "source": [
    "#### **return (리턴)**\n",
    " + 기본적으로 함수의 종료를 명시\n",
    "   + return 옆에 값이나 수식이 있다면 해당 값을 호출자(caller)에게 반환(전달)\n",
    "   + return 만 존재하면 None 반환\n",
    "   + return이 없는 경우, 기본적으로 함수 코드 블록이 종료되면 종료로 간주. 이때도 None 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1299be43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weird_multiply(x, y):\n",
    "    if x > 10:\n",
    "        return\n",
    "    print(x + y)\n",
    "    return (x + 2) * y\n",
    "\n",
    "c = weird_multiply(12, 5)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd0d7cb",
   "metadata": {},
   "source": [
    "#### multiple return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d78f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_mul(x, y):\n",
    "    s = x + y\n",
    "    m = x * y\n",
    "    \n",
    "    return s, m\n",
    "\n",
    "c = add_mul(20, 3)\n",
    "print(type(c))\n",
    "print(c)\n",
    "\n",
    "a, b = add_mul(20, 3)\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441f17ee",
   "metadata": {},
   "source": [
    "#### **variable scope (변수의 범위)** \n",
    " + 변수가 참조 가능한 코드상의 범위를 명시\n",
    " + 함수내의 변수는 자신이 속한 코드 블록이 종료되면 소멸됨\n",
    " + 이렇게 특정 코드 블록에서 선언된 변수를 **지역변수(local variable)** 이라고 함\n",
    " + 반대로 가장 상단에서 정의되어 프로그램 종료 전까지 유지되는 변수를 **전역변수(global variable)**이라고 함\n",
    " + 같은 이름의 지역변수와 전역변수가 존재할 경우, 지역변수의 우선순위가 더 높음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1229b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num1 = 10\n",
    "num2 = 30\n",
    "\n",
    "def test(num1, num2):\n",
    "    print(num1, num2)\n",
    "    return num1 + num2\n",
    "\n",
    "test(30, 40)\n",
    "print(num1, num2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fed2d2",
   "metadata": {},
   "source": [
    "#### **variable length argument (가변길이 인자)**\n",
    " - 전달되는 파라미터의 개수가 고정적이지 않은 경우 사용\n",
    " - e.g)\n",
    "   - print 함수\n",
    "   - format 함수\n",
    "  \n",
    "> ***args**,  ****kwargs**\n",
    "\n",
    "> ***args**    : 파라미터를 튜플의 형태로 전달\n",
    "\n",
    "> ****kwargs** : 파리미터를 딕셔너리 형태로 전달(네임드 파라미터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc39ea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(*x):\n",
    "    for item in x:\n",
    "        print(item)\n",
    "    \n",
    "test(10, 20, 'hello')\n",
    "\n",
    "## 가변길이 함수는 관례적으로 *args 라고 쓴다.\n",
    "## 파라미터가 튜플로 인식된다.\n",
    "\n",
    "def test2(**x):\n",
    "    for key, value in x.items():\n",
    "        print('key:', key, ', value:', value)\n",
    "    \n",
    "\n",
    "test2(a=1, b=2, c=9, d='hello')\n",
    "\n",
    "a = '오늘 온도: {today_temp}도, 강수확률은: {today_prob}%, 내일온도: {tomorrow_temp}도'.format(today_temp=10, today_prob=20, tomorrow_temp=30)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea546b42",
   "metadata": {},
   "source": [
    "## Lambda 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766c8f39",
   "metadata": {},
   "source": [
    "* **Lambda 함수**\n",
    " + 단일문으로 표현되는 익명함수\n",
    " + 익명함수란 이름이 없는 구현체만 존재하는 간단한 함수를 의미\n",
    " + 코드 상에서 한번만 사용되는 기능이 있을 때, 굳이 함수로 만들지 않고 1회성으로 만들어서 쓸 때 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551ffe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "square = lambda x:x**2\n",
    "type(square)\n",
    "\n",
    "square(5)\n",
    "\n",
    "strings = ['bob', 'charles', 'alexander', 'teddy']\n",
    "strings.sort(key=lambda s:len(s))\n",
    "    # key 파라미터에는 function이 들어와야 하는데,\n",
    "    # 여기다 하나 넣자고 def 쓰기는 번거롭다.\n",
    "\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9e9c24",
   "metadata": {},
   "source": [
    "#### **filter, map, reduce**\n",
    " + lambda가 유용하게 사용되는 3가지 대표적 함수\n",
    " + 함수형 프로그래밍의 기본 요소이기도 함\n",
    " + filter : 특정 조건을 만족하는 요소만 남기고 필터링\n",
    " + map    : 각 원소를 주어진 수식에 따라 변형하여 새로운 리스트를 반환\n",
    " + reduce : 차례대로 앞 2개의 원소를 가지고 연산. 연산의 결과가 또 다음 연산의 입력으로 진행됨. 따라서 마지막까지 진행되면 최종 출력은 한개의 값만 남게 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c595f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = [1, 2, 3, 4, 6, 8, 9]\n",
    "\n",
    "list(filter(lambda x:x%2==0, nums))\n",
    "\n",
    "# map은 주어진 리스트를 토대로 새로운 리스트를 반환하는 것\n",
    "\n",
    "nums = [1, 2, 3, 4, 5, 6, 7, 10, 11, 13]\n",
    "print(list(map(lambda n:n**2, nums)))\n",
    "\n",
    "print(list(map(lambda n:n%2==0, nums)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13923d9b",
   "metadata": {},
   "source": [
    "\n",
    "#### 함수 연습문제\n",
    " 1. 주어진 숫자 리스트의 평균을 구하는 함수를 출력하시오\n",
    " 1. 해당 숫자가 소수인지 아닌지 판별하시오.\n",
    " 2. 2부터 해당 숫자사이에 소수가 몇개인지 출력하는 함수를 구하시오 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed09dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(nums):\n",
    "    _sum = 0\n",
    "    for x in nums:\n",
    "        _sum += x\n",
    "        \n",
    "    return _sum / len(nums)\n",
    "\n",
    "print(mean([1,2,3,4,5,6,10,12]))\n",
    "\n",
    "# sum 내장함수 사용하면\n",
    "def mean(nums):\n",
    "    return sum(nums) / len(nums)\n",
    "\n",
    "print(mean([1,2,3,4,5,6,10,12]))\n",
    "\n",
    "def is_prime(num):   # true/false를 반환시키는 함수는 대부분 'is~~'라고 쓰인다.\n",
    "    for i in range(2, num):  # range가 num까지이면, num은 포함 안 되는 거임. 왜냐면.. range는 index 번호니까!\n",
    "        if num % i == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "print(is_prime(100))\n",
    "print(is_prime(89))\n",
    "print(is_prime(11))\n",
    "print(is_prime(4))\n",
    "\n",
    "def num_of_prime(num):\n",
    "    count = 0\n",
    "    for i in range(2,num+1):\n",
    "        if is_prime(i):\n",
    "            count += 1\n",
    "    \n",
    "    return count\n",
    "\n",
    "print(num_of_prime(7))\n",
    "print(num_of_prime(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a8220d",
   "metadata": {},
   "source": [
    "## 클래스와 인스턴스"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b37fb44",
   "metadata": {},
   "source": [
    "#### **class란?**\n",
    " + 실세계의 것을 모델링하여 속성(attribute)와 동작(method)를 갖는 데이터 타입\n",
    " + python에서의 string, int, list, dict.. 모두가 다 클래스로 존재\n",
    " + 예를들어 학생이라는 클래스를 만든다면, 학생을 나타내는 속성과 학생이 행하는 행동을 함께 정의 할 수 있음\n",
    " + 따라서, 다루고자 하는 데이터(변수) 와 데이터를 다루는 연산(함수)를 하나로 캡슐화(encapsulation)하여 클래스로 표현\n",
    " + 모델링에서 중요시 하는 속성에 따라 클래스의 속성과 행동이 각각 달라짐\n",
    " \n",
    "#### **object 란?**\n",
    " - 클래스로 생성되어 구체화된 객체(인스턴스)\n",
    " - 파이썬의 모든 것(int, str, list..etc)은 객체(인스턴스)\n",
    " - 실제로 class가 인스턴스화 되어 메모리에 상주하는 상태를 의미\n",
    " - class가 빵틀이라면, object는 실제로 빵틀로 찍어낸 빵이라고 비유 가능\n",
    "\n",
    "#### **class 선언하기**\n",
    "  - 객체를 생성하기 위해선 객체의 모체가 되는 class를 미리 선언해야 함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c405d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person:\n",
    "    pass    # pass를 하면 오류를 내지 않고 '빈 클래스를 생성 가능'\n",
    "\n",
    "bob = Person()\n",
    "cathy = Person()\n",
    "\n",
    "a = list()\n",
    "b = list()\n",
    "\n",
    "print(type(bob), type(cathy))\n",
    "print(type(a), type(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99a997d",
   "metadata": {},
   "source": [
    "#### __init__(self)\n",
    " + 생성자, 클래스 인스턴스가 생성될 때 호출됨\n",
    " + self인자는 항상 첫번째에 오며 자기 자신을 가리킴\n",
    " + 이름이 꼭 self일 필요는 없지만, 관례적으로 self로 사용\n",
    " \n",
    " + 생성자에서는 해당 클래스가 다루는 데이터를 정의\n",
    "   - 이 데이터를 멤버 변수(member variable) 또는 속성(attribute)라고 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc25e7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person:\n",
    "    def __init__(self):\n",
    "        print(self, 'is generated')\n",
    "        self.name = 'Kate'\n",
    "        self.age = 10\n",
    "        \n",
    "p1 = Person()\n",
    "p2 = Person()\n",
    "\n",
    "print(p1.name, p1.age)\n",
    "\n",
    "p2.name = 'aaron'\n",
    "p2.age = 20\n",
    "\n",
    "print(p2.name, p2.age)\n",
    "\n",
    "# Person을 생성할 때 파라미터를 전달할 수 있도록\n",
    "\n",
    "class Person:\n",
    "    def __init__(self, name, age=10):\n",
    "        print(self, 'is generated')\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "        \n",
    "p1 = Person('bob', 30)\n",
    "p2 = Person('kate', 20)\n",
    "p3 = Person('aaron')\n",
    "\n",
    "print(p1.name, p1.age)\n",
    "print(p2.name, p2.age)\n",
    "print(p3.name, p3.age)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da74d86",
   "metadata": {},
   "source": [
    "### self 의 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbd5cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person:\n",
    "    def __init__(self, name, age):\n",
    "        print(self, 'is generated')\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "    \n",
    "    def sleep(self):\n",
    "        print(self.name, '은 잠을 잡니다.')\n",
    "\n",
    "a = Person('Aaron', 20)\n",
    "\n",
    "a.sleep()   # self는 'a'이다. => 여기서 a.sleep(a)인 거랑 같은 말이다.\n",
    "            # 'a'라는 변수 자체는 \"어떤 메모리 주소에 저장된 값\", 혹은 \"어떤 주소\" 그 자체다.\n",
    "\n",
    "## SELF 는 관례적으로 모든 METHOD 파라미터의 첫 번째 자리에 온다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982f3403",
   "metadata": {},
   "source": [
    "#### **mehtod 정의**\n",
    " + 멤버함수라고도 하며, 해당 클래스의 object에서만 호출가능\n",
    " + 메쏘드는 객체 레벨에서 호출되며, 해당 객체의 속성에 대한 연산을 행함\n",
    " + {obj}.{method}() 형태로 호출됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9d6d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 숫자를 하나 증가시킴\n",
    "# 2. 숫자를 0으로 초기화\n",
    "\n",
    "class Counter:\n",
    "    def __init__(self):\n",
    "        self.num = 0\n",
    "        \n",
    "    def increment(self):\n",
    "        self.num += 1\n",
    "        \n",
    "    def reset(self):\n",
    "        self.num = 0\n",
    "        \n",
    "    def print_current_value(self):\n",
    "        print('현재 값은: ', self.num)\n",
    "\n",
    "\n",
    "c1 = Counter()\n",
    "c1.print_current_value()\n",
    "c1.increment()\n",
    "c1.increment()\n",
    "c1.print_current_value()\n",
    "\n",
    "c1.reset()\n",
    "c1.print_current_value()\n",
    "\n",
    "c2 = Counter()\n",
    "c2.increment()\n",
    "c2.print_current_value()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a074d02",
   "metadata": {},
   "source": [
    "### class method - class로 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92887744",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Math:\n",
    "    @staticmethod\n",
    "    def add(a, b):\n",
    "        return a + b\n",
    "    \n",
    "    @staticmethod\n",
    "    def multiply(a, b):\n",
    "        return a * b\n",
    "    \n",
    "Math.add(10, 20)\n",
    "Math.multiply(10, 20)\n",
    "\n",
    "## 만약 위의 내용이 instance method였다면..\n",
    "class Math:\n",
    "    def add(self, a, b):\n",
    "        return a + b\n",
    "    \n",
    "    def multiply(self, a, b):\n",
    "        return a * b\n",
    "\n",
    "m = Math()\n",
    "m.add(10, 20)\n",
    "m.multiply(10, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a8deea",
   "metadata": {},
   "source": [
    "#### **Class Inheritance (상속)**\n",
    "  - 기존에 정의해둔 클래스의 기능을 그대로 물려받을 수 있다.\n",
    "  - 기존 클래스에 기능 일부를 추가하거나, 변경하여 새로운 클래스를 정의한다.\n",
    "  - 코드를 재사용할 수 있게된다.\n",
    "  - 상속 받고자 하는 대상인 기존 클래스는 (Parent, Super, Base class 라고 부른다.)\n",
    "  - 상속 받는 새로운 클래스는(Child, Sub, Derived class 라고 부른다.)\n",
    "  - 의미적으로 is-a관계를 갖는다\n",
    "  \n",
    "#### **method override**\n",
    " - 부모 클래스의 method를 재정의(override)\n",
    " - 하위 클래스(자식 클래스) 의 인스턴스로 호출시, 재정의된 메소드가 호출됨\n",
    " \n",
    "#### super \n",
    " - 하위클래스(자식 클래스)에서 부모클래스의 method를 호출할 때 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564a7060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student is-a person ; Employee is-a person\n",
    "\n",
    "class Person:\n",
    "    def __init__(self, name, age):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "        \n",
    "    def eat(self, food):\n",
    "        print('{}은 {}를 먹습니다.'.format(self.name, food))\n",
    "        \n",
    "    def sleep(self, minute):\n",
    "        print('{}은 {}분 동안 잡니다.'.format(self.name, minute))\n",
    "\n",
    "    def work(self, minute):\n",
    "        print('{}은 {}분 동안 준비합니다.'.format(self.name, minute))\n",
    "        \n",
    "\n",
    "class Student(Person):\n",
    "    def __init__(self, name, age):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "        \n",
    "    def work(self, minute):\n",
    "        super().work(minute)   # 부모 class의 기능을 이용\n",
    "        print('{}은 {}분 동안 공부합니다.'.format(self.name, minute))\n",
    "        \n",
    "class Employee(Person):\n",
    "    def __init__(self, name, age):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "        \n",
    "    def work(self, minute):\n",
    "        print('{}은 {}분 동안 업무를 합니다.'.format(self.name, minute))  # method overriding\n",
    "\n",
    "bob = Student('bob', 25)\n",
    "bob.eat('BBQ')\n",
    "bob.sleep(30)\n",
    "bob.work(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93db523f",
   "metadata": {},
   "source": [
    "#### **special method**\n",
    " - __로 시작 __로 끝나는 특수 함수\n",
    " - 해당 메쏘드들을 구현하면, 커스텀 객체에 여러가지 파이썬 내장 함수나 연산자를 적용 가능\n",
    " - 오버라이딩 가능한 함수 목록은 아래 링크에서 참조 \n",
    "   - https://docs.python.org/3/reference/datamodel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbf128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Point:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __add__(self, pt):\n",
    "        new_x = self.x + pt.x\n",
    "        new_y = self.y + pt.y\n",
    "        return Point(new_x, new_y)\n",
    "    \n",
    "    def __sub__(self, pt):\n",
    "        new_x = self.x - pt.x\n",
    "        new_y = self.y - pt.y\n",
    "        return Point(new_x, new_y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x ** 2 + self.y ** 2   # 제곱근 구하는 것 생략\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if index == 0:\n",
    "            return self.x\n",
    "        elif index == 1:\n",
    "            return self.y\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "    def __mul__(self, factor):\n",
    "        return Point(self.x * factor, self.y * factor)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return '({}, {})'.format(self.x, self.y)\n",
    "\n",
    "p1 = Point(3, 4)\n",
    "p2 = Point(2, 7)\n",
    "\n",
    "p3 = p1 + p2\n",
    "p4 = p1 - p2\n",
    "\n",
    "p5 = p1 * 3\n",
    "\n",
    "print(p1)\n",
    "print(p2)\n",
    "print(p3)\n",
    "print(p4)\n",
    "print(p5)\n",
    "print(len(p1))\n",
    "print(p1[0])\n",
    "print(p1[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e93c9ce",
   "metadata": {},
   "source": [
    "#### 연습문제)\n",
    " - 복소수 클래스를 정의 해봅시다.\n",
    " - 덧셈, 뺄셈, 곱셈 연산자 지원\n",
    " - 길이 (복소수의 크기) 지원 \n",
    " - 복소수 출력 '1 + 4j'와 같이 표현\n",
    " - 비교 연산 ==, != 지원\n",
    " - >=, <= , <, > 연산 지원\n",
    " - 절대값 지원\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c14cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class ComplexNumber:\n",
    "    def __init__(self, real, img):\n",
    "        self.real = real\n",
    "        self.img = img\n",
    "    \n",
    "    def __str__(self):\n",
    "        if self.img > 0:\n",
    "            return '{} + {}j'.format(self.real, self.img)\n",
    "        else:\n",
    "            return '{} - {}j'.format(self.real, abs(self.img))\n",
    "    \n",
    "    def __add__(self, cn):\n",
    "        return ComplexNumber(self.real + cn.real, self.img + cn.img)\n",
    "    \n",
    "    def __sub__(self, cn):\n",
    "        return ComplexNumber(self.real - cn.real, self.img - cn.img)\n",
    "    \n",
    "    def __eq__(self, cn):\n",
    "        return self.real == cn.real and self.img == cn.img\n",
    "    \n",
    "    def __abs__(self):\n",
    "        return math.sqrt(self.real **2 + self.img ** 2)\n",
    "    \n",
    "    \n",
    "a = ComplexNumber(1, 2)\n",
    "b = ComplexNumber(3, 5)\n",
    "\n",
    "print(a)\n",
    "print(a + b)\n",
    "print(a - b)\n",
    "\n",
    "print(a != b)\n",
    "\n",
    "print(abs(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57968e5",
   "metadata": {},
   "source": [
    "## 정규표현식 \n",
    " - regular expression\n",
    " - 특정한 패턴과 일치하는 문자열를 '검색', '치환', '제거' 하는 기능을 지원\n",
    " - 정규표현식의 도움없이 패턴을 찾는 작업(Rule 기반)은 불완전 하거나, 작업의 cost가 높음\n",
    " - e.g) 이메일 형식 판별, 전화번호 형식 판별, 숫자로만 이루어진 문자열 등"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687ef5a1",
   "metadata": {},
   "source": [
    "* **raw string**\n",
    " - 문자열 앞에 r이 붙으면 해당 문자열이 구성된 그대로 문자열로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b345ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'abcdef\\n' # escape 문자열\n",
    "print(a)\n",
    "\n",
    "b = r'abcdef\\n'\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e72a74",
   "metadata": {},
   "source": [
    "#### **기본 패턴**\n",
    " - a, X, 9 등등 문자 하나하나의 character들은 정확히 해당 문자와 일치\n",
    "   - e.g) 패턴 test는 test 문자열과 일치\n",
    "   - 대소문자의 경우 기본적으로 구별하나, 구별하지 않도록 설정 가능\n",
    " - 몇몇 문자들에 대해서는 예외가 존재하는데, 이들은 틀별한 의미로 사용 됨\n",
    "   - . ^ $ * + ? { } [ ] \\ | ( )\n",
    " \n",
    " - . (마침표) - 어떤 한개의 character와 일치 (newline(엔터) 제외)\n",
    " \n",
    " - \\w - 문자 character와 일치 [a-zA-Z0-9_]\n",
    " - \\s - 공백문자와 일치\n",
    " - \\t, \\n, \\r - tab, newline, return\n",
    " - \\d - 숫자 character와 일치 [0-9]\n",
    " - ^ = 시작, $ = 끝 각각 문자열의 시작과 끝을 의미\n",
    " - \\가 붙으면 스페셜한 의미가 없어짐. 예를들어 \\\\.는 .자체를 의미 \\\\\\는 \\를 의미\n",
    " - 자세한 내용은 링크 참조 https://docs.python.org/3/library/re.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ec4b2c",
   "metadata": {},
   "source": [
    "#### **search method**\n",
    " - 첫번째로 패턴을 찾으면 match 객체를 반환\n",
    " - 패턴을 찾지 못하면 None 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19c8493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "m = re.search(r'abc', '123abcdef')\n",
    "print(m.start())\n",
    "print(m.end())\n",
    "\n",
    "m = re.search(r'\\d\\d\\d', '112abcdef119')\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70afe90a",
   "metadata": {},
   "source": [
    "#### **metacharacters (메타 캐릭터)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1366cca",
   "metadata": {},
   "source": [
    "#### **[]** 문자들의 범위를 나타내기 위해 사용\n",
    "   - [] 내부의 메타 캐릭터는 캐릭터 자체를 나타냄\n",
    "   - e.g)\n",
    "   - [abck] : a or b or c or k\n",
    "   - [abc.^] : a or b or c or . or ^\n",
    "   - [a-d]  : -와 함께 사용되면 해당 문자 사이의 범위에 속하는 문자 중 하나\n",
    "   - [0-9]  : 모든 숫자\n",
    "   - [a-z]  : 모든 소문자\n",
    "   - [A-Z]  : 모든 대문자\n",
    "   - [a-zA-Z0-9] : 모든 알파벳 문자 및 숫자\n",
    "   - [^0-9] : ^가 맨 앞에 사용 되는 경우 해당 문자 패턴이 아닌 것과 매칭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77be9307",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(r'[cbm]at', 'cat')\n",
    "\n",
    "re.search(r'[abc.^]aron', 'caron')\n",
    "\n",
    "# ^가 맨 앞에 쓰이는 경우: not의 의미를 지닌다.\n",
    "re.search(r'[^abc]aron', '#aron')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf26306b",
   "metadata": {},
   "source": [
    "#### **\\** \n",
    " 1. 다른 문자와 함께 사용되어 특수한 의미를 지님\n",
    "   - \\d : 숫자를          [0-9]와 동일\n",
    "   - \\D : 숫자가 아닌 문자  [^0-9]와 동일\n",
    "   - \\s : 공백 문자(띄어쓰기, 탭, 엔터 등)\n",
    "   - \\S : 공백이 아닌 문자\n",
    "   - \\w : 알파벳대소문자, 숫자 [0-9a-zA-Z]와 동일\n",
    "   - \\W : non alpha-numeric 문자 [^0-9a-zA-Z]와 동일\n",
    " 2. 메타 캐릭터가 캐릭터 자체를 표현하도록 할 경우 사용\n",
    "   - \\\\. , \\\\\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d288b0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(r'\\sand', 'apple aㅇnd banana')\n",
    "\n",
    "# . 은 모든 문자를 의미하는 메타캐릭터인데, 이걸 캐릭터 자체로 표현하게 하려면 앞에 백슬래시\n",
    "re.search(r'\\.and', '.and')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a4b03b",
   "metadata": {},
   "source": [
    "#### **반복패턴**\n",
    " - 패턴 뒤에 위치하는 *, +, ?는 해당 패턴이 반복적으로 존재하는지 검사 \n",
    "   - '+' -> 1번 이상의 패턴이 발생\n",
    "   - '*' -> 0번 이상의 패턴이 발생\n",
    "   - '?' -> 0 혹은 1번의 패턴이 발생\n",
    " - 반복을 패턴의 경우 greedy하게 검색 함, 즉 가능한 많은 부분이 매칭되도록 함\n",
    "  - e.g) a[bcd]*b  패턴을 abcbdccb에서 검색하는 경우\n",
    "    - ab, abcb, abcbdccb 전부 가능 하지만 최대한 많은 부분이 매칭된 abcbdccb가 검색된 패턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7c94ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(r'a[bcd]*b', 'abcbdccb')\n",
    "# 반복패턴 검색은 '최소'가 아니라 '최대 매칭'이다.\n",
    "# [bcd]*는 0번 반복도 포함이기 때문에, r'ab'도 매칭이 된다.\n",
    "\n",
    "re.search(r'b\\w+a', 'banana')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcec914",
   "metadata": {},
   "source": [
    "#### **^**, **$**\n",
    " - ^  문자열의 맨 앞부터 일치하는 경우 검색\n",
    " \n",
    "re.search(r'b\\w+a' ...) 이라면 무조건! 'b'부터 시작해야 한다.\n",
    " - \\$  문자열의 맨 뒤부터 일치하는 경우 검색"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72047479",
   "metadata": {},
   "source": [
    " #### **grouping**\n",
    "  - ()을 사용하여 그루핑\n",
    "  - 매칭 결과를 각 그룹별로 분리 가능\n",
    "  - 패턴 명시 할 때, 각 그룹을 괄호() 안에 넣어 분리하여 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26eb8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = re.search(r'(\\w+)@(.+)', 'test@gmail.com')\n",
    "print(m.group(1))\n",
    "print(m.group(2))\n",
    "print(m.group(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e125b03a",
   "metadata": {},
   "source": [
    " #### **{}**\n",
    "  - *, +, ?을 사용하여 반복적인 패턴을 찾는 것이 가능하나, 반복의 횟수 제한은 불가\n",
    "  - 패턴뒤에 위치하는 중괄호{}에 숫자를 명시하면 해당 숫자 만큼의 반복인 경우에만 매칭\n",
    "  - {4} - 4번 반복\n",
    "  - {3,4} - 3 ~ 4번 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b3d4b2",
   "metadata": {},
   "source": [
    "#### **미니멈 매칭(non-greedy way)**\n",
    " - 기본적으로 *, +, ?를 사용하면 greedy(맥시멈 매칭)하게 동작함\n",
    " - *?, +?을 이용하여 해당 기능을 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8901e065",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(r'<.+>', '<html>haha</html>')   # greedy하게 search\n",
    "\n",
    "re.search(r'<.+?>', '<html>haha</html>')  # *? 이나 +?을 사용하면 minimum matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83e416c",
   "metadata": {},
   "source": [
    "#### **{}?**\n",
    " - {m,n}의 경우 m번 에서 n번 반복하나 greedy하게 동작\n",
    " - {m,n}?로 사용하면 non-greedy하게 동작. 즉, 최소 m번만 매칭하면 만족"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14910cfb",
   "metadata": {},
   "source": [
    "#### **match**\n",
    " - search와 유사하나, 주어진 문자열의 시작부터 비교하여 패턴이 있는지 확인\n",
    " - 시작부터 해당 패턴이 존재하지 않다면 None 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acff191",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.match(r'\\d\\d\\d', 'my number is 123')\n",
    "\n",
    "re.match(r'\\d\\d\\d', '123 is my number')  # 문자열의 시작에 패턴이 없으면 안됌. like r'^...'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13840780",
   "metadata": {},
   "source": [
    "#### **findall**\n",
    " - search가 최초로 매칭되는 패턴만 반환한다면, findall은 매칭되는 전체의 패턴을 반환\n",
    " - 매칭되는 모든 결과를 리스트 형태로 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b255c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'[\\w-]+@[\\w.]+', 'test@gmail.com haha test2@gmail.com nice weather today test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2add0500",
   "metadata": {},
   "source": [
    "#### **sub**\n",
    " - 주어진 문자열에서 일치하는 모든 패턴을 replace\n",
    " - 그 결과를 문자열로 다시 반환함\n",
    " - 두번째 인자는 특정 문자열이 될 수도 있고, 함수가 될 수 도 있음\n",
    " - count가 0인 경우는 전체를, 1이상이면 해당 숫자만큼 치환 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca4cf8f",
   "metadata": {},
   "source": [
    "#### **compile**\n",
    " - 동일한 정규표현식을 매번 다시 쓰기 번거로움을 해결\n",
    " - compile로 해당표현식을 re.RegexObject 객체로 저장하여 사용가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc21b6fd",
   "metadata": {},
   "source": [
    "### 연습문제 \n",
    "  - 아래 뉴스에서 이메일 주소를 추출해 보세요\n",
    "  - 다음중 올바른 (http, https) 웹페이지만 찾으시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a2dfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# 위의 두 모듈이 없는 경우에는 pip install requests bs4 실행\n",
    "\n",
    "def get_news_content(url):\n",
    "    response = requests.get(url)\n",
    "    content = response.text\n",
    "\n",
    "    soup = BeautifulSoup(content, 'html5lib')\n",
    "\n",
    "    div = soup.find('div', attrs = {'id' : 'harmonyContainer'})\n",
    "    \n",
    "    content = ''\n",
    "    for paragraph in div.find_all('p'):\n",
    "        content += paragraph.get_text()\n",
    "        \n",
    "    return content\n",
    "\n",
    "news1 = get_news_content('https://news.v.daum.net/v/20190617073049838')\n",
    "print(news1)\n",
    "\n",
    "\n",
    "email_reg = re.compile(r'[\\w-]+@[\\w.]+\\w+')\n",
    "email_reg.search(news1)\n",
    "\n",
    "webs = ['http://www.test.co.kr', \n",
    "        'https://www.test1.com', \n",
    "        'http://www.test.com', \n",
    "        'ftp://www.test.com', \n",
    "        'http:://www.test.com',\n",
    "       'htp://www.test.com',\n",
    "       'http://www.google.com', \n",
    "       'https://www.homepage.com.']\n",
    "\n",
    "web_reg = re.compile(r'https?://[\\w.]+\\w+$')\n",
    "list(map(lambda w:web_reg.search(w) != None, webs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e39c8b",
   "metadata": {},
   "source": [
    "#### requests 모듈\n",
    "1. http request/response를 위한 모듈\n",
    "2. HTTP method를 메소드 명으로 사용하여 request 요청 예) get, post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785d066f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f712b375",
   "metadata": {},
   "source": [
    "#### get 요청하기\n",
    "1. http get 요청하기\n",
    "2. query parameter 이용하여 데이터 전달하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78ca7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://news.v.daum.net/v/20190728165812603'\n",
    "resp = requests.get(url)\n",
    "\n",
    "resp   # '응답'에는 코드가 주어지는데, 첫 숫자가 2이면 성공이다.\n",
    "\n",
    "resp.text  # view page source에서 보는 텍스트와 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1392b3ab",
   "metadata": {},
   "source": [
    "#### post 요청하기\n",
    "1. http post 요청하기\n",
    "2. post data 이용하여 데이터 전달하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86d9663",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.kangcom.com/member/member_check.asp'\n",
    "\n",
    "data = {\n",
    "    'id': 'caphi0202',\n",
    "    'pwd': 'WLgns7872!'\n",
    "}\n",
    "\n",
    "resp = requests.post(url, data=data)\n",
    "resp.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4f502c",
   "metadata": {},
   "source": [
    "#### HTTP header 데이터 이용하기\n",
    "1. header 데이터 구성하기\n",
    "2. header 데이터 전달하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddb2096",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://news.v.daum.net/v/20190728165812603'\n",
    "headers = {\n",
    "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36'\n",
    "}\n",
    "\n",
    "resp = requests.get(url, headers=headers)\n",
    "resp.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd698318",
   "metadata": {},
   "source": [
    "#### HTTP response 처리하기\n",
    "1. response 객체의 이해\n",
    "2. status_code 확인하기\n",
    "3. text 속성 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45b9e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://news.v.daum.net/v/20190728165812603'\n",
    "resp = requests.get(url)\n",
    "\n",
    "if resp.status_code == 200:\n",
    "    print(resp.text)\n",
    "else:\n",
    "    print('error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2234c10b",
   "metadata": {},
   "source": [
    "### API 활용하기\n",
    "\n",
    "공공데이터 포털에 가서,\n",
    "- 데이터 활용 신청\n",
    "- API 호출에 필요한 parameter 값 확인 및 구성\n",
    "- 인증키 입력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bffe5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "serviceKey = 'tBgGZ6H%2B7SsFy0sxJQDw6BekvxeLL4Cg1fKoXaw%2Bjy8UVtdG3qNi%2BdLtKiT%2FEFhDTbWsutRNRDTJj02RvBhS9A%3D%3D'\n",
    "\n",
    "endpoint = 'http://apis.data.go.kr/B552584/ArpltnInforInqireSvc/getMsrstnAcctoRltmMesureDnsty?stationName=종로구&dataTerm=month&pageNo=1&numOfRows=100&returnType=json&serviceKey={}'.format(serviceKey)\n",
    "resp = requests.get(endpoint)\n",
    "\n",
    "print(endpoint)\n",
    "print(resp.status_code)\n",
    "print(resp.text)\n",
    "\n",
    "# 받아온 데이터가 json인 경우..\n",
    "\n",
    "data = resp.json()   # resp.json()은 dict type이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49130c64",
   "metadata": {},
   "source": [
    "### BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41aafada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed008766",
   "metadata": {},
   "source": [
    "#### html 문자열 파싱\n",
    " - 문자열로 정의된 html 데이터 파싱하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0231897",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '''\n",
    "<html>\n",
    "  <head>\n",
    "    <title>BeautifulSoup test</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <div id='upper' class='test' custom='good'>\n",
    "      <h3 title='Good Content Title'>Contents Title</h3>\n",
    "      <p>Test contents</p>\n",
    "    </div>\n",
    "    <div id='lower' class='test' custom='nice'>\n",
    "      <p>Test Test Test 1</p>\n",
    "      <p>Test Test Test 2</p>\n",
    "      <p>Test Test Test 3</p>\n",
    "    </div>\n",
    "  </body>\n",
    "</html>'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b10c5d7",
   "metadata": {},
   "source": [
    "#### find 함수\n",
    " - 특정 html tag를 검색\n",
    " - 검색 조건을 명시하여 찾고자하는 tag를 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e49883e",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html)\n",
    "\n",
    "soup.find('h3')  # 가장 먼저 찾는 태그를 반환시킨다.\n",
    "\n",
    "soup.find('div', custom='nice')   # 원하는 태그, 원하는 attribute값을 검색할 수 있음.\n",
    "\n",
    "# class 값을 검색하려면 class_ 로 입력해야됌.\n",
    "soup.find('div', class_='test')\n",
    "\n",
    "# dict로 attributes 입력 가능\n",
    "\n",
    "attrs = {'id': 'upper', 'class': 'test'}\n",
    "soup.find('div', attrs=attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac181c5",
   "metadata": {},
   "source": [
    "#### find_all 함수\n",
    " - find가 조건에 만족하는 하나의 tag만 검색한다면, find_all은 조건에 맞는 모든 tag를 리스트로 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca6a0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('p')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18acf551",
   "metadata": {},
   "source": [
    "#### get_text 함수\n",
    " - tag안의 value를 추출\n",
    " - 부모tag의 경우, 모든 자식 tag의 value를 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54733e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = soup.find('h3')\n",
    "print(tag)\n",
    "tag.get_text()    # 태그 사이의 value를 반환한다.\n",
    "\n",
    "tag = soup.find('div')\n",
    "print(tag)\n",
    "tag.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ff5cc3",
   "metadata": {},
   "source": [
    "#### attribute 값 추출하기\n",
    " - 경우에 따라 추출하고자 하는 값이 attribute에도 존재함\n",
    " - 이 경우에는 검색한 tag에 attribute 이름을 [ ]연산을 통해 추출가능\n",
    " - 예) div.find('h3')['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b0a8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = soup.find('h3')\n",
    "print(tag)\n",
    "tag['title']  # title이라는 attr의 값을 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d44cc3a",
   "metadata": {},
   "source": [
    "### 학습목표\n",
    "1. beautifulsoup 모듈 사용하기\n",
    "2. id, class 속성으로 tag 찾기\n",
    "3. CSS를 이용하여 tag 찾기\n",
    "4. 속성 값으로 tag 찾기\n",
    "5. 정규표현식으로 tag 찾기\n",
    "6. 개발자도구를 이용하여 동적으로 로딩되는 데이터 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5457496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8384a54f",
   "metadata": {},
   "source": [
    "#### 다음 뉴스 데이터 추출\n",
    " - 뉴스기사에서 제목, 작성자, 작성일\n",
    " , 댓글 개수 추출\n",
    " - [뉴스링크](https://news.v.daum.net/v/20190728165812603)\n",
    " - tag를 추출할때는 가장 그 tag를 쉽게 특정할 수 있는 속성을 사용\n",
    "  - id의 경우 원칙적으로 한 html 문서 내에서 유일\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a70e6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://news.v.daum.net/v/20190728165812603'\n",
    "\n",
    "resp = requests.get(url)\n",
    "\n",
    "soup = bs(resp.text)\n",
    "soup.find('h3', class_='tit_view').get_text()   # 제목 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9502869e",
   "metadata": {},
   "source": [
    "* id, class 속성으로 tag 찾기\n",
    " - 타이틀\n",
    " - 작성자, 작성일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453d7fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('span', class_='txt_info') # 작성자와 작성일이 나온다.\n",
    "\n",
    "soup.find_all('span', class_='txt_info')[0] # 작성자\n",
    "soup.find_all('span', class_='txt_info')[1] # 작성일\n",
    "\n",
    "# 아니면 parent를 변수에 넣어놓은 다음에, 그 변수 내에서 다시 찾는다.\n",
    "info = soup.find('span', class_='info_view')   # 이 예시에서는 부모가 <span, class=info_view>\n",
    "info.find('span', class_='txt_info')\n",
    "\n",
    "url = 'https://news.v.daum.net/v/20190728165812603'\n",
    "resp = requests.get(url)\n",
    "soup = bs(resp.text)\n",
    "\n",
    "# p 태그는 워낙 자주 쓰이기 때문에.. 본문의 parent container를 먼저 설정해두고,\n",
    "# container 내에서 p 태그를 찾는다.\n",
    "# 이렇게만 해놓으면 p 태그가 모두 '리스트' 형식으로 반환되기 때문에 아래 for문으로 text만 추출한다.\n",
    "container = soup.find('div', id='harmonyContainer')\n",
    "contents = ''\n",
    "for p in container.find_all('p'):\n",
    "    contents += p.get_text().strip()\n",
    "    \n",
    "contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e0bf8a",
   "metadata": {},
   "source": [
    "* CSS를 이용하여 tag 찾기\n",
    " - select, select_one함수 사용 \n",
    " - css selector 사용법\n",
    "   - 태그명 찾기 tag \n",
    "   - 자손 태그 찾기 - 자손 관계 (tag tag)\n",
    "   - 자식 태그 찾기 - 다이렉트 자식 관계 (tag > tag)\n",
    "   - 아이디 찾기 #id\n",
    "   - 클래스 찾기 .class\n",
    "   - 속성값 찾기 [name='test']\n",
    "     - 속성값 prefix 찾기 [name ^='test']\n",
    "     - 속성값 suffix 찾기 [name $='test']\n",
    "     - 속성값 substring 찾기 [name *='test]\n",
    "   - n번째 자식 tag 찾기 :nth-child(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f091e166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select는 만족하는 모든 값을 반환 / select_one은 첫 번째 값만.\n",
    "\n",
    "url = 'https://news.v.daum.net/v/20190728165812603'\n",
    "resp = requests.get(url)\n",
    "soup = bs(resp.text)\n",
    "\n",
    "soup.select('#harmonyContainer p')\n",
    "\n",
    "soup.select('#harmonyContainer > p')\n",
    "\n",
    "soup.select('h3.tit_view')\n",
    "\n",
    "soup.select('h3[class^=\"t\"]')\n",
    "\n",
    "soup.select('h3[class^=\"tx\"]')\n",
    "\n",
    "soup.select('h3[class*=\"view\"]')\n",
    "\n",
    "## ^=는 첫 시작\n",
    "## $=는 끝부터\n",
    "## *=는 ~~을 포함하는\n",
    "\n",
    "soup.select('span.txt_info:nth-child(1)')\n",
    "\n",
    "soup.select('span.txt_info:nth-child(2)')\n",
    "\n",
    "# 여러 개의 동일한 태그가 있는 경우에는 :nth-child(n)을 쓰면 됌."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fec2602",
   "metadata": {},
   "source": [
    "#### 정규표현식으로 tag 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc6471e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 모든 h 관련 태그를 가져오기\n",
    "soup.find_all(re.compile('h\\d'))\n",
    "\n",
    "soup.find_all('img', attrs={'src': re.compile('.+\\.jpg')})\n",
    "\n",
    "# img 태그에서 src 속성의 값이 .jpg로 끝나는 모든 값 반환\n",
    "\n",
    "soup.find_all('h3', class_=re.compile('.+view$'))\n",
    "\n",
    "# h3 태그에서 class가 view로 끝나는 모든 값 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1526b9e4",
   "metadata": {},
   "source": [
    "### 학습목표\n",
    "1. 다음 뉴스 댓글 개수 크롤링\n",
    "2. 로그인 하여 크롤링 하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b761065",
   "metadata": {},
   "source": [
    "* 댓글 개수 추출\n",
    " - 댓글의 경우, 최초 로딩시에 전달되지 않음\n",
    " - 이 경우는 추가적으로 AJAX로 비동기적 호출을 하여 따로 data 전송을 함\n",
    "   - 개발자 도구의 network 탭에서 확인(XHR: XmlHTTPRequest)\n",
    "   - 비동기적 호출: 사이트의 전체가 아닌 일부분만 업데이트 가능하도록 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c361375",
   "metadata": {},
   "source": [
    "#### HTTP 상태 코드\n",
    " - 1xx (정보): 요청을 받았으며 프로세스를 계속한다\n",
    " - 2xx (성공): 요청을 성공적으로 받았으며 인식했고 수용하였다\n",
    " - 3xx (리다이렉션): 요청 완료를 위해 추가 작업 조치가 필요하다\n",
    " - 4xx (클라이언트 오류): 요청의 문법이 잘못되었거나 요청을 처리할 수 없다\n",
    " - 5xx (서버 오류): 서버가 명백히 유효한 요청에 대해 충족을 실패했다\n",
    "\n",
    "[출처: 위키피디아](https://ko.wikipedia.org/wiki/HTTP_%EC%83%81%ED%83%9C_%EC%BD%94%EB%93%9C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3431410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개발자 도구 - 네트워크 - XHR(비동기적) - Response에서 원하는 값을 찾기 -\n",
    "# Headers에서 Endpoint 찾기(url)\n",
    "\n",
    "url = 'https://comment.daum.net/apis/v1/ui/single/main/@20190728165812603'\n",
    "\n",
    "resp = requests.get(url)\n",
    "print(resp)\n",
    "\n",
    "# 클라이언트 오류가 나온 경우.. Request header를 dict 형태로 입력할 경우 호출에 성공할 확률이 높아진다.\n",
    "\n",
    "headers = {\n",
    "    'Authorization': 'Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJmb3J1bV9rZXkiOiJuZXdzIiwiZ3JhbnRfdHlwZSI6ImFsZXhfY3JlZGVudGlhbHMiLCJzY29wZSI6W10sImV4cCI6MTYyNjE3NzUzMSwiYXV0aG9yaXRpZXMiOlsiUk9MRV9DTElFTlQiXSwianRpIjoiNjQwY2Q4NWMtZGEyYy00ZjUzLTk5MTMtMTE2OTFiNTA2OWRlIiwiZm9ydW1faWQiOi05OSwiY2xpZW50X2lkIjoiMjZCWEF2S255NVdGNVowOWxyNWs3N1k4In0.y2pkPeCWE6bcBlTtTG9jEca963vRT3yNCB_OPGChe3c',\n",
    "    'Origin': 'https://news.v.daum.net',\n",
    "    'Referer': 'https://news.v.daum.net/v/20190728165812603',\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36',\n",
    "}\n",
    "\n",
    "resp = requests.get(url, headers=headers)\n",
    "\n",
    "\n",
    "resp.text  # json 형식으로 값을 가져온다.\n",
    "\n",
    "print(resp.json())\n",
    "resp.json()['post']['commentCount']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7f4643",
   "metadata": {},
   "source": [
    "#### 로그인하여 데이터 크롤링하기\n",
    " - 특정한 경우, 로그인을 해서 크롤링을 해야만 하는 경우가 존재\n",
    " - 예) 쇼핑몰에서 주문한 아이템 목록, 마일리지 조회 등\n",
    " - 이 경우, 로그인을 자동화 하고 로그인에 사용한 세션을 유지하여 크롤링을 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17300e84",
   "metadata": {},
   "source": [
    "#### 로그인 후 데이터 크롤링 하기\n",
    " 1. endpoint 찾기 (개발자 도구의 network를 활용)\n",
    " 2. id와 password가 전달되는 form data찾기\n",
    " 3. session 객체 생성하여 login 진행\n",
    " 4. 이후 session 객체로 원하는 페이지로 이동하여 크롤링\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaa0de3",
   "metadata": {},
   "source": [
    "* endpoint 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9243d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "url = 'https://www.kangcom.com/member/member_check.asp'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfb6e4f",
   "metadata": {},
   "source": [
    "* id, password로 구성된 form data 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3e42f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'id': 'caphi0202',\n",
    "    'pwd': 'WLgns7872!'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d723e468",
   "metadata": {},
   "source": [
    "* login\n",
    " - endpoint(url)과 data를 구성하여 post 요청\n",
    " - login의 경우 post로 구성하는 것이 정상적인 웹사이트!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd25834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = requests.Session()\n",
    "\n",
    "resp = s.post(url, data=data)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b2de27",
   "metadata": {},
   "source": [
    "* crawling\n",
    " - login 시 사용했던 session을 다시 사용하여 요청"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd83211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_page = 'https://www.kangcom.com/mypage/'\n",
    "resp = s.get(my_page)\n",
    "\n",
    "soup = bs(resp.text)\n",
    "\n",
    "soup.select_one('td.a_bbslist55:nth-child(3)').get_text() # 마일리지 값 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077c2588",
   "metadata": {},
   "source": [
    "### 학습목표\n",
    " 1. selenium 모듈 사용법 알아보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830cf710",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install selenium\n",
    "pip install webdriver-manager\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09ec119",
   "metadata": {},
   "source": [
    "#### selenium\n",
    " - 웹페이지 테스트 자동화용 모듈\n",
    " - 개발/테스트용 드라이버(웹브라우저)를 사용하여 실제 사용자가 사용하는 것처럼 동작\n",
    " - 실습전 확인사항\n",
    "   - selenium 모듈 설치\n",
    "   - [크롬 드라이버 다운로드](https://chromedriver.chromium.org/downloads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007d2707",
   "metadata": {},
   "source": [
    "#### selenium 예제\n",
    " - python.org 로 이동하여 자동으로 검색해보기\n",
    "   1. python.org 사이트 오픈\n",
    "   2. input 필드를 검색하여 Key 이벤트 전달\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f81ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_driver = '/Users/Jihun/Desktop/Fastcampus/chromedriver'\n",
    "driver = webdriver.Chrome(chrome_driver)\n",
    "\n",
    "driver.get('https://www.python.org')\n",
    "\n",
    "search = driver.find_element_by_id('id-search-field')\n",
    "search.clear()\n",
    "time.sleep(3)\n",
    "\n",
    "search.send_keys('lambda')\n",
    "time.sleep(3)\n",
    "\n",
    "search.send_keys(Keys.RETURN)\n",
    "time.sleep(3)\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d850f15f",
   "metadata": {},
   "source": [
    "#### selenium을 이용한 다음뉴스 웹사이트 크롤링\n",
    " - driver 객체의 find_xxx_by 함수 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e562d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_driver = '/Users/Jihun/Desktop/Fastcampus/chromedriver'\n",
    "driver = webdriver.Chrome(chrome_driver)\n",
    "\n",
    "url = 'https://news.v.daum.net/v/20190728165812603'\n",
    "driver.get(url)\n",
    "\n",
    "time.sleep(2)   # 이 sleep을 안 주고 코드 돌리면 comment 0이 된다.\n",
    "                # sleep 동안 로딩이 되고 나서 page_source를 받으면 comment count가 제대로 나온다.\n",
    "src = driver.page_source\n",
    "soup = bs(src)\n",
    "\n",
    "driver.close()\n",
    "\n",
    "comment = soup.select_one('span.alex-count-area')\n",
    "comment.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b06fad4",
   "metadata": {},
   "source": [
    "#### selenium을 활용하여 특정 element의 로딩 대기\n",
    " - WebDriverWait 객체를 이용하여 해당 element가 로딩 되는 것을 대기\n",
    " - 실제로 해당 기능을 활용하여 거의 모든 사이트의 크롤링이 가능\n",
    " - WebDriverWait(driver, 시간(초)).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'CSS_RULE')))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faca588",
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_driver = '/Users/Jihun/Desktop/Fastcampus/chromedriver'\n",
    "driver = webdriver.Chrome(chrome_driver)\n",
    "\n",
    "url = 'https://news.naver.com/main/read.naver?mode=LSD&mid=shm&sid1=102&oid=056&aid=0011081710'\n",
    "driver.get(url)\n",
    "\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'span.u_cbox_info_txt')))\n",
    "# 10초 안에 로딩이 되면 그대로 완료; 10초 안에 로딩 안 되면 오류 발생\n",
    "\n",
    "src = driver.page_source\n",
    "soup = bs(src)\n",
    "\n",
    "driver.close()\n",
    "\n",
    "comment = soup.select_one('span.u_cbox_info_txt')\n",
    "comment.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9395ae5e",
   "metadata": {},
   "source": [
    "### 학습목표\n",
    " - 1. 다음 뉴스와 그 뉴스의 댓글 크롤링하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282e165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97240629",
   "metadata": {},
   "source": [
    "#### 뉴스 제목 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe93d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daum_news_title(news_id):\n",
    "    url = 'https://news.v.daum.net/v/{}'.format(news_id)\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    soup = bs(resp.text)\n",
    "    \n",
    "    title_tag = soup.select_one('h3.tit_view')\n",
    "    if title_tag:\n",
    "        return title_tag.get_text()\n",
    "    return \"\"\n",
    "\n",
    "get_daum_news_title('20190728165812603')\n",
    "\n",
    "get_daum_news_title('20190801114158041')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54772e5e",
   "metadata": {},
   "source": [
    "#### 뉴스 본문 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e77a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daum_news_content(news_id):\n",
    "    url = 'https://news.v.daum.net/v/{}'.format(news_id)\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    soup = bs(resp.text)\n",
    "    \n",
    "    content = ''\n",
    "    for p in soup.select('div#harmonyContainer p'):  # div 태그에 harmonyContainer 아이디의 child p 태그\n",
    "        content += p.get_text()\n",
    "    return content\n",
    "\n",
    "get_daum_news_content('20190728165812603')\n",
    "\n",
    "get_daum_news_content('20190801114158041')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfaa827",
   "metadata": {},
   "source": [
    "#### 뉴스 댓글 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602d6fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://comment.daum.net/apis/v1/posts/@20190728165812603/comments?parentId=0&offset=43&limit=10&sort=RECOMMEND&isInitial=false'\n",
    "headers = {\n",
    "    'Authorization': 'Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJmb3J1bV9rZXkiOiJuZXdzIiwiZ3JhbnRfdHlwZSI6ImFsZXhfY3JlZGVudGlhbHMiLCJzY29wZSI6W10sImV4cCI6MTYyNjE3NzUzMSwiYXV0aG9yaXRpZXMiOlsiUk9MRV9DTElFTlQiXSwianRpIjoiNjQwY2Q4NWMtZGEyYy00ZjUzLTk5MTMtMTE2OTFiNTA2OWRlIiwiZm9ydW1faWQiOi05OSwiY2xpZW50X2lkIjoiMjZCWEF2S255NVdGNVowOWxyNWs3N1k4In0.y2pkPeCWE6bcBlTtTG9jEca963vRT3yNCB_OPGChe3c',\n",
    "    'Origin': 'https://news.v.daum.net',\n",
    "    'Referer': 'https://news.v.daum.net/v/20190728165812603',\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36',\n",
    "}\n",
    "resp = requests.get(url, headers=headers)\n",
    "\n",
    "def get_daum_news_comments(news_id):\n",
    "    headers = {\n",
    "        'Authorization': 'Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJncmFudF90eXBlIjoiYWxleF9jcmVkZW50aWFscyIsInNjb3BlIjpbXSwiZXhwIjoxNTY0Njc4NjQ1LCJhdXRob3JpdGllcyI6WyJST0xFX0NMSUVOVCJdLCJqdGkiOiJlZGUxNzM0MS1hNWNjLTRmYmQtODJkMy0zZTMwOGMwMGViZTEiLCJjbGllbnRfaWQiOiIyNkJYQXZLbnk1V0Y1WjA5bHI1azc3WTgifQ.Cxs2g1hUUAjyuSrUDAhaKGol8vvyW-_mwPtV0X0DvEU',\n",
    "        'Origin': 'https://news.v.daum.net',\n",
    "        'Referer': 'https://news.v.daum.net/v/20190728165812603',\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    url_template = 'https://comment.daum.net/apis/v1/posts/@{}/comments?parentId=0&offset={}&limit=10&sort=RECOMMEND&isInitial=false'\n",
    "    offset = 0\n",
    "    comments = []\n",
    "    while True:\n",
    "        url = url_template.format(news_id, offset)\n",
    "        resp = requests.get(url, headers=headers)\n",
    "        data = resp.json()\n",
    "        if not data:\n",
    "            break\n",
    "            \n",
    "        comments.extend(data)\n",
    "        offset += 10\n",
    "        \n",
    "    return comments\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823cb293",
   "metadata": {},
   "source": [
    "#### np.array / np.arange 함수로 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ee83af",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3, 4])\n",
    "print(x)\n",
    "\n",
    "y = np.array([[2, 3, 4], [1, 2, 5]])  # 2행 3열의 행렬\n",
    "print(y)\n",
    "\n",
    "\n",
    "# np.arange는 파이썬의 range함수와 같다.\n",
    "np.arange(1,10,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120a47ca",
   "metadata": {},
   "source": [
    "#### np.ones, np.zeros로 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d536a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ones((4,5)) # 4행 5열의 모든 원소가 1인 행렬\n",
    "\n",
    "np.ones((2, 3, 4)) # 3행 4열의 행렬이 2개가 있다. (3차원)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96839776",
   "metadata": {},
   "source": [
    "#### np.empty, np.full로 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a566391",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.empty((3, 4)) # 초기화된 값\n",
    "\n",
    "np.full((3, 4), 7)   # 7로만 이루어진 3행 4열 행렬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f252f0",
   "metadata": {},
   "source": [
    "#### np.eye로 생성하기\n",
    " - 단위 행렬 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b8e075",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.eye(5)  # 5차원 단위행렬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0d11ba",
   "metadata": {},
   "source": [
    "#### np.linspace로 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6440850",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(1, 10, 3)  # 1이 시작 10이 끝. 총 3개 숫자가 균일하게 분포되도록\n",
    "\n",
    "np.linspace(1, 10, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bcee57",
   "metadata": {},
   "source": [
    "#### reshape 함수 활용\n",
    " - ndarray의 형태, 차원을 바꾸기 위해 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd26d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(1, 16)\n",
    "print(x)\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "x.reshape(3, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74da6b0f",
   "metadata": {},
   "source": [
    "### random 서브모듈"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac3fc35",
   "metadata": {},
   "source": [
    "#### rand 함수\n",
    " - 0, 1사이의 분포로 랜덤한 ndarray 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3020f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.rand(2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b964fdc",
   "metadata": {},
   "source": [
    "#### randn함수\n",
    " - n: normal distribution(정규분포)\n",
    " - 정규분포로 샘플링된 랜덤 ndarray 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91043a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randn(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bbffeb",
   "metadata": {},
   "source": [
    "#### randint 함수\n",
    " - 특정 정수 사이에서 랜덤하게 샘플링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ad0527",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randint(1, 100, size=(3, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9747ffdc",
   "metadata": {},
   "source": [
    "#### seed 함수\n",
    " - 랜덤한 값을 동일하게 다시 생성하고자 할때 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec31367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한번 랜덤하게 불러온 값을 다시 불러야 할 때..\n",
    "# seed를 실행하고 rand하면 동일한 값을 반환한다.\n",
    "\n",
    "np.random.seed(100)\n",
    "np.random.randn(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543fb1f0",
   "metadata": {},
   "source": [
    "#### choice\n",
    " - 주어진 1차원 ndarray로 부터 랜덤으로 샘플링\n",
    " - 정수가 주어진 경우, np.arange(해당숫자)로 간주"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f343e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(np.arange(20), (3, 4), replace=False)\n",
    "\n",
    "x = np.array([1, 2, 3, 1.5, 2.6, 4.9])\n",
    "np.random.choice(x, size=(2, 2), replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01fa26e",
   "metadata": {},
   "source": [
    "#### 확률분포에 따른 ndarray 생성\n",
    " - uniform\n",
    " - normal 등등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766eedcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.uniform(1.0, 3.0, size=(4, 5))\n",
    "\n",
    "np.random.normal(loc=20, scale=5, size=(3, 4))  # default는 X ~ N(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359f6b1a",
   "metadata": {},
   "source": [
    "### 인덱싱\n",
    " - 파이썬 리스트와 동일한 개념으로 사용\n",
    " - ,를 사용하여 각 차원의 인덱스에 접근 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b71ffc",
   "metadata": {},
   "source": [
    " * 1차원 벡터 인덱싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611a1eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(10)\n",
    "print(x)\n",
    "\n",
    "print(x[3])\n",
    "\n",
    "x[3] = 100\n",
    "print(x[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6721c526",
   "metadata": {},
   "source": [
    "* 2차원 행렬 인덱싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f618185",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(10).reshape(2, 5)\n",
    "print(x)\n",
    "\n",
    "print(x[0])\n",
    "print(x[0, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b653507",
   "metadata": {},
   "source": [
    "* 3차원 텐서 인덱싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64623c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(36).reshape(3, 4, 3)\n",
    "print(x)\n",
    "\n",
    "print(x[0])\n",
    "print(x[0, 2])\n",
    "print(x[0, 2, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da2e791",
   "metadata": {},
   "source": [
    "### 슬라이싱\n",
    " - 리스트, 문자열 slicing과 동일한 개념으로 사용\n",
    " - ,를 사용하여 각 차원 별로 슬라이싱 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d0cbe1",
   "metadata": {},
   "source": [
    "* 1차원 벡터 슬라이싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e03a101",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(10)\n",
    "\n",
    "x[1:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3cfd44",
   "metadata": {},
   "source": [
    "* 2차원 행렬 슬라이싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f6d3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(10).reshape(2, 5)\n",
    "print(x)\n",
    "\n",
    "x[:, 1:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1f78cb",
   "metadata": {},
   "source": [
    "* 3차원 텐서 슬라이싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549aae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(54).reshape(2, 9, 3)\n",
    "print(x)\n",
    "\n",
    "x[:1, :2, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854dc2ab",
   "metadata": {},
   "source": [
    "### ndarray shape 변경하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a311a2e",
   "metadata": {},
   "source": [
    "#### ravel, np.ravel\n",
    "  - 다차원배열을 1차원으로 변경\n",
    "  - 'order' 파라미터\n",
    "    - 'C' - row 우선 변경\n",
    "    - 'F - column 우선 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ad081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(15).reshape(3, 5)\n",
    "print(x)\n",
    "\n",
    "np.ravel(x)  # 다차원 벡터를 1차원으로 늘려준다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5860add",
   "metadata": {},
   "source": [
    "#### flatten\n",
    " - 다차원 배열을 1차원으로 변경\n",
    " - ravel과의 차이점: copy를 생성하여 변경함(즉 원본 데이터가 아닌 복사본을 반환)\n",
    " - 'order' 파라미터\n",
    "   - 'C' - row 우선 변경\n",
    "   - 'F - column 우선 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdae91e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.arange(15).reshape(3, 5)\n",
    "print(y)\n",
    "\n",
    "y.flatten()\n",
    "\n",
    "y.flatten(order='F')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a55fd24",
   "metadata": {},
   "source": [
    "## numpy documentation\n",
    " - [numpy 공식 문서 링크](https://www.numpy.org/devdocs/reference/)\n",
    " - numpy에서 제공되는 함수등에 대한 문서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90e8b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(15).reshape(3, 5)\n",
    "y = np.random.rand(15).reshape(3, 5)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8dffa8",
   "metadata": {},
   "source": [
    "#### 연산 함수\n",
    " - add, substract, multiply, divide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78402c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.add(x, y)  # element-wise add\n",
    "              # 두 행렬의 shape이 다른 경우 불가능!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c34974c",
   "metadata": {},
   "source": [
    "#### 통계 함수\n",
    " - 평균, 분산, 중앙, 최대, 최소값 등등 통계 관련된 함수가 내장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994c57b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y)\n",
    "y.mean()\n",
    "\n",
    "np.max(y), np.var(y), np.median(y), np.std(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1810387f",
   "metadata": {},
   "source": [
    "#### 집계함수\n",
    " - 합계(sum), 누적합계(cumsum) 등등 계산 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700d6cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "y\n",
    "\n",
    "np.sum(y)\n",
    "np.sum(y, axis=0) # 열 방향으로 합\n",
    "np.sum(y, axis=1) # 행 방향으로 합\n",
    "\n",
    "np.cumsum(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cf1424",
   "metadata": {},
   "source": [
    "#### any, all 함수\n",
    " - any: 특정 조건을 만족하는 것이 하나라도 있으면 True, 아니면 False\n",
    " - all: 모든 원소가 특정 조건을 만족한다면 True, 아니면 False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946e69e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.random.randn(10)\n",
    "print(z)\n",
    "\n",
    "z > 0\n",
    "\n",
    "np.any(z > 0), np.all(z > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f067ac3",
   "metadata": {},
   "source": [
    "### where 함수 *중요\n",
    " - 조건에 따라 선별적으로 값을 선택 가능\n",
    " - 사용 예) 음수인경우는 0, 나머지는 그대로 값을 쓰는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9a6d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.random.randn(10)\n",
    "print(z)\n",
    "\n",
    "np.where(z > 0, z, 0)  # z가 0보다 크면 z를 쓰고, 아니면 0을 써라"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390371a1",
   "metadata": {},
   "source": [
    "## 학습목표\n",
    " 1. axis의 이해\n",
    " 2. axis를 파라미터로 갖는 함수 이해하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3aa525",
   "metadata": {},
   "source": [
    "#### axis 이해하기\n",
    " - 몇몇 함수에는 axis keyword 파라미터가 존재\n",
    " - axis값이 없는 경우에는 전체 데이터에 대해 적용\n",
    " - axis값이 있는 경우에는, 해당 axis를 **따라서** 연산 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b9c933",
   "metadata": {},
   "source": [
    "* axis를 파라미터로 갖는 함수를 이용하기\n",
    " - 거의 대부분의 연산 함수들이 axis 파라미터를 사용\n",
    " - 이 경우, 해당 값이 주어졌을 때, 해당 axis를 **따라서** 연산이 적용\n",
    "   - 따라서 결과는 해당 axis가 제외된 나머지 차원의 데이터만 남게 됨\n",
    " - 예) np.sum, np.mean, np.any 등등\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8074823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(15)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b403f660",
   "metadata": {},
   "source": [
    "* 1차원 데이터에 적용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8a2ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a14a123",
   "metadata": {},
   "source": [
    "* 행렬에 적용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f872e54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x.reshape(3, 5)\n",
    "print(y)\n",
    "\n",
    "np.sum(y, axis=0), np.sum(y, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0092a1",
   "metadata": {},
   "source": [
    "* 3차원 텐서에 적용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61389e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.arange(36).reshape(3, 4, 3)\n",
    "print(z)\n",
    "\n",
    "np.sum(z, axis=0), np.sum(z, axis=1), np.sum(z, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f74e70",
   "metadata": {},
   "source": [
    "* axis의 값이 튜플일 경우\n",
    " - 해당 튜플에 명시된 모든 axis에 대해서 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30a0a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(z)\n",
    "\n",
    "np.sum(z, axis=(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f9a1d7",
   "metadata": {},
   "source": [
    "## 브로드캐스팅\n",
    "  - Shape이 같은 두 ndarray에 대한 연산은 각 원소별로 진행\n",
    "  - 연산되는 두 ndarray가 다른 Shape을 갖는 경우 브로드 캐스팅(Shape을 맞춤) 후 진행\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ed0628",
   "metadata": {},
   "source": [
    "#### 브로드캐스팅 Rule\n",
    " - [공식문서](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html#general-broadcasting-rules)\n",
    " - 뒷 차원에서 부터 비교하여 Shape이 같거나, 차원 중 값이 1인 것이 존재하면 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a1a145",
   "metadata": {},
   "source": [
    "![브로드캐스팅 예](https://www.tutorialspoint.com/numpy/images/array.jpg)\n",
    "    - 출처: https://www.tutorialspoint.com/numpy/images/array.jpg "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea52cbf",
   "metadata": {},
   "source": [
    "* Shape이 같은 경우의 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6478d82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(15).reshape(3, 5)\n",
    "y = np.random.rand(15).reshape(3, 5)\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2801a5",
   "metadata": {},
   "source": [
    "* Scalar(상수)와의 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843c1d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "2 + x, 5 * y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5c40f4",
   "metadata": {},
   "source": [
    "* Shape이 다른 경우 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542211a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(12).reshape(4, 3)\n",
    "b = np.arange(100, 103)\n",
    "c = np.arange(1000, 1004)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "a + b\n",
    "\n",
    "print(c)\n",
    "# a + c는 오류 반환\n",
    "\n",
    "d = b.reshape(1, 3)\n",
    "\n",
    "a + d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b17833",
   "metadata": {},
   "source": [
    "## Boolean indexing\n",
    "  - ndarry 인덱싱 시, bool 리스트를 전달하여 True인 경우만 필터링\n",
    "\n",
    "* 브로드캐스팅을 활용하여 ndarray로 부터 bool list 얻기\n",
    " - 예) 짝수인 경우만 찾아보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0206a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randint(1, 100, size=10)\n",
    "print(x)\n",
    "\n",
    "even_mask = x % 2 == 0\n",
    "\n",
    "x[even_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39bbf26",
   "metadata": {},
   "source": [
    "####  다중조건 사용하기\n",
    " - 파이썬 논리 연산지인 and, or, not 키워드 사용 불가\n",
    " - & - AND \n",
    " - | - OR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56d940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (x > 30) & (x % 2 == 0)\n",
    "\n",
    "x[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c002fc",
   "metadata": {},
   "source": [
    "#### 예제) 2019년 7월 서울 평균기온 데이터\n",
    " - 평균기온이 25도를 넘는 날수는?\n",
    " - 평균기온이 25를 넘는 날의 평균 기온은?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2ff386",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.array(\n",
    "        [23.9, 24.4, 24.1, 25.4, 27.6, 29.7,\n",
    "         26.7, 25.1, 25.0, 22.7, 21.9, 23.6, \n",
    "         24.9, 25.9, 23.8, 24.7, 25.6, 26.9, \n",
    "         28.6, 28.0, 25.1, 26.7, 28.1, 26.5, \n",
    "         26.3, 25.9, 28.4, 26.1, 27.5, 28.1, 25.8])\n",
    "\n",
    "print(len(temp))\n",
    "\n",
    "np.sum(temp > 25.0)\n",
    "\n",
    "np.mean(temp[temp > 25.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8efd1e1",
   "metadata": {},
   "source": [
    "## 선형대수 연산\n",
    "\n",
    "#### np.linalg.inv\n",
    " - 역행렬을 구할 때 사용\n",
    " - 모든 차원의 값이 같아야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea3e17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(3, 3)\n",
    "print(x)\n",
    "\n",
    "x @ np.linalg.inv(x) # 행렬의 곱은 @\n",
    "                     # 행렬 * 역행렬 = 단위행렬\n",
    "\n",
    "# np.matmul(x, np.linalg.inv(x)) 도 행렬의 곱을 반환\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4719749",
   "metadata": {},
   "source": [
    "#### np.linalg.solve\n",
    " - Ax = B 형태의 선형대수식 솔루션을 제공\n",
    " - 예제) 호랑이와 홍합의 합 : 25 호랑이 다리와 홍합 다리의 합은 64\n",
    "   - x + y = 25\n",
    "   - 2x + 4y = 64\n",
    "   \n",
    " $$\\begin{pmatrix} 1 & 1 \\\\ 2 & 4 \\end{pmatrix}\\begin{pmatrix} x \\\\ y \\end{pmatrix}= \\begin{pmatrix} 25 \\\\ 64 \\end{pmatrix}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52f634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1, 1], [2, 4]])\n",
    "b = np.array([25, 64])\n",
    "\n",
    "x = np.linalg.solve(a, b)\n",
    "print(x)\n",
    "\n",
    "# 답이 맞는지 확인\n",
    "np.allclose(a@x, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0881b880",
   "metadata": {},
   "source": [
    "## matplotlib 모듈 이용하여 그래프 표현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e7bda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72101bc1",
   "metadata": {},
   "source": [
    "### 그래프 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540be1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 10, 11)\n",
    "y = x ** 2 + x + 2 + np.random.randn(11)\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaf1ccb",
   "metadata": {},
   "source": [
    "### 그래프 출력하기\n",
    " - plot함수 (선 그래프), scatter(점 그래프), hist(히스토그램) 등 사용\n",
    "   - 함수의 parameter 혹은 plt의 다른 함수로 그래프 형태 및 설정을 변경 가능\n",
    "   - 기본적으로, x, y에 해당하는 값이 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7bff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y)\n",
    "\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87057fe",
   "metadata": {},
   "source": [
    "### 그래프에 주석 추가 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6efe0a8",
   "metadata": {},
   "source": [
    "* **x, y 축 및 타이틀**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d97084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('X values')\n",
    "plt.ylabel('Y values')\n",
    "plt.title('X-Y relation')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.xlim(0, 20)\n",
    "plt.ylim(0, 200)\n",
    "\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc57596",
   "metadata": {},
   "source": [
    "### plot 함수 parameters\n",
    " - 그래프의 형태에 대한 제어 가능\n",
    " - [plot함수 도큐먼트](https://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca84f30e",
   "metadata": {},
   "source": [
    "* 그래프의 색상 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b34b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'r')\n",
    "plt.plot(x, y, '#ff00ff')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a8b4f6",
   "metadata": {},
   "source": [
    "* 그래프 선스타일 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39731f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, '-.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00aaac3",
   "metadata": {},
   "source": [
    "* 그래프 두께 변경\n",
    " - linewidth 명시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1133486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'm:', linewidth=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435fc55b",
   "metadata": {},
   "source": [
    "* keyword parameter 이용하여 모든 속성 설정\n",
    " - https://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.plot\n",
    " - color\n",
    " - linestyle\n",
    " - marker\n",
    " - markerfacecolor\n",
    " - markersize 등등"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95185b4",
   "metadata": {},
   "source": [
    "### subplot으로 여러 그래프 출력하기\n",
    " - subplot함수로 구획을 구별하여 각각의 subplot에 그래프 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b360a6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(x, y, 'r')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(x, y, 'g')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(y, x, 'k')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(x, np.exp(x), 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067d5a2c",
   "metadata": {},
   "source": [
    "### hist함수 사용\n",
    " - histogram 생성\n",
    " - bins로 historgram bar 개수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795d1700",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randint(1, 100, size=200)\n",
    "print(data)\n",
    "\n",
    "plt.hist(data, bins=20, alpha=0.3)\n",
    "plt.xlabel('value')\n",
    "plt.ylabel('number')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5904a9",
   "metadata": {},
   "source": [
    "#### 연습문제\n",
    " 1. 로또 번호 자동 생성기(함수로)를 만드시오\n",
    " 2. numpy를 이용하여 pi(원주율) 값을 계산하시오\n",
    "  - 몬테 카를로 방법 이용하기\n",
    "  - [파이 값 구하기](http://mathfaculty.fullerton.edu/mathews/n2003/montecarlopi/MonteCarloPiMod/Images/MonteCarloPiMod_gr_25.gif)\n",
    "  - 이미지 출처: [이미지](http://mathfaculty.fullerton.edu/mathews/n2003/montecarlopi/MonteCarloPiMod/Images/MonteCarloPiMod_gr_25.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2eaa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lotto_nums():\n",
    "    return np.random.choice(np.arange(1, 46), size=6, replace=False)\n",
    "    \n",
    "generate_lotto_nums()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911d48ce",
   "metadata": {},
   "source": [
    "pi/4 : 1 = (4분원 안에 생성된 점 개수) : 전체  시도 횟수\n",
    "\n",
    "pi = 4 * (4분원 안에 생성된 점 개수) / 1e7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9914f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = int(1e7)\n",
    "points = np.random.rand(total, 2)\n",
    "4 * np.sum(np.sum(points ** 2, axis=1) < 1) / total\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90ca242",
   "metadata": {},
   "source": [
    "## Series\n",
    "  - pandas의 기본 객체 중 하나\n",
    "  - numpy의 ndarray를 기반으로 인덱싱을 기능을 추가하여 1차원 배열을 나타냄\n",
    "  - index를 지정하지 않을 시, 기본적으로 ndarray와 같이 0-based 인덱스 생성, 지정할 경우 명시적으로 지정된 index를 사용\n",
    "  - 같은 타입의 0개 이상의 데이터를 가질 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c0852c",
   "metadata": {},
   "source": [
    "* data로만 생성하기\n",
    " - index는 기본적으로 0부터 자동적으로 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037e4ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.Series([1, 2, 3])\n",
    "s2 = pd.Series(['a', 'b', 'c'])\n",
    "\n",
    "s1, s2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ab7edf",
   "metadata": {},
   "source": [
    "* data, index함께 명시하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8d27d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy와 다르게 pandas는 인덱스를 명시할 수 있음.\n",
    "s3 = pd.Series([1, 2, 3], ['a', 'b', 'c'])\n",
    "s3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ff833a",
   "metadata": {},
   "source": [
    "* data, index, data type 함께 명시하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26262d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "s4 = pd.Series(np.arange(5), np.arange(100, 105), dtype=np.int32)\n",
    "s4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d5c1f2",
   "metadata": {},
   "source": [
    "#### 인덱스 활용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653f1124",
   "metadata": {},
   "outputs": [],
   "source": [
    "s4.index\n",
    "\n",
    "s4.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935f1dad",
   "metadata": {},
   "source": [
    "1. 인덱스를 통한 데이터 접근"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731dcf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s4[100], s4[104]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54479db2",
   "metadata": {},
   "source": [
    "2. 인덱스를 통한 데이터 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea77c3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "s4[104] = 70\n",
    "s4\n",
    "\n",
    "# 기존에 값이 없었더라도 인덱스를 통해 업데이트 가능\n",
    "s4[105] = 90\n",
    "s4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423267ae",
   "metadata": {},
   "source": [
    "3. 인덱스 재사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04d5834",
   "metadata": {},
   "outputs": [],
   "source": [
    "s5 = pd.Series(np.arange(6), s4.index) # 다른 series에서 index 가져오기 가능\n",
    "s5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4d2ae7",
   "metadata": {},
   "source": [
    "## **Series size, shape, unique, count, value_counts 함수**\n",
    " - size : 개수 반환\n",
    " - shape : 튜플형태로 shape반환\n",
    " - unique: 유일한 값만 ndarray로 반환\n",
    " - count : NaN을 제외한 개수를 반환\n",
    " - mean: NaN을 제외한 평균 \n",
    " - value_counts: NaN을 제외하고 각 값들의 빈도를 반환 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1cfb93",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = pd.Series([1, 1, 2, 1, 2, 2, 2, 1, 1, 3, 3, 4, 5, 6, 7, np.NaN])\n",
    "s\n",
    "\n",
    "s.size\n",
    "\n",
    "s.shape\n",
    "\n",
    "s.unique()\n",
    "\n",
    "s.count()\n",
    "\n",
    "s.mean()   # a = np.array([2, 2, 2, 2, np.NaN])\n",
    "           # a.mean()    =>  값으로 NaN이 나온다.\n",
    "           # series의 mean()은 NaN을 무시한다.\n",
    "\n",
    "s.value_counts()\n",
    "\n",
    "# index를 활용해 멀티플한 값에 접근\n",
    "\n",
    "s[[5, 7, 8]]  # list로 indexing한다.\n",
    "\n",
    "s[[5, 7, 8]].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0921047",
   "metadata": {},
   "source": [
    "#### **head, tail 함수**\n",
    " - head : 상위 n개 출력 기본 5개\n",
    " - tail : 하위 n개 출력 기본 5개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1044f905",
   "metadata": {},
   "outputs": [],
   "source": [
    "s.head()\n",
    "\n",
    "s.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71481e3b",
   "metadata": {},
   "source": [
    "## Series 연산\n",
    "\n",
    "#### Index를 기준으로 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b8db46",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.Series([1, 2, 3, 4], ['a', 'b', 'c', 'd'])\n",
    "s2 = pd.Series([6, 3, 2, 1], ['d', 'c', 'b', 'a'])\n",
    "\n",
    "s1, s2\n",
    "\n",
    "s1 + s2\n",
    "# 같은 index끼리 연산이 이뤄진다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bbfb0c",
   "metadata": {},
   "source": [
    "#### **산술연산**\n",
    " - Series의 경우에도 스칼라와의 연산은 각 원소별로 스칼라와의 연산이 적용\n",
    " - Series와의 연산은 각 인덱스에 맞는 값끼리 연산이 적용\n",
    "   - 이때, 인덱스의 pair가 맞지 않으면, 결과는 NaN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f03f65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# broadcasting\n",
    "s1 ** 2\n",
    "\n",
    "s1 ** s2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c4a9d1",
   "metadata": {},
   "source": [
    "#### **index pair가 맞지 않는 경우**\n",
    " - 해당 index에 대해선 NaN 값 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3256666",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1['k'] = 7\n",
    "s2['e'] = 9\n",
    "\n",
    "s1, s2\n",
    "\n",
    "s1 + s2\n",
    "\n",
    "# index pair가 맞지 않으면 NaN 값 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6afef9",
   "metadata": {},
   "source": [
    " #### **Boolean selection**\n",
    "  - boolean Series가 []와 함께 사용되면 True 값에 해당하는 값만 새로 반환되는 Series객체에 포함됨\n",
    "  - 다중조건의 경우, &(and), |(or)를 사용하여 연결 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c9d62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.Series(np.arange(10), np.arange(10)+1)\n",
    "s1\n",
    "\n",
    "s1[s1 > 5]\n",
    "\n",
    "s1[s1.index % 2 == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d120b7d",
   "metadata": {},
   "source": [
    "## Series 슬라이싱"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad1effa",
   "metadata": {},
   "source": [
    "#### **Series 값 변경**\n",
    "  - 추가 및 업데이트: 인덱스를 이용\n",
    "  - 삭제: drop함수 이용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d343b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(np.arange(100, 105), ['a', 'b', 'c', 'd', 'e'])\n",
    "s\n",
    "\n",
    "s['a'] = 200\n",
    "s\n",
    "\n",
    "s['k'] = 300\n",
    "s\n",
    "\n",
    "s.drop('k')\n",
    "\n",
    "print(s)\n",
    "\n",
    "# drop 함수는 s 자체에는 변화를 주지 않는다.\n",
    "# drop의 parameter 중 inplace=True로 하면 원본에 변화를 준다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e3468a",
   "metadata": {},
   "source": [
    "#### **Slicing**\n",
    " - 리스트, ndarray와 동일하게 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdc319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = pd.Series(np.arange(100, 105), ['a', 'c', 'b', 'd', 'e'])\n",
    "s2\n",
    "\n",
    "s2[1:3]\n",
    "\n",
    "s2['c':'d']\n",
    "# 문자열로 슬라이싱하면 end value까지 포함 시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12884e85",
   "metadata": {},
   "source": [
    "## DataFrame\n",
    "  - Series가 1차원이라면 DataFrame은 2차원으로 확대된 버젼\n",
    "  - Excel spreadsheet이라고 생각하면 이해하기 쉬움\n",
    "  - 2차원이기 때문에 인덱스가 row, column로 구성됨\n",
    "   - row는 각 개별 데이터를, column은 개별 속성을 의미\n",
    "  - Data Analysis, Machine Learning에서 data 변형을 위해 가장 많이 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa0d718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 출처: https://www.kaggle.com/hesh97/titanicdataset-traincsv/data\n",
    "train_data = pd.read_csv('/Users/jihun/Desktop/Fastcampus/Part 01~04) Python/04. 데이터 분석을 위한 Python (Pandas)/data/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8bf1cb",
   "metadata": {},
   "source": [
    "#### head, tail 함수\n",
    " - 데이터 전체가 아닌, 일부(처음부터, 혹은 마지막부터)를 간단히 보기 위한 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd867abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head(n=3)\n",
    "\n",
    "train_data.tail(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981f0de9",
   "metadata": {},
   "source": [
    "#### dataframe 데이터 파악하기\n",
    " - shape 속성 (row, column)\n",
    " - describe 함수 - 숫자형 데이터의 통계치 계산\n",
    " - info 함수 - 데이터 타입, 각 아이템의 개수 등 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02a48a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape\n",
    "\n",
    "train_data.describe()\n",
    "\n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417ba315",
   "metadata": {},
   "source": [
    "#### 인덱스(index)\n",
    " - index 속성\n",
    " - 각 아이템을 특정할 수 있는 고유의 값을 저장\n",
    " - 복잡한 데이터의 경우, 멀티 인덱스로 표현 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1585cd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001b7bcc",
   "metadata": {},
   "source": [
    "#### 컬럼(column)\n",
    " - columns 속성\n",
    " - 각각의 특성(feature)을 나타냄\n",
    " - 복잡한 데이터의 경우, 멀티 컬럼으로 표현 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c00c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e162ab7",
   "metadata": {},
   "source": [
    "## DataFrame 생성하기\n",
    " - 일반적으로 분석을 위한 데이터는 다른 데이터 소스(database, 외부 파일)을 통해 dataframe을 생성\n",
    " - 여기서는 실습을 통해, dummy 데이터를 생성하는 방법을 다룰 예정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948efbec",
   "metadata": {},
   "source": [
    "#### dictionary로 부터 생성하기\n",
    " - dict의 key -> column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208d3568",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'a': 100, 'b': 200, 'c': 300}\n",
    "pd.DataFrame(data, index = [0, 1, 2])\n",
    "\n",
    "data = {'a' : [100, 200, 300], 'b': [1, 2, 3], 'c': [4, 5, 6]}\n",
    "pd.DataFrame(data, index=[100, 200, 300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd2ab2e",
   "metadata": {},
   "source": [
    "#### Series로 부터 생성하기\n",
    " - 각 Series의 인덱스 -> column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9a2733",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.Series([100, 200, 300], ['a', 'b', 'c'])\n",
    "b = pd.Series([101, 202, 303], ['a', 'b', 'c'])\n",
    "c = pd.Series([110, 220, 330], ['a', 'b', 'c'])\n",
    "\n",
    "pd.DataFrame([a, b, c])\n",
    "\n",
    "a = pd.Series(np.random.randint(100,200,50), np.arange(0,50))\n",
    "b = pd.Series(np.random.randint(100,200,50), np.arange(0,50))\n",
    "c = pd.Series(np.random.randint(100,200,50), np.arange(0,50))\n",
    "\n",
    "pd.DataFrame([a, b, c])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3e558d",
   "metadata": {},
   "source": [
    "## csv 데이터로 부터 Dataframe 생성\n",
    " - 데이터 분석을 위해, dataframe을 생성하는 가장 일반적인 방법\n",
    " - 데이터 소스로부터 추출된 csv(comma separated values) 파일로부터 생성\n",
    " - pandas.read_csv 함수 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8728113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 출처: https://www.kaggle.com/hesh97/titanicdataset-traincsv/data\n",
    "train_data = pd.read_csv('/Users/jihun/Desktop/Fastcampus/Part 01~04) Python/04. 데이터 분석을 위한 Python (Pandas)/data/train.csv')\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87e04af",
   "metadata": {},
   "source": [
    "#### read_csv 함수 파라미터\n",
    " - sep - 각 데이터 값을 구별하기 위한 구분자(separator) 설정 \n",
    " - header - header를 무시할 경우, None 설정\n",
    " - index_col - index로 사용할 column 설정\n",
    " - usecols - 실제로 dataframe에 로딩할 columns만 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202dbfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/jihun/Desktop/Fastcampus/Part 01~04) Python/04. 데이터 분석을 위한 Python (Pandas)/data/train.csv'\n",
    "train_data = pd.read_csv(path, index_col='PassengerId', usecols=['PassengerId', 'Survived', 'Pclass', 'Name'])\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e97c25b",
   "metadata": {},
   "source": [
    "## DataFrame 데이터 처리\n",
    "\n",
    "\n",
    "#### column 선택하기\n",
    "  - 기본적으로 [ ]는 column을 추출 \n",
    "  - 컬럼 인덱스일 경우 인덱스의 리스트 사용 가능\n",
    "    - 리스트를 전달할 경우 결과는 Dataframe\n",
    "    - 하나의 컬럼명을 전달할 경우 결과는 Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36908158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 출처: https://www.kaggle.com/hesh97/titanicdataset-traincsv/data\n",
    "train_data = pd.read_csv('/Users/jihun/Desktop/Fastcampus/Part 01~04) Python/04. 데이터 분석을 위한 Python (Pandas)/data/train.csv')\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a3643f",
   "metadata": {},
   "source": [
    "#### 하나의 컬럼 선택하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c9bc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dceec1",
   "metadata": {},
   "source": [
    "#### 복수의 컬럼 선택하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5782b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[['Survived', 'Age', 'Name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b981688",
   "metadata": {},
   "source": [
    "#### dataframe slicing\n",
    "  - dataframe의 경우 기본적으로 [] 연산자가 **column 선택**에 사용\n",
    "  - 하지만, slicing은 row 레벨로 지원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c3ca24",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[:10] # slicing은 row-level\n",
    "                # 기본적으로 df 대괄호는 column-level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db35c4a8",
   "metadata": {},
   "source": [
    "#### row 선택하기\n",
    "  - Seires의 경우 []로 row 선택이 가능하나, **DataFrame의 경우는 기본적으로 column을 선택하도록 설계**\n",
    "  - **.loc, .iloc함수**로 row 선택 가능\n",
    "    - loc - 인덱스 자체를 사용\n",
    "    - iloc - 0 based index로 사용\n",
    "    - 이 두 함수는 ,를 사용하여 column 선택도 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d209834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.index = np.\n",
    "\n",
    "train_data.index = np.arange(100, 991)\n",
    "train_data.head()\n",
    "\n",
    "train_data.loc[986]\n",
    "\n",
    "# '986' index의 데이터를 Series로 가져온다.\n",
    "\n",
    "train_data.iloc[0:3]\n",
    "\n",
    "# 0-based index를 사용해서 데이터를 가져온다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c67940",
   "metadata": {},
   "source": [
    "### loc & iloc\n",
    "\n",
    "- loc과 iloc을 사용하면 row와 column을 모두 선택할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1d5bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.loc[[986, 100, 110, 990], ['Name', 'Survived', 'Sex', 'Age']]\n",
    "\n",
    "# row와 column을 모두 선택할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cda96fc",
   "metadata": {},
   "source": [
    "#### **boolean selection으로 row 선택하기**\n",
    " - numpy에서와 동일한 방식으로 해당 조건에 맞는 row만 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18825b2b",
   "metadata": {},
   "source": [
    "## 30대이면서 1등석에 탄 사람 선택하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa43f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 출처: https://www.kaggle.com/hesh97/titanicdataset-traincsv/data\n",
    "train_data = pd.read_csv('/Users/jihun/Desktop/Fastcampus/Part 01~04) Python/04. 데이터 분석을 위한 Python (Pandas)/data/train.csv')\n",
    "\n",
    "train_data.head()\n",
    "\n",
    "class_ = train_data['Pclass'] == 1\n",
    "age_ = (train_data['Age'] >= 30 & (train_data['Age'] < 40))\n",
    "\n",
    "train_data[class_ & age_]\n",
    "\n",
    "# 30대이면서 1등석에 탄 사람 중 생존한 사람 숫자\n",
    "\n",
    "train_data[class_ & age_]['Survived'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e4d988",
   "metadata": {},
   "source": [
    "#### 새 column 추가하기\n",
    " - [] 사용하여 추가하기\n",
    " - insert 함수 사용하여 원하는 위치에 추가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41098e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# broadcasting\n",
    "train_data['Age'] * 2\n",
    "\n",
    "train_data['Age_double'] = train_data['Age'] * 2\n",
    "train_data.head()\n",
    "\n",
    "train_data.insert(3, 'Fare10', train_data['Fare'] / 10)\n",
    "        # column index 3번에 fare10을 추가하는데, 값은 'Fare'/10\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98742e41",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### column 삭제하기\n",
    " - drop 함수 사용하여 삭제\n",
    "   - 리스트를 사용하여 멀티플 삭제 가능 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b887222",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop('Age_double', axis=1)\n",
    "\n",
    "# 이것도 원본 df를 수정하는 게 아님! 복사본에서 drop하는 거임!\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49efa432",
   "metadata": {},
   "source": [
    "## 변수(column) 사이의 상관계수(correlation) \n",
    " - corr함수를 통해 상관계수 연산 (-1, 1 사이의 결과)\n",
    "   - 연속성(숫자형)데이터에 대해서만 연산\n",
    "   - 인과관계를 의미하진 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f05e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 출처: https://www.kaggle.com/hesh97/titanicdataset-traincsv/data\n",
    "train_data = pd.read_csv('/Users/jihun/Desktop/Fastcampus/Part 01~04) Python/04. 데이터 분석을 위한 Python (Pandas)/data/train.csv')\n",
    "\n",
    "train_data.head()\n",
    "\n",
    "train_data.corr()\n",
    "\n",
    "plt.matshow(train_data.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316b35fb",
   "metadata": {},
   "source": [
    "#### NaN 값 확인\n",
    " - info함수를 통하여 개수 확인\n",
    " - isna함수를 통해 boolean 타입으로 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5baa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()\n",
    "\n",
    "train_data.isna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3af108",
   "metadata": {},
   "source": [
    "#### NaN 처리 방법\n",
    " - 데이터에서 삭제\n",
    "   - dropna 함수 \n",
    " - 다른 값으로 치환\n",
    "   - fillna 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f82bb4",
   "metadata": {},
   "source": [
    "* NaN 데이터 삭제하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cad45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어떤 row에 한 개라도 NaN이 있다면 해당 row를 drop한다.\n",
    "train_data.dropna()\n",
    "\n",
    "# Age가 NaN일 때만 drop한다.\n",
    "train_data.dropna(subset=['Age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2efda7",
   "metadata": {},
   "source": [
    "* NaN 값 대체하기\n",
    " - 평균으로 대체하기\n",
    " - 생존자/사망자 별 평균으로 대체하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a597c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age의 NaN을 모두 Age의 평균으로 대체한다.\n",
    "train_data['Age'].fillna(train_data['Age'].mean())\n",
    "\n",
    "# 생존자 나이 평균\n",
    "mean1 = train_data[train_data['Survived'] == 1]['Age'].mean()\n",
    "\n",
    "# 사망자 나이 평균\n",
    "mean2 = train_data[train_data['Survived'] == 0]['Age'].mean()\n",
    "\n",
    "print(mean1, mean2)\n",
    "\n",
    "# 생존자 NaN 대체\n",
    "train_data[train_data['Survived'] == 1]['Age'].fillna(mean1)\n",
    "\n",
    "# 사망자 NaN 대체\n",
    "train_data[train_data['Survived'] == 0]['Age'].fillna(mean2)\n",
    "\n",
    "#실제로 원본 데이터를 수정하려면...\n",
    "train_data.loc[train_data['Survived'] == 1, 'Age'] = train_data[train_data['Survived'] == 1]['Age'].fillna(mean1)\n",
    "train_data.loc[train_data['Survived'] == 0, 'Age'] = train_data[train_data['Survived'] == 0]['Age'].fillna(mean2)\n",
    "\n",
    "# 수정사항 확인\n",
    "train_data[train_data['Age'] == 28.3436896551724]\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "# data 출처: https://www.kaggle.com/hesh97/titanicdataset-traincsv/data\n",
    "train_data = pd.read_csv('/Users/jihun/Desktop/Fastcampus/Part 01~04) Python/04. 데이터 분석을 위한 Python (Pandas)/data/train.csv')\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed6ba30",
   "metadata": {},
   "source": [
    "## 데이터 타입 이해\n",
    "\n",
    "#### info함수로 각 변수의 데이터 타입 확인\n",
    " - 타입 변경은 astype함수를 사용\n",
    "\n",
    "#### 숫자형(Numerical Type) 데이터\n",
    " - 연속성을 띄는 숫자로 이루어진 데이터\n",
    "   - 예) Age, Fare 등\n",
    "\n",
    "#### 범주형(Categorical Type) 데이터\n",
    " - 연속적이지 않은 값(대부분의 경우 숫자를 제외한 나머지 값)을 갖는 데이터를 의미\n",
    "   - 예) Name, Sex, Ticket, Cabin, Embarked \n",
    " - 어떤 경우, 숫자형 타입이라 할지라도 개념적으로 범주형으로 처리해야할 경우가 있음\n",
    "   - 예) Pclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df13e63e",
   "metadata": {},
   "source": [
    "#### Pclass 변수 변환하기\n",
    " - astype 사용하여 간단히 타입만 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d460c57",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data['Pclass'] = train_data['Pclass'].astype(str)\n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3873fc76",
   "metadata": {},
   "source": [
    "#### Age 변수 변환하기\n",
    " - 변환 로직을 함수로 만든 후, apply 함수로 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca6321c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def age_categorize(age):\n",
    "    if math.isnan(age):\n",
    "        return -1\n",
    "    return math.floor(age /10) * 10\n",
    "\n",
    "train_data['Age'].apply(age_categorize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2916104",
   "metadata": {},
   "source": [
    "## One-hot encoding\n",
    " - 범주형 데이터는 분석단계에서 계산이 어렵기 때문에 숫자형으로 변경이 필요함\n",
    " - 범주형 데이터의 각 범주(category)를 column레벨로 변경\n",
    " - 해당 범주에 해당하면 1, 아니면 0으로 채우는 인코딩 기법\n",
    " - pandas.get_dummies 함수 사용\n",
    "   - drop_first : 첫번째 카테고리 값은 사용하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2c1a8b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data 출처: https://www.kaggle.com/hesh97/titanicdataset-traincsv/data\n",
    "train_data = pd.read_csv('/Users/jihun/Desktop/Fastcampus/Part 01~04) Python/04. 데이터 분석을 위한 Python (Pandas)/data/train.csv')\n",
    "\n",
    "train_data.head()\n",
    "\n",
    "# 모든 범주형 데이터를 더미변수로 만드는 함수\n",
    "pd.get_dummies(train_data, columns=['Pclass', 'Sex', 'Embarked'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf05dbd",
   "metadata": {},
   "source": [
    "## group by\n",
    "  + 아래의 세 단계를 적용하여 데이터를 그룹화(groupping) (SQL의 group by 와 개념적으로는 동일, 사용법은 유사)\n",
    "    - 데이터 분할\n",
    "    - operation 적용\n",
    "    - 데이터 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb2d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 출처: https://www.kaggle.com/hesh97/titanicdataset-traincsv/data\n",
    "df = pd.read_csv('/Users/jihun/Desktop/Fastcampus/Part 01~04) Python/04. 데이터 분석을 위한 Python (Pandas)/data/train.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c05867f",
   "metadata": {},
   "source": [
    "#### GroupBy groups 속성\n",
    " - 각 그룹과 그룹에 속한 index를 dict 형태로 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b218f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_group = df.groupby('Pclass')\n",
    "class_group\n",
    "\n",
    "class_group.groups\n",
    "\n",
    "gender_group = df.groupby('Sex')\n",
    "gender_group.groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36666b7e",
   "metadata": {},
   "source": [
    "#### groupping 함수\n",
    " - 그룹 데이터에 적용 가능한 통계 함수(NaN은 제외하여 연산)\n",
    " - count - 데이터 개수 \n",
    " - sum   - 데이터의 합\n",
    " - mean, std, var - 평균, 표준편차, 분산\n",
    " - min, max - 최소, 최대값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cd16fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_group.count()\n",
    "\n",
    "class_group.mean()['Age']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d93d26",
   "metadata": {},
   "source": [
    "#### 성별에 따른 생존율 구해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180f750d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Sex').mean()['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533747a3",
   "metadata": {},
   "source": [
    "#### 복수 columns로 groupping 하기\n",
    " - groupby에 column 리스트를 전달\n",
    " - 통계함수를 적용한 결과는 multiindex를 갖는 dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738d090b",
   "metadata": {},
   "source": [
    "* 클래스와 성별에 따른 생존률 구해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f7f966",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby(['Pclass', 'Sex']).mean()['Survived']\n",
    "\n",
    "# 1등석과 2등석에 탄 여성이 거의 다 살아 남았다.\n",
    "\n",
    "df.groupby(['Pclass', 'Sex']).mean().loc[(2, 'Age')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdac819",
   "metadata": {},
   "source": [
    "#### index를 이용한 group by\n",
    " - index가 있는 경우, groupby 함수에 level 사용 가능\n",
    "   - level은 index의 depth를 의미하며, 가장 왼쪽부터 0부터 증가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2ce777",
   "metadata": {},
   "source": [
    "* **set_index** 함수\n",
    " - column 데이터를 index 레벨로 변경\n",
    "* **reset_index** 함수\n",
    " - 인덱스 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe7d4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('Age').groupby(level=0).mean()\n",
    "\n",
    "# age를 인덱스로 해놓고 groupby level=0 이라는 건\n",
    "# age로 groupby 하라는 말과 똑같음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e450dfa9",
   "metadata": {},
   "source": [
    "#### 나이대별로 생존율 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662e7448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def age_categorize(age):\n",
    "    if math.isnan(age):\n",
    "        return -1\n",
    "    return math.floor(age / 10) * 10\n",
    "\n",
    "df.set_index('Age').groupby(age_categorize).mean()['Survived']\n",
    "\n",
    "# index가 존재할 때, groupby에 함수를 넣으면 index값이 바뀐다. (함수값으로 그룹핑이 가능한 것이다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b003f7",
   "metadata": {},
   "source": [
    "#### MultiIndex를 이용한 groupping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e83613",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index(['Pclass', 'Sex']).groupby(level=[1, 0]).mean()\n",
    "\n",
    "df.set_index(['Pclass', 'Sex']).groupby(level=[0, 1]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a40c077",
   "metadata": {},
   "source": [
    "#### aggregate(집계) 함수 사용하기\n",
    " - groupby 결과에 집계함수를 적용하여 그룹별 데이터 확인 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfa313d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index(['Pclass', 'Sex']).groupby(level=[0, 1]).aggregate([np.mean, np.sum, np.max])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b39655",
   "metadata": {},
   "source": [
    "## transform 함수\n",
    " - groupby 후 transform 함수를 사용하면 원래의 index를 유지한 상태로 통계함수를 적용\n",
    " - 전체 데이터의 집계가 아닌 각 그룹에서의 집계를 계산\n",
    " - 따라서 새로 생성된 데이터를 원본 dataframe과 합치기 쉬움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3942c019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 출처: https://www.kaggle.com/hesh97/titanicdataset-traincsv/data\n",
    "df = pd.read_csv('/Users/jihun/Desktop/Fastcampus/Part 01~04) Python/04. 데이터 분석을 위한 Python (Pandas)/data/train.csv')\n",
    "\n",
    "df.head()\n",
    "\n",
    "df.groupby('Pclass').mean()\n",
    "\n",
    "df.groupby('Pclass').transform(np.mean)\n",
    "\n",
    "# 원본의 index를 유지하면서.. 즉 원본의 포맷을 유지하면서\n",
    "# groupby aggregate value를 넣을 수 있다.\n",
    "\n",
    "df['Age2'] = df.groupby('Pclass').transform(np.mean)['Age']\n",
    "df\n",
    "\n",
    "# Pclass로 groupby된 mean(age)를 'age2'라는 새로운 칼럼으로 쉽게 추가할 수 있다.\n",
    "\n",
    "df.groupby(['Pclass', 'Sex']).mean()\n",
    "\n",
    "df['Age3'] = df.groupby(['Pclass', 'Sex']).transform(np.mean)['Age']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72f32b6",
   "metadata": {},
   "source": [
    "## Pivot & Pivot Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9a0a9f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    '지역': ['서울', '서울', '서울', '경기', '경기', '부산', '서울', '서울', '부산', '경기', '경기', '경기'],\n",
    "    '요일': ['월요일', '월요일', '수요일', '월요일', '화요일', '월요일', '목요일', '금요일', '화요일', '수요일', '목요일', '금요일'],\n",
    "    '강수량': [100, 80, 1000, 200, 200, 100, 50, 100, 200, 100, 50, 100],\n",
    "    '강수확률': [80, 70, 90, 10, 20, 30, 50, 90, 20, 80, 50, 10]\n",
    "                  })\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ee0edb",
   "metadata": {},
   "source": [
    "#### pivot \n",
    " - dataframe의 형태를 변경\n",
    " - 인덱스, 컬럼, 데이터로 사용할 컬럼을 명시\n",
    " - 중복되는 entry가 있을 경우 오류 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850845d2",
   "metadata": {},
   "source": [
    "#### pivot_table\n",
    " - 기능적으로 pivot과 동일\n",
    " - pivot과의 차이점\n",
    "   - 중복되는 모호한 값이 있을 경우, aggregation 함수 사용하여 값을 채움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c372eb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pivot_table(index='지역', columns='요일', aggfunc=np.mean)\n",
    "\n",
    "# 중복되는 entry가 있을 경우 aggregate를 어떻게 할지 명시할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d211962a",
   "metadata": {},
   "source": [
    "## Stack & Unstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015e3e9e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    '지역': ['서울', '서울', '서울', '경기', '경기', '부산', '서울', '서울', '부산', '경기', '경기', '경기'],\n",
    "    '요일': ['월요일', '화요일', '수요일', '월요일', '화요일', '월요일', '목요일', '금요일', '화요일', '수요일', '목요일', '금요일'],\n",
    "    '강수량': [100, 80, 1000, 200, 200, 100, 50, 100, 200, 100, 50, 100],\n",
    "    '강수확률': [80, 70, 90, 10, 20, 30, 50, 90, 20, 80, 50, 10]\n",
    "                  })\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766f57e5",
   "metadata": {},
   "source": [
    "####  stack & unstack\n",
    " - stack : 컬럼 레벨에서 인덱스 레벨로 dataframe 변경\n",
    "  - 즉, 데이터를 쌓아올리는 개념으로 이해하면 쉬움\n",
    " - unstack : 인덱스 레벨에서 컬럼 레벨로 dataframe 변경\n",
    "  - stack의 반대 operation\n",
    " \n",
    " - 둘은 역의 관계에 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc0c1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.set_index(['지역', '요일'])\n",
    "new_df\n",
    "\n",
    "new_df.unstack(0)   # 기존에 level 0 인덱스였던 '지역'이 column으로 올라간다.\n",
    "\n",
    "new_df.unstack(0).stack(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7eac45",
   "metadata": {},
   "source": [
    "## concat 함수 사용하여 dataframe 병합하기\n",
    " - pandas.concat 함수\n",
    " - 축을 따라 dataframe을 병합 가능\n",
    "   - 기본 axis = 0 -> 행단위 병합"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f760e4e",
   "metadata": {},
   "source": [
    "* column명이 같은 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fe989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'key1' : np.arange(10), 'value1' : np.random.randn(10)})\n",
    "df1\n",
    "\n",
    "df2 = pd.DataFrame({'key1' : np.arange(10), 'value1' : np.random.randn(10)})\n",
    "df2\n",
    "\n",
    "pd.concat([df1, df2])  # column이 똑같은 두 가지가 이어 붙인 모습\n",
    "\n",
    "pd.concat([df1, df2], axis=1)    # column 방향으로 붙임."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f44cbcc",
   "metadata": {},
   "source": [
    "* column 명이 다른 경우 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f42f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pd.concat([df1, df3])   # column이 다를 땐 이렇게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572e2b1b",
   "metadata": {},
   "source": [
    "## Dataframe Merge & Join\n",
    "\n",
    "#### dataframe merge\n",
    " - SQL의 join처럼 특정한 column을 기준으로 병합\n",
    "   - join 방식: how 파라미터를 통해 명시\n",
    "     - inner: 기본값, 일치하는 값이 있는 경우 \n",
    "     - left: left outer join\n",
    "     - right: right outer join\n",
    "     - outer: full outer join\n",
    "     \n",
    " - pandas.merge 함수가 사용됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06a31cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer = pd.DataFrame({'customer_id' : np.arange(6), \n",
    "                    'name' : ['철수', '영희', '길동', '영수', '수민', '동건'], \n",
    "                    '나이' : [40, 20, 21, 30, 31, 18]})\n",
    "\n",
    "customer\n",
    "\n",
    "orders = pd.DataFrame({'customer_id' : [1, 1, 2, 2, 2, 3, 3, 1, 4, 9], \n",
    "                    'item' : ['치약', '칫솔', '이어폰', '헤드셋', '수건', '생수', '수건', '치약', '생수', '케이스'], \n",
    "                    'quantity' : [1, 2, 1, 1, 3, 2, 2, 3, 2, 1]})\n",
    "orders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aa3334",
   "metadata": {},
   "source": [
    "* on \n",
    " - join 대상이 되는 column 명시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05a4a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(customer, orders, on='customer_id', how='left')\n",
    "\n",
    "# how = left 는 주문한 내역이 없는 customer, 즉 on='customer_id'에 해당되지 않는\n",
    "# customer 목록도 반환함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670ea35a",
   "metadata": {},
   "source": [
    "* index 기준으로 join하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8741098a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cust1 = customer.set_index('customer_id')\n",
    "order1 = orders.set_index('customer_id')\n",
    "\n",
    "cust1, order1\n",
    "\n",
    "pd.merge(cust1, order1, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f059981",
   "metadata": {},
   "source": [
    "#### 연습문제\n",
    "1. 가장 많이 팔린 아이템은?\n",
    "2. 영희가 가장 많이 구매한 아이템은?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1bb39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(customer, orders, on='customer_id').groupby('item').sum().sort_values(by='quantity', ascending=False)\n",
    "\n",
    "# 'how' parameter 없으니까 default=inner join\n",
    "\n",
    "pd.merge(customer, orders, on='customer_id').groupby(['name','item']).sum().loc['영희','quantity']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c381307",
   "metadata": {},
   "source": [
    "#### join 함수\n",
    " - 내부적으로 pandas.merge 함수 사용\n",
    " - 기본적으로 index를 사용하여 left join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f62df72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# 현재경로 확인\n",
    "os.getcwd()\n",
    "\n",
    "# /Users/jihun/Desktop/Fastcampus/Part 05~11) Machine Learning/06. 회귀분석/실습코드\n",
    "\n",
    "# 데이터 불러오기\n",
    "boston = pd.read_csv('/Users/jihun/Desktop/Fastcampus/Data/Boston_house.csv')\n",
    "boston.head()\n",
    "\n",
    "# target 제외한 데이터만 뽑기\n",
    "boston_data = boston.drop(['Target'],axis=1)\n",
    "\n",
    "# data 통계 뽑아보기\n",
    "boston_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee24e09",
   "metadata": {},
   "source": [
    "'''\n",
    "타겟 데이터\n",
    "1978 보스턴 주택 가격\n",
    "506개 타운의 주택 가격 중앙값 (단위 1,000 달러)\n",
    "\n",
    "특징 데이터\n",
    "CRIM: 범죄율\n",
    "INDUS: 비소매상업지역 면적 비율\n",
    "NOX: 일산화질소 농도\n",
    "RM: 주택당 방 수\n",
    "LSTAT: 인구 중 하위 계층 비율\n",
    "B: 인구 중 흑인 비율\n",
    "PTRATIO: 학생/교사 비율\n",
    "ZN: 25,000 평방피트를 초과 거주지역 비율\n",
    "CHAS: 찰스강의 경계에 위치한 경우는 1, 아니면 0\n",
    "AGE: 1940년 이전에 건축된 주택의 비율\n",
    "RAD: 방사형 고속도로까지의 거리\n",
    "DIS: 직업센터의 거리\n",
    "TAX: 재산세율\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e531c8",
   "metadata": {},
   "source": [
    "# crim/rm/lstat 세게의 변수로 각각 단순 선형 회귀 분석하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e617970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 변수 설정 target/crim/rm/lstat\n",
    "target = boston[['Target']]\n",
    "crim = boston[['CRIM']]\n",
    "rm = boston[['RM']]\n",
    "lstat = boston[['LSTAT']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7278cda6",
   "metadata": {},
   "source": [
    "## target ~ crim 선형회귀분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac9ad5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crim변수에 상수항추가하기 \n",
    "# 상수항을 추가하지 않으면 y절편값이 계산이 안 된다.\n",
    "    # x변수값 + 상수값 = y값이 되어야 하는 거니까\n",
    "    # crim값 + constant = target 형식으로 fitting하는 거다.\n",
    "crim1 = sm.add_constant(crim,has_constant=\"add\")\n",
    "\n",
    "# sm.OLS 적합시키기\n",
    "model1 = sm.OLS(target, crim1)\n",
    "fitted_model1 = model1.fit()\n",
    "\n",
    "# summary함수통해 결과출력\n",
    "fitted_model1.summary()\n",
    "\n",
    "# y의 총 변동성 중에 x(범죄율)이 설명하는 비율은 약 0.151이다 (R^2)\n",
    "# 범죄율의 회귀계수는 -0.4152 (범죄율이 1 증가할 때, y가 약 0.4 감소한다)\n",
    "# p value는 0에 가까우며, 매우 유의하다.\n",
    "\n",
    "## 회귀 계수 출력\n",
    "fitted_model1.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acf8331",
   "metadata": {},
   "source": [
    "### y_hat=beta0 + beta1 * X 계산해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63eeae3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#회귀 계수 x 데이터(X)\n",
    "\n",
    "# np.dot 즉 벡터 내적으로 계산한다.\n",
    "    # crim1 np.dot fitted_model1.params 는...\n",
    "    # (상수1, x변수)의 행벡터 * (y절편, coefficient)의 열벡터 =   y절편 + (x변수 * coefficient)\n",
    "np.dot(crim1,fitted_model1.params)\n",
    "\n",
    "## predict함수를 통해 yhat구하기\n",
    "pred1 = fitted_model1.predict(crim1)\n",
    "\n",
    "## 직접구한 yhat과 predict함수를 통해 구한 yhat차이\n",
    "np.dot(crim1,fitted_model1.params) - pred1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5879a2dd",
   "metadata": {},
   "source": [
    "## 적합시킨 직선 시각화 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453b36b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.yticks(fontname = \"Arial\")\n",
    "plt.scatter(crim,target,label=\"data\")\n",
    "plt.plot(crim,pred1,label=\"result\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(target,pred1)\n",
    "plt.xlabel(\"real_value\")\n",
    "plt.ylabel(\"pred_value\")\n",
    "plt.show()\n",
    "\n",
    "## residual 시각화 \n",
    "\n",
    "fitted_model1.resid.plot()\n",
    "plt.xlabel(\"residual_number\")\n",
    "plt.show()\n",
    "\n",
    "##잔차의 합계산해보기\n",
    "# 잔차의 합은 거의 0에 수렴한다.\n",
    "\n",
    "np.sum(fitted_model1.resid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c576e89a",
   "metadata": {},
   "source": [
    "## 위와 동일하게 rm변수와 lstat 변수로 각각 단순선형회귀분석 적합시켜보기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4564be55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상수항추가\n",
    "rm1 = sm.add_constant(rm,has_constant=\"add\")\n",
    "lstat1 = sm.add_constant(lstat,has_constant=\"add\")\n",
    "\n",
    "# 회귀모델 적합\n",
    "model2 = sm.OLS(target, rm1)\n",
    "fitted_model2 = model2.fit()\n",
    "\n",
    "model3 = sm.OLS(target, lstat1)\n",
    "fitted_model3 = model3.fit()\n",
    "\n",
    "# rm모델 결과 출력\n",
    "fitted_model2.summary()\n",
    "\n",
    "# lstat모델 결과 출력 \n",
    "fitted_model3.summary()\n",
    "\n",
    "## 각각 yhat_예측하기\n",
    "np.dot(rm1,fitted_model2.params)\n",
    "pred2 = fitted_model2.predict(rm1)\n",
    "\n",
    "np.dot(lstat1,fitted_model3.params)\n",
    "pred3 = fitted_model3.predict(lstat1)\n",
    "\n",
    "## rm모델 시각화\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(rm,target,label=\"data\")\n",
    "plt.plot(rm,pred2,label=\"result\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "## lstat모델 직선 시각화\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(lstat,target,label=\"data\")\n",
    "plt.plot(lstat,pred3,label=\"result\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# rm모델 reisidual 시각화 \n",
    "fitted_model2.resid.plot()\n",
    "plt.xlabel(\"residual_number\")\n",
    "plt.show()\n",
    "\n",
    "# lstat모델 residual시각화\n",
    "fitted_model3.resid.plot()\n",
    "plt.xlabel(\"residual_number\")\n",
    "plt.show()\n",
    "\n",
    "## 세모델의 residual비교 \n",
    "fitted_model1.resid.plot(label=\"crim\")\n",
    "fitted_model2.resid.plot(label=\"rm\")\n",
    "fitted_model3.resid.plot(label=\"lstat\")\n",
    "plt.legend()\n",
    "\n",
    "# resid는.. y - yhat이다.\n",
    "# yhat은 np.dot으로 구할 수도 있고, pred로 구할 수도 있다.\n",
    "fitted_model3.resid == boston['Target'] - np.dot(lstat1,fitted_model3.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e870a597",
   "metadata": {},
   "source": [
    "# 다중선형회귀분석\n",
    "## crim, rm, lstat 세개의 변수를 통해 다중회귀적합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd26f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## bostan data에서 crim, rm, lstat 변수만 뽑아오기 \n",
    "x_data = boston[['CRIM', 'RM', 'LSTAT']]\n",
    "x_data.head()\n",
    "\n",
    "#상수항 추가\n",
    "x_data1 = sm.add_constant(x_data, has_constant='add')\n",
    "\n",
    "# 회귀모델 적합\n",
    "multi_model = sm.OLS(target, x_data1)\n",
    "fitted_multi_model = multi_model.fit()\n",
    "\n",
    "# summary함수를 통해 결과출력\n",
    "fitted_multi_model.summary()\n",
    "\n",
    "# 단순선형회귀분석을 할 때의 R^2 값 3개를 더한 것보다\n",
    "# 다중선형회귀분석의 R^2 값이 작다.\n",
    "    # 그 이유는.. \"3개의 x변수가 각각 설명하는 y값 변동의 비율이 겹친다.\"\n",
    "    # 그래서 각각 변수의 coefficient값도 변화한다.\n",
    "    # 이 현상이 \"다중공선성\"이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c4130a",
   "metadata": {},
   "source": [
    "## 단순선형회귀모델의 회귀계수와 비교 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad26fc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 단순선형회귀모델의 회귀 계수\n",
    "print(fitted_model1.params)\n",
    "print(fitted_model2.params)\n",
    "print(fitted_model3.params)\n",
    "\n",
    "## 다중선형회귀모델의 회귀 계수\n",
    "fitted_multi_model.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e97f59e",
   "metadata": {},
   "source": [
    "## 행렬연산을 통해 beta구하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fc863f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg ##행렬연산을 통해 beta구하기\n",
    "\n",
    "# SSE를 최소화하는 beta 구하는 법은.. SSE 도함수가 0이 되는 값을 구하는 것이었다.\n",
    "    # (X'X)^-1X'y\n",
    "\n",
    "ba = linalg.inv(np.dot(x_data1.T, x_data1))   #(X'X)^-1\n",
    "np.dot(np.dot(ba, x_data1.T),target) # 위에 나온 회귀계수 값과 같다!\n",
    "\n",
    "# y_hat구하기\n",
    "# p.dot(x_data1,fitted_multi_model.params)\n",
    "pred4 = fitted_multi_model.predict(x_data1)   # fitted_multi_model은 적합도 값을 가지고 있는 함수\n",
    "                                              # x_data1의 변수를 이 함수에 집어넣어서 \"y\"값을 예측하라는 말이다.\n",
    "pred4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07d93ac",
   "metadata": {},
   "source": [
    "### residual plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f808e3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_multi_model.resid.plot()\n",
    "plt.xlabel(\"residual_number\")\n",
    "plt.show()\n",
    "\n",
    "fitted_model1.resid.plot(label=\"crim\")\n",
    "fitted_model2.resid.plot(label=\"rm\")\n",
    "fitted_model3.resid.plot(label=\"lstat\")\n",
    "fitted_multi_model.resid.plot(label=\"full\")\n",
    "plt.legend()\n",
    "\n",
    "# full모델이 가장 잔차가 적다 = 변수가 많아지고, R^2가 증가하니까, 당연히 잔차가 가장 적을 수밖에 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012c4830",
   "metadata": {},
   "source": [
    "# 다중 선형 회귀분석 - 실습2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cc4f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data=boston[['CRIM','RM','LSTAT']] ##변수 여러개\n",
    "target = boston[['Target']]\n",
    "x_data.head()\n",
    "\n",
    "x_data1 = sm.add_constant(x_data, has_constant='add')\n",
    "\n",
    "multi_model = sm.OLS(target,x_data1)\n",
    "fitted_multi_model=multi_model.fit()\n",
    "\n",
    "fitted_multi_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67878b2",
   "metadata": {},
   "source": [
    "# crim, rm, lstat, b, tax, age, zn, nox, indus 변수를 통한 다중선형회귀분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211a7f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## bostan data에서 원하는 변수만 뽑아오기 \n",
    "x_data2 = boston[['CRIM', 'RM', 'LSTAT', 'B', 'TAX', 'AGE', 'ZN', 'NOX', 'INDUS']]\n",
    "x_data2.head()\n",
    "\n",
    "# 상수항추기\n",
    "x_data2_ = sm.add_constant(x_data2, has_constant='add')\n",
    "\n",
    "# 회귀모델 적합\n",
    "multi_model2 = sm.OLS(target, x_data2_)\n",
    "fitted_multi_model2 = multi_model2.fit()\n",
    "\n",
    "# 결과 출력\n",
    "fitted_multi_model2.summary()\n",
    "\n",
    "# 세변수만 추가한 모델의 회귀 계수\n",
    "fitted_multi_model.params\n",
    "\n",
    "# full모델의 회귀계수\n",
    "fitted_multi_model2.params\n",
    "\n",
    "# base모델과 full모델의 잔차비교 \n",
    "import matplotlib.pyplot as plt\n",
    "fitted_multi_model.resid.plot(label=\"full\")\n",
    "fitted_multi_model2.resid.plot(label=\"full_add\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5e7c91",
   "metadata": {},
   "source": [
    "# 상관계수/산점도를 통해 다중공선성 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c1b8a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 상관행렬\n",
    "\n",
    "# 행렬로 베타를 구하는 방법은 (X'X)^-1X'y 인데..\n",
    "    # 다중공선성이 심해지면 이론적으로 (X'X)^-1을 구할 수 없다.\n",
    "    \n",
    "x_data2.corr()\n",
    "\n",
    "## 상관행렬 시각화 해서 보기 \n",
    "import seaborn as sns;\n",
    "cmap = sns.light_palette(\"darkgray\", as_cmap=True)\n",
    "sns.heatmap(x_data2.corr(), annot=True, cmap=cmap)\n",
    "plt.show()\n",
    "\n",
    "## 변수별 산점도 시각화\n",
    "sns.pairplot(x_data2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdd8606",
   "metadata": {},
   "source": [
    "# VIF를 통한 다중공선성 확인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a15236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(\n",
    "    x_data2.values, i) for i in range(x_data2.shape[1])]\n",
    "vif[\"features\"] = x_data2.columns\n",
    "vif\n",
    "\n",
    "## nox 변수 제거후(X_data3) VIF 확인 \n",
    "\n",
    "vif = pd.DataFrame()\n",
    "x_data3= x_data2.drop('NOX',axis=1)\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(\n",
    "    x_data3.values, i) for i in range(x_data3.shape[1])]\n",
    "vif[\"features\"] = x_data3.columns\n",
    "vif\n",
    "\n",
    "## RM 변수 제거후(x_data4) VIF 확인 \n",
    "\n",
    "vif = pd.DataFrame()\n",
    "x_data4= x_data3.drop('RM',axis=1)\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(\n",
    "    x_data4.values, i) for i in range(x_data4.shape[1])]\n",
    "vif[\"features\"] = x_data4.columns\n",
    "vif\n",
    "\n",
    "# nox 변수 제거한 데이터(x_data3) 상수항 추가 후 회귀 모델 적합\n",
    "# nox, rm 변수 제거한 데이터(x_data4) 상수항 추가 후 회귀 모델 적합\n",
    "\n",
    "x_data3_ = sm.add_constant(x_data3, has_constant='add')\n",
    "x_data4_ = sm.add_constant(x_data4, has_constant='add')\n",
    "\n",
    "multi_model3 = sm.OLS(target, x_data3_)\n",
    "fitted_multi_model3 = multi_model3.fit()\n",
    "\n",
    "multi_model4 = sm.OLS(target, x_data4_)\n",
    "fitted_multi_model4 = multi_model4.fit()\n",
    "\n",
    "## 회귀모델 결과 비교 \n",
    "fitted_multi_model3.summary()\n",
    "\n",
    "fitted_multi_model4.summary()\n",
    "\n",
    "# RM은 제거하면 안 되는 중요한 변수라는 점을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9256ae23",
   "metadata": {},
   "source": [
    "# 학습 / 검증데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34946b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = x_data2_\n",
    "y = target\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, train_size=0.7, test_size=0.3,random_state = 1)\n",
    "print(train_x.shape, test_x.shape, train_y.shape, test_y.shape)\n",
    "\n",
    "# train_x 회귀모델 적합\n",
    "fit_1 = sm.OLS(train_y, train_x)\n",
    "fit_1 = fit_1.fit()\n",
    "\n",
    "## 검등데이터 에대한 예측값과 true값 비교 \n",
    "plt.plot(np.array(fit_1.predict(test_x)),label=\"pred\")\n",
    "plt.plot(np.array(test_y),label=\"true\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "## x_data3와 x_data4 학습 검증데이터 분할\n",
    "X = x_data3_\n",
    "y = target\n",
    "train_x2, test_x2, train_y2, test_y2 = train_test_split(X, y, train_size=0.7, test_size=0.3,random_state = 1)\n",
    "\n",
    "X = x_data4_\n",
    "y = target\n",
    "train_x3, test_x3, train_y3, test_y3 = train_test_split(X, y, train_size=0.7, test_size=0.3,random_state = 1)\n",
    "\n",
    "# x_data3/x_data4 회귀 모델 적합(fit2,fit3)\n",
    "fit_2 = sm.OLS(train_y2, train_x2)\n",
    "fit_2 = fit_2.fit()\n",
    "\n",
    "fit_3 = sm.OLS(train_y3, train_x3)\n",
    "fit_3 = fit_3.fit()\n",
    "\n",
    "## true값과 예측값 비교 \n",
    "plt.plot(np.array(fit_2.predict(test_x2)),label=\"pred1\")\n",
    "plt.plot(np.array(fit_3.predict(test_x3)),label=\"pred2\")\n",
    "plt.plot(np.array(test_y2),label=\"true\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "## full모델 추가해서 비교 \n",
    "plt.plot(np.array(fit_1.predict(test_x)),label=\"pred\")\n",
    "plt.plot(np.array(fit_2.predict(test_x2)),label=\"pred_vif\")\n",
    "plt.plot(np.array(fit_3.predict(test_x3)),label=\"pred_vif2\")\n",
    "plt.plot(np.array(test_y2),label=\"true\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.array(test_y2['Target']-fit_1.predict(test_x)),label=\"pred_full\")\n",
    "plt.plot(np.array(test_y2['Target']-fit_2.predict(test_x2)),label=\"pred_vif\")\n",
    "plt.plot(np.array(test_y2['Target']-fit_3.predict(test_x3)),label=\"pred_vif2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d991a7d",
   "metadata": {},
   "source": [
    "# MSE를 통한 검증데이터에 대한 성능비교 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d20582c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(y_true=test_y['Target'], y_pred=fit_1.predict(test_x))\n",
    "\n",
    "mean_squared_error(y_true=test_y['Target'], y_pred=fit_2.predict(test_x2))\n",
    "\n",
    "mean_squared_error(y_true=test_y['Target'], y_pred=fit_3.predict(test_x3))\n",
    "\n",
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 현재경로 확인\n",
    "os.getcwd()\n",
    "\n",
    "# 데이터 불러오기\n",
    "corolla = pd.read_csv('/Users/jihun/Desktop/Fastcampus/Data/ToyotaCorolla.csv')\n",
    "corolla.head()\n",
    "\n",
    "# 데이터 수와 변수의 수 확인하기\n",
    "nCar = corolla.shape[0]\n",
    "nVar = corolla.shape[1]\n",
    "print(nCar, nVar)\n",
    "\n",
    "# corolla.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51f8f36",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## 범주형 변수를 이진형 변수로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fb8ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 가변수 생성\n",
    "corolla.Fuel_Type.unique()\n",
    "\n",
    "dummy_p = np.repeat(0,nCar)\n",
    "dummy_d = np.repeat(0,nCar)\n",
    "dummy_c = np.repeat(0,nCar)\n",
    "\n",
    "dummy_p\n",
    "\n",
    "# 인덱스 슬라이싱 후 (binary = 1) 대입\n",
    "p_idx = np.array(corolla.Fuel_Type == 'Petrol')\n",
    "d_idx = np.array(corolla.Fuel_Type == 'Diesel')\n",
    "c_idx = np.array(corolla.Fuel_Type == 'CNG')\n",
    "\n",
    "dummy_p[p_idx] = 1\n",
    "dummy_d[d_idx] = 1\n",
    "dummy_c[c_idx] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc00bf2",
   "metadata": {},
   "source": [
    "## 불필요한 변수 제거 및 가변수 추가\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbaf107",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fuel = pd.DataFrame({'Petrol': dummy_p, 'Diesel': dummy_d, 'CNG': dummy_c})\n",
    "\n",
    "Fuel\n",
    "\n",
    "# data에 id와 model 지우고, 더미변수 추가\n",
    "\n",
    "corolla_ = corolla.drop(['Id', 'Model', 'Fuel_Type'], axis=1, inplace=False)\n",
    "mlr_data = pd.concat((corolla_, Fuel),axis=1)\n",
    "mlr_data.head()\n",
    "\n",
    "# bias 추가\n",
    "mlr_data = sm.add_constant(mlr_data, has_constant='add')\n",
    "mlr_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c409e42",
   "metadata": {},
   "source": [
    "## 설명변수(X), 타켓변수(Y) 분리 및 학습데이터와 평가데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce9a87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = list(mlr_data.columns.difference(['Price']))  # price 빼고 모든 column name list\n",
    "\n",
    "X = mlr_data[feature_columns]\n",
    "y = mlr_data.Price\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, train_size=0.7, test_size=0.3)\n",
    "print(train_x.shape, test_x.shape, train_y.shape, test_y.shape)\n",
    "\n",
    "# Train the MLR / 회귀모델적합\n",
    "full_model = sm.OLS(train_y, train_x)\n",
    "fitted_full_model = full_model.fit()\n",
    "\n",
    "## R2가 높고, 대부분의 변수들이 유의\n",
    "fitted_full_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f76c1a",
   "metadata": {},
   "source": [
    "## VIF를 통한 다중공선성 확인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaac4f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(\n",
    "    mlr_data.values, i) for i in range(mlr_data.shape[1])]\n",
    "vif[\"features\"] = mlr_data.columns\n",
    "vif\n",
    "\n",
    "\n",
    "# 여기서.. Age_08_04 같은 경우는 VIF 가 '무한'이 나왔는데,\n",
    "# summary에서 p-value는 굉장히 유의하게 나왔다. 이런 경우 이 변수를 지우지 않는 게 맞다.\n",
    "\n",
    "# 학습데이터의 잔차 확인\n",
    "res = fitted_full_model.resid\n",
    "\n",
    "# q-q plot -> 선 위에 점이 모두 올라가 있으면 '정규분포'다.\n",
    "    # 내 데이터가 정규분포에 가까운지 보기 위해 사용\n",
    "fig = sm.qqplot(res, fit=True, line='45')\n",
    "\n",
    "pred_y = fitted_full_model.predict(train_x)\n",
    "\n",
    "# residual pattern 확인\n",
    "    # 등분산성 => 잔차의 분산이 일정한지 확인한다.\n",
    "    # e.g. 예측값이 커짐에 따라 분산이 커지는가?\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.scatter(pred_y,res, s=4)\n",
    "plt.xlim(4000,30000)\n",
    "plt.xlim(4000,30000)\n",
    "plt.xlabel('Fitted values')\n",
    "plt.ylabel('Residual')\n",
    "\n",
    "## 검증 데이터에 대한 예측\n",
    "pred_y2 = fitted_full_model.predict(test_x)\n",
    "\n",
    "## 잔차 plot\n",
    "plt.plot(np.array(test_y-pred_y2),label=\"pred_full\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "## MSE 성능\n",
    "mean_squared_error(y_true=test_y, y_pred=pred_y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43704a40",
   "metadata": {},
   "source": [
    "# 변수선택법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d01565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y, feature를 받았을 때 AIC를 return하는 함수\n",
    "\n",
    "def processSubset(X, y, feature_set):\n",
    "            model = sm.OLS(y,X[list(feature_set)]) # Modeling\n",
    "            regr = model.fit() # 모델 학습\n",
    "            AIC = regr.aic # 모델의 AIC\n",
    "            return {\"model\":regr, \"AIC\":AIC}\n",
    "        \n",
    "print(processSubset(X=train_x, y=train_y, feature_set = feature_columns[0:5]))\n",
    "\n",
    "processSubset(X=train_x, y=train_y, feature_set = feature_columns)\n",
    "\n",
    "\n",
    "import time\n",
    "import itertools\n",
    "# getBest: 가장 낮은 AIC를 가지는 모델 선택 및 저장\n",
    "def getBest(X,y,k):\n",
    "    tic = time.time() # 시작시간\n",
    "    results = [] # 결과 저장공간\n",
    "    for combo in itertools.combinations(X.columns.difference(['const']), k): # 각 변수조합을 고려한 경우의 수\n",
    "        combo=(list(combo)+['const'])\n",
    "        \n",
    "        results.append(processSubset(X,y,feature_set=combo))  # 모델링된 것들을 저장\n",
    "    models = pd.DataFrame(results) # 데이터 프레임으로 변환\n",
    "    # 가장 낮은 AIC를 가지는 모델 선택 및 저장\n",
    "    best_model = models.loc[models['AIC'].argmin()] # index\n",
    "    toc = time.time() # 종료시간\n",
    "    print(\"Processed \", models.shape[0], \"models on\", k, \"predictors in\", (toc - tic),\n",
    "          \"seconds.\")\n",
    "    return best_model\n",
    "\n",
    "print(getBest(X=train_x, y=train_y,k=2))\n",
    "\n",
    "# 변수 선택에 따른 학습시간과 저장\n",
    "models = pd.DataFrame(columns=[\"AIC\", \"model\"])\n",
    "tic = time.time()\n",
    "for i in range(1,4):\n",
    "    models.loc[i] = getBest(X=train_x,y=train_y,k=i)\n",
    "toc = time.time()\n",
    "print(\"Total elapsed time:\", (toc-tic), \"seconds.\")\n",
    "\n",
    "models\n",
    "\n",
    "models.loc[3, \"model\"].summary()\n",
    "\n",
    "# 모든 변수들 모델링 한것과 비교 \n",
    "print(\"full model Rsquared: \",\"{:.5f}\".format(fitted_full_model.rsquared))\n",
    "print(\"full model AIC: \",\"{:.5f}\".format(fitted_full_model.aic))\n",
    "print(\"full model MSE: \",\"{:.5f}\".format(fitted_full_model.mse_total))\n",
    "print(\"selected model Rsquared: \",\"{:.5f}\".format(models.loc[3, \"model\"].rsquared))\n",
    "print(\"selected model AIC: \",\"{:.5f}\".format(models.loc[3, \"model\"].aic))\n",
    "print(\"selected model MSE: \",\"{:.5f}\".format(models.loc[3, \"model\"].mse_total))\n",
    "\n",
    "# Plot the result\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.rcParams.update({'font.size': 18, 'lines.markersize': 10})\n",
    "\n",
    "## Mallow Cp\n",
    "plt.subplot(2, 2, 1)\n",
    "Cp= models.apply(lambda row: (row[1].params.shape[0]+(row[1].mse_total-\n",
    "                               fitted_full_model.mse_total)*(train_x.shape[0]-\n",
    "                                row[1].params.shape[0])/fitted_full_model.mse_total\n",
    "                               ), axis=1)\n",
    "plt.plot(Cp)\n",
    "plt.plot(Cp.argmin(), Cp.min(), \"or\")\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('Cp')\n",
    "\n",
    "# adj-rsquared plot\n",
    "# adj-rsquared = Explained variation / Total variation\n",
    "adj_rsquared = models.apply(lambda row: row[1].rsquared_adj, axis=1)\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(adj_rsquared)\n",
    "plt.plot(adj_rsquared.argmax(), adj_rsquared.max(), \"or\")\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('adjusted rsquared')\n",
    "\n",
    "# aic\n",
    "aic = models.apply(lambda row: row[1].aic, axis=1)\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(aic)\n",
    "plt.plot(aic.argmin(), aic.min(), \"or\")\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('AIC')\n",
    "\n",
    "# bic\n",
    "bic = models.apply(lambda row: row[1].bic, axis=1)\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(bic)\n",
    "plt.plot(bic.argmin(), bic.min(), \"or\")\n",
    "plt.xlabel(' # Predictors')\n",
    "plt.ylabel('BIC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327bf309",
   "metadata": {},
   "source": [
    "# 전진선택법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0aa6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "########전진선택법(step=1)\n",
    "\n",
    "def forward(X, y, predictors):\n",
    "    # 데이터 변수들이 미리정의된 predictors에 있는지 없는지 확인 및 분류\n",
    "    remaining_predictors = [p for p in X.columns.difference(['const']) if p not in predictors]\n",
    "    tic = time.time()\n",
    "    results = []\n",
    "    for p in remaining_predictors:\n",
    "        results.append(processSubset(X=X, y= y, feature_set=predictors+[p]+['const']))\n",
    "    # 데이터프레임으로 변환\n",
    "    models = pd.DataFrame(results)\n",
    "\n",
    "    # AIC가 가장 낮은 것을 선택\n",
    "    best_model = models.loc[models['AIC'].argmin()] # index\n",
    "    toc = time.time()\n",
    "    print(\"Processed \", models.shape[0], \"models on\", len(predictors)+1, \"predictors in\", (toc-tic))\n",
    "    print('Selected predictors:',best_model['model'].model.exog_names,' AIC:',best_model[0] )\n",
    "    return best_model\n",
    "\n",
    "#### 전진선택법 모델\n",
    "\n",
    "def forward_model(X,y):\n",
    "    Fmodels = pd.DataFrame(columns=[\"AIC\", \"model\"])\n",
    "    tic = time.time()\n",
    "    # 미리 정의된 데이터 변수\n",
    "    predictors = []\n",
    "    # 변수 1~10개 : 0~9 -> 1~10\n",
    "    for i in range(1, len(X.columns.difference(['const'])) + 1):\n",
    "        Forward_result = forward(X=X,y=y,predictors=predictors)\n",
    "        if i > 1:\n",
    "            if Forward_result['AIC'] > Fmodel_before:\n",
    "                break\n",
    "        Fmodels.loc[i] = Forward_result\n",
    "        predictors = Fmodels.loc[i][\"model\"].model.exog_names\n",
    "        Fmodel_before = Fmodels.loc[i][\"AIC\"]\n",
    "        predictors = [ k for k in predictors if k != 'const']\n",
    "    toc = time.time()\n",
    "    print(\"Total elapsed time:\", (toc - tic), \"seconds.\")\n",
    "\n",
    "    return(Fmodels['model'][len(Fmodels['model'])])\n",
    "\n",
    "Forward_best_model = forward_model(X=train_x, y= train_y)\n",
    "\n",
    "Forward_best_model.aic\n",
    "\n",
    "Forward_best_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2b391f",
   "metadata": {},
   "source": [
    "# 후진소거법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175fce8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## 후진소거법(step=1)\n",
    "def backward(X,y,predictors):\n",
    "    tic = time.time()\n",
    "    results = []\n",
    "    # 데이터 변수들이 미리정의된 predictors 조합 확인\n",
    "    for combo in itertools.combinations(predictors, len(predictors) - 1):\n",
    "        results.append(processSubset(X=X, y= y,feature_set=list(combo)+['const']))\n",
    "    models = pd.DataFrame(results)\n",
    "    # 가장 낮은 AIC를 가진 모델을 선택\n",
    "    best_model = models.loc[models['AIC'].argmin()]\n",
    "    toc = time.time()\n",
    "    print(\"Processed \", models.shape[0], \"models on\", len(predictors) - 1, \"predictors in\",\n",
    "          (toc - tic))\n",
    "    print('Selected predictors:',best_model['model'].model.exog_names,' AIC:',best_model[0] )\n",
    "    return best_model\n",
    "\n",
    "# 후진 소거법 모델\n",
    "def backward_model(X, y):\n",
    "    Bmodels = pd.DataFrame(columns=[\"AIC\", \"model\"], index = range(1,len(X.columns)))\n",
    "    tic = time.time()\n",
    "    predictors = X.columns.difference(['const'])\n",
    "    Bmodel_before = processSubset(X,y,predictors)['AIC']\n",
    "    while (len(predictors) > 1):\n",
    "        Backward_result = backward(X=train_x, y= train_y, predictors = predictors)\n",
    "        if Backward_result['AIC'] > Bmodel_before:\n",
    "            break\n",
    "        Bmodels.loc[len(predictors) - 1] = Backward_result\n",
    "        predictors = Bmodels.loc[len(predictors) - 1][\"model\"].model.exog_names\n",
    "        Bmodel_before = Backward_result['AIC']\n",
    "        predictors = [ k for k in predictors if k != 'const']\n",
    "\n",
    "    toc = time.time()\n",
    "    print(\"Total elapsed time:\", (toc - tic), \"seconds.\")\n",
    "    return (Bmodels['model'].dropna().iloc[0])\n",
    "\n",
    "Backward_best_model = backward_model(X=train_x,y=train_y)\n",
    "\n",
    "Backward_best_model.aic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ced5302",
   "metadata": {},
   "source": [
    "# 단계적 선택법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390cc9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stepwise_model(X,y):\n",
    "    Stepmodels = pd.DataFrame(columns=[\"AIC\", \"model\"])\n",
    "    tic = time.time()\n",
    "    predictors = []\n",
    "    Smodel_before = processSubset(X,y,predictors+['const'])['AIC']\n",
    "    # 변수 1~10개 : 0~9 -> 1~10\n",
    "    for i in range(1, len(X.columns.difference(['const'])) + 1):\n",
    "        Forward_result = forward(X=X, y=y, predictors=predictors) # constant added\n",
    "        print('forward')\n",
    "        Stepmodels.loc[i] = Forward_result\n",
    "        predictors = Stepmodels.loc[i][\"model\"].model.exog_names\n",
    "        predictors = [ k for k in predictors if k != 'const']\n",
    "        Backward_result = backward(X=X, y=y, predictors=predictors)\n",
    "        if Backward_result['AIC']< Forward_result['AIC']:\n",
    "            Stepmodels.loc[i] = Backward_result\n",
    "            predictors = Stepmodels.loc[i][\"model\"].model.exog_names\n",
    "            Smodel_before = Stepmodels.loc[i][\"AIC\"]\n",
    "            predictors = [ k for k in predictors if k != 'const']\n",
    "            print('backward')\n",
    "        if Stepmodels.loc[i]['AIC']> Smodel_before:\n",
    "            break\n",
    "        else:\n",
    "            Smodel_before = Stepmodels.loc[i][\"AIC\"]\n",
    "    toc = time.time()\n",
    "    print(\"Total elapsed time:\", (toc - tic), \"seconds.\")\n",
    "    return (Stepmodels['model'][len(Stepmodels['model'])])\n",
    "\n",
    "Stepwise_best_model=Stepwise_model(X=train_x,y=train_y)\n",
    "\n",
    "Stepwise_best_model.aic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d6d309",
   "metadata": {},
   "source": [
    "# 성능평가 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66f91b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of params\n",
    "print(Forward_best_model.params.shape, Backward_best_model.params.shape, Stepwise_best_model.params.shape)\n",
    "\n",
    "# 모델에 의해 예측된/추정된 값 <->  test_y\n",
    "pred_y_full = fitted_full_model.predict(test_x)\n",
    "pred_y_forward = Forward_best_model.predict(test_x[Forward_best_model.model.exog_names])\n",
    "pred_y_backward = Backward_best_model.predict(test_x[Backward_best_model.model.exog_names])\n",
    "pred_y_stepwise = Stepwise_best_model.predict(test_x[Stepwise_best_model.model.exog_names])\n",
    "\n",
    "perf_mat = pd.DataFrame(columns=[\"ALL\", \"FORWARD\", \"BACKWARD\", \"STEPWISE\"],\n",
    "                        index =['MSE', 'RMSE','MAE', 'MAPE'])\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "from sklearn import metrics\n",
    "\n",
    "# 성능지표\n",
    "perf_mat.loc['MSE']['ALL'] = metrics.mean_squared_error(test_y,pred_y_full)\n",
    "perf_mat.loc['MSE']['FORWARD'] = metrics.mean_squared_error(test_y,pred_y_forward)\n",
    "perf_mat.loc['MSE']['BACKWARD'] = metrics.mean_squared_error(test_y,pred_y_backward)\n",
    "perf_mat.loc['MSE']['STEPWISE'] = metrics.mean_squared_error(test_y,pred_y_stepwise)\n",
    "\n",
    "perf_mat.loc['RMSE']['ALL'] = np.sqrt(metrics.mean_squared_error(test_y, pred_y_full))\n",
    "perf_mat.loc['RMSE']['FORWARD'] = np.sqrt(metrics.mean_squared_error(test_y, pred_y_forward))\n",
    "perf_mat.loc['RMSE']['BACKWARD'] = np.sqrt(metrics.mean_squared_error(test_y, pred_y_backward))\n",
    "perf_mat.loc['RMSE']['STEPWISE'] = np.sqrt(metrics.mean_squared_error(test_y, pred_y_stepwise))\n",
    "\n",
    "perf_mat.loc['MAE']['ALL'] = metrics.mean_absolute_error(test_y, pred_y_full)\n",
    "perf_mat.loc['MAE']['FORWARD'] = metrics.mean_absolute_error(test_y, pred_y_forward)\n",
    "perf_mat.loc['MAE']['BACKWARD'] = metrics.mean_absolute_error(test_y, pred_y_backward)\n",
    "perf_mat.loc['MAE']['STEPWISE'] = metrics.mean_absolute_error(test_y, pred_y_stepwise)\n",
    "\n",
    "perf_mat.loc['MAPE']['ALL'] = mean_absolute_percentage_error(test_y, pred_y_full)\n",
    "perf_mat.loc['MAPE']['FORWARD'] = mean_absolute_percentage_error(test_y, pred_y_forward)\n",
    "perf_mat.loc['MAPE']['BACKWARD'] = mean_absolute_percentage_error(test_y, pred_y_backward)\n",
    "perf_mat.loc['MAPE']['STEPWISE'] = mean_absolute_percentage_error(test_y, pred_y_stepwise)\n",
    "\n",
    "print(perf_mat)\n",
    "\n",
    "print(len(fitted_full_model.params))\n",
    "print(len(Forward_best_model.params))\n",
    "print(len(Backward_best_model.params))\n",
    "print(len(Stepwise_best_model.params))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c40a81d",
   "metadata": {},
   "source": [
    "# 잔차분석 내용 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9667fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# 분석에 필요한 패키지 불러오기\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca0f0a1",
   "metadata": {},
   "source": [
    "# 로지스틱 회귀분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4865444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재경로 확인\n",
    "os.getcwd()\n",
    "\n",
    "# Personal Loan 데이터 불러오기\n",
    "ploan = pd.read_csv('/Users/jihun/Desktop/Fastcampus/Data/Personal Loan.csv')\n",
    "\n",
    "'''\n",
    "Experience 경력\n",
    "Income 수입\n",
    "Famliy 가족단위\n",
    "CCAvg 월 카드사용량 \n",
    "Education 교육수준 (1: undergrad; 2, Graduate; 3; Advance )\n",
    "Mortgage 가계대출\n",
    "Securities account 유가증권계좌유무\n",
    "CD account 양도예금증서 계좌 유무\n",
    "Online 온라인계좌유무\n",
    "CreidtCard 신용카드유무 \n",
    "\n",
    "'''\n",
    "ploan.head()\n",
    "\n",
    "# 의미없는 변수 제거 ID, zip code 제외\n",
    "ploan_processed = ploan.dropna().drop(['ID', 'ZIP Code'], axis=1, inplace=False)\n",
    "\n",
    "# 상수항 추가\n",
    "ploan_processed = sm.add_constant(ploan_processed, has_constant='add')\n",
    "ploan_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba03007",
   "metadata": {},
   "source": [
    "# 설명변수(X), 타켓변수(Y) 분리 및 학습데이터와 평가데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052644ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대출여부: 1 or 0\n",
    "feature_columns = ploan_processed.columns.difference(['Personal Loan'])\n",
    "X = ploan_processed[feature_columns]\n",
    "y = ploan_processed['Personal Loan']\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, stratify=y,train_size=0.7,test_size=0.3,random_state=42)\n",
    "print(train_x.shape, test_x.shape, train_y.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cf2a5c",
   "metadata": {},
   "source": [
    "# 로지스틱회귀모형 모델링 y = f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3da0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 로지스틱 모형 적합 \n",
    "\n",
    "model = sm.Logit(train_y, train_x)\n",
    "results = model.fit(method='newton')\n",
    "\n",
    "results.summary()\n",
    "\n",
    "results.params\n",
    "\n",
    "#회귀계수 출력 \n",
    "\n",
    "## 나이가 한살 많을수록록 대출할 확률이 1.024배 높다.\n",
    "## 수입이 1단위 높을소룩 대출할 확률이 1.05배 높다 \n",
    "## 가족 구성원수가 1많을수록 대출할 확률이 2.13배 높다\n",
    "## 경력이 1단위 높을수록 대출할 확률이 0.99배 높다(귀무가설 채택)\n",
    "# Experience,  Mortgage는 제외할 필요성이 있어보임\n",
    "np.exp(results.params)\n",
    "\n",
    "\n",
    "## y_hat 예측\n",
    "pred_y = results.predict(test_x)\n",
    "pred_y\n",
    "\n",
    "def cut_off(y,threshold):\n",
    "    Y = y.copy() # copy함수를 사용하여 이전의 y값이 변화지 않게 함\n",
    "    Y[Y>threshold]=1\n",
    "    Y[Y<=threshold]=0\n",
    "    return(Y.astype(int))\n",
    "\n",
    "pred_Y = cut_off(pred_y,0.5) # threshold 값을 바꿔가면서 confusion matrix 값이 바뀌는 걸 분석\n",
    "pred_Y\n",
    "\n",
    "# confusion matrix\n",
    "cfmat = confusion_matrix(test_y,pred_Y)\n",
    "print(cfmat)\n",
    "\n",
    "## confusion matrix accuracy계산하기\n",
    "(cfmat[0,0]+cfmat[1,1])/len(pred_Y)\n",
    "\n",
    "## accuracy 구하는 함수\n",
    "def acc(cfmat):\n",
    "    acc = (cfmat[0,0]+cfmat[1,1])/(cfmat[0,0]+cfmat[1,1]+cfmat[0,1]+cfmat[1,0])\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f778e62a",
   "metadata": {},
   "source": [
    "## 임계값(cut-off)에 따른 성능지표 비교\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e5057d",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.arange(0,1,0.1)  # 0에서 1 사이 0.1 간격 range\n",
    "table = pd.DataFrame(columns=['ACC'])\n",
    "for i in threshold:\n",
    "    pred_Y = cut_off(pred_y,i)\n",
    "    cfmat = confusion_matrix(test_y, pred_Y)\n",
    "    table.loc[i] = acc(cfmat)\n",
    "table.index.name='threshold'\n",
    "table.columns.name='performance'\n",
    "table\n",
    "\n",
    "# sklearn ROC 패키지 제공\n",
    "fpr, tpr, thresholds = metrics.roc_curve(test_y, pred_y, pos_label=1)\n",
    "\n",
    "# Print ROC curve\n",
    "plt.plot(fpr,tpr)\n",
    "\n",
    "# Print AUC\n",
    "auc = np.trapz(tpr,fpr)\n",
    "print('AUC:', auc)\n",
    "\n",
    "\n",
    "\n",
    "### Experience, Mortage 변수 제거 \n",
    "feature_columns = list(ploan_processed.columns.difference([\"Personal Loan\", \"Experience\", \"Mortgage\"]))\n",
    "X = ploan_processed[feature_columns]\n",
    "y = ploan_processed['Personal Loan'] # 대출여부: 1 or 0\n",
    "\n",
    "train_x2, test_x2, train_y, test_y = train_test_split(X, y, stratify=y,train_size=0.7,test_size=0.3,random_state=42)\n",
    "print(train_x.shape, test_x.shape, train_y.shape, test_y.shape)\n",
    "\n",
    "## 로지스틱 모델 적합\n",
    "model = sm.Logit(train_y, train_x2)\n",
    "result2 = model.fit(method='newton')\n",
    "\n",
    "#이전 모델과 비교\n",
    "results.summary()\n",
    "\n",
    "result2.summary()\n",
    "\n",
    "## 예측\n",
    "pred_y = result2.predict(test_x2)\n",
    "\n",
    "# threshold 0.5\n",
    "pred_y = cut_off(pred_y,0.5)\n",
    "\n",
    "## accuracy계산\n",
    "cfmat = confusion_matrix(test_y,pred_y)\n",
    "print(acc(cfmat))\n",
    "\n",
    "threshold = np.arange(0,1,0.1)\n",
    "pred_y = result2.predict(test_x2)\n",
    "table = pd.DataFrame(columns=['ACC'])\n",
    "for i in threshold:\n",
    "    pred_y2 = cut_off(pred_y,i)\n",
    "    cfmat = confusion_matrix(test_y, pred_y2)\n",
    "    table.loc[i] = acc(cfmat)\n",
    "table.index.name='threshold'\n",
    "table.columns.name='performance'\n",
    "table\n",
    "\n",
    "# sklearn ROC 패키지 제공\n",
    "fpr, tpr, thresholds = metrics.roc_curve(test_y, pred_y, pos_label=1)\n",
    "\n",
    "# Print ROC curve\n",
    "plt.plot(fpr,tpr)\n",
    "\n",
    "# Print AUC\n",
    "auc = np.trapz(tpr,fpr)\n",
    "print('AUC:', auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a757ab3",
   "metadata": {},
   "source": [
    "# 변수선택법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07c1eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = list(ploan_processed.columns.difference([\"Personal Loan\"]))\n",
    "X = ploan_processed[feature_columns]\n",
    "y = ploan_processed['Personal Loan'] # 대출여부: 1 or 0\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, stratify=y,train_size=0.7,test_size=0.3,random_state=42)\n",
    "print(train_x.shape, test_x.shape, train_y.shape, test_y.shape)\n",
    "\n",
    "def processSubset(X,y, feature_set):\n",
    "            model = sm.Logit(y,X[list(feature_set)])\n",
    "            regr = model.fit()\n",
    "            AIC = regr.aic\n",
    "            return {\"model\":regr, \"AIC\":AIC}\n",
    "        \n",
    "'''\n",
    "전진선택법\n",
    "'''\n",
    "def forward(X, y, predictors):\n",
    "    # 데이터 변수들이 미리정의된 predictors에 있는지 없는지 확인 및 분류\n",
    "    remaining_predictors = [p for p in X.columns.difference(['const']) if p not in predictors]\n",
    "    tic = time.time()\n",
    "    results = []\n",
    "    for p in remaining_predictors:\n",
    "        results.append(processSubset(X=X, y= y, feature_set=predictors+[p]+['const']))\n",
    "    # 데이터프레임으로 변환\n",
    "    models = pd.DataFrame(results)\n",
    "\n",
    "    # AIC가 가장 낮은 것을 선택\n",
    "    best_model = models.loc[models['AIC'].argmin()] # index\n",
    "    toc = time.time()\n",
    "    print(\"Processed \", models.shape[0], \"models on\", len(predictors)+1, \"predictors in\", (toc-tic))\n",
    "    print('Selected predictors:',best_model['model'].model.exog_names,' AIC:',best_model[0] )\n",
    "    return best_model\n",
    "\n",
    "def forward_model(X,y):\n",
    "    Fmodels = pd.DataFrame(columns=[\"AIC\", \"model\"])\n",
    "    tic = time.time()\n",
    "    # 미리 정의된 데이터 변수\n",
    "    predictors = []\n",
    "    # 변수 1~10개 : 0~9 -> 1~10\n",
    "    for i in range(1, len(X.columns.difference(['const'])) + 1):\n",
    "        Forward_result = forward(X=X,y=y,predictors=predictors)\n",
    "        if i > 1:\n",
    "            if Forward_result['AIC'] > Fmodel_before:\n",
    "                break\n",
    "        Fmodels.loc[i] = Forward_result\n",
    "        predictors = Fmodels.loc[i][\"model\"].model.exog_names\n",
    "        Fmodel_before = Fmodels.loc[i][\"AIC\"]\n",
    "        predictors = [ k for k in predictors if k != 'const']\n",
    "    toc = time.time()\n",
    "    print(\"Total elapsed time:\", (toc - tic), \"seconds.\")\n",
    "\n",
    "    return(Fmodels['model'][len(Fmodels['model'])])\n",
    "\n",
    "\n",
    "'''\n",
    "후진소거법\n",
    "'''\n",
    "def backward(X,y,predictors):\n",
    "    tic = time.time()\n",
    "    results = []\n",
    "    \n",
    "    # 데이터 변수들이 미리정의된 predictors 조합 확인\n",
    "    for combo in itertools.combinations(predictors, len(predictors) - 1):\n",
    "        results.append(processSubset(X=X, y= y,feature_set=list(combo)+['const']))\n",
    "    models = pd.DataFrame(results)\n",
    "    \n",
    "    # 가장 낮은 AIC를 가진 모델을 선택\n",
    "    best_model = models.loc[models['AIC'].argmin()]\n",
    "    toc = time.time()\n",
    "    print(\"Processed \", models.shape[0], \"models on\", len(predictors) - 1, \"predictors in\",\n",
    "          (toc - tic))\n",
    "    print('Selected predictors:',best_model['model'].model.exog_names,' AIC:',best_model[0] )\n",
    "    return best_model\n",
    "\n",
    "\n",
    "def backward_model(X, y):\n",
    "    Bmodels = pd.DataFrame(columns=[\"AIC\", \"model\"], index = range(1,len(X.columns)))\n",
    "    tic = time.time()\n",
    "    predictors = X.columns.difference(['const'])\n",
    "    Bmodel_before = processSubset(X,y,predictors)['AIC']\n",
    "    while (len(predictors) > 1):\n",
    "        Backward_result = backward(X=train_x, y= train_y, predictors = predictors)\n",
    "        if Backward_result['AIC'] > Bmodel_before:\n",
    "            break\n",
    "        Bmodels.loc[len(predictors) - 1] = Backward_result\n",
    "        predictors = Bmodels.loc[len(predictors) - 1][\"model\"].model.exog_names\n",
    "        Bmodel_before = Backward_result['AIC']\n",
    "        predictors = [ k for k in predictors if k != 'const']\n",
    "\n",
    "    toc = time.time()\n",
    "    print(\"Total elapsed time:\", (toc - tic), \"seconds.\")\n",
    "    return (Bmodels['model'].dropna().iloc[0])\n",
    "\n",
    "\n",
    "'''\n",
    "단계적 선택법\n",
    "'''\n",
    "def Stepwise_model(X,y):\n",
    "    Stepmodels = pd.DataFrame(columns=[\"AIC\", \"model\"])\n",
    "    tic = time.time()\n",
    "    predictors = []\n",
    "    Smodel_before = processSubset(X,y,predictors+['const'])['AIC']\n",
    "    # 변수 1~10개 : 0~9 -> 1~10\n",
    "    for i in range(1, len(X.columns.difference(['const'])) + 1):\n",
    "        Forward_result = forward(X=X, y=y, predictors=predictors) # constant added\n",
    "        print('forward')\n",
    "        Stepmodels.loc[i] = Forward_result\n",
    "        predictors = Stepmodels.loc[i][\"model\"].model.exog_names\n",
    "        predictors = [ k for k in predictors if k != 'const']\n",
    "        Backward_result = backward(X=X, y=y, predictors=predictors)\n",
    "        if Backward_result['AIC']< Forward_result['AIC']:\n",
    "            Stepmodels.loc[i] = Backward_result\n",
    "            predictors = Stepmodels.loc[i][\"model\"].model.exog_names\n",
    "            Smodel_before = Stepmodels.loc[i][\"AIC\"]\n",
    "            predictors = [ k for k in predictors if k != 'const']\n",
    "            print('backward')\n",
    "        if Stepmodels.loc[i]['AIC']> Smodel_before:\n",
    "            break\n",
    "        else:\n",
    "            Smodel_before = Stepmodels.loc[i][\"AIC\"]\n",
    "    toc = time.time()\n",
    "    print(\"Total elapsed time:\", (toc - tic), \"seconds.\")\n",
    "    return (Stepmodels['model'][len(Stepmodels['model'])])\n",
    "\n",
    "Forward_best_model = forward_model(X=train_x, y= train_y)\n",
    "\n",
    "Backward_best_model = backward_model(X=train_x,y=train_y)\n",
    "\n",
    "Stepwise_best_model = Stepwise_model(X=train_x,y=train_y)\n",
    "\n",
    "pred_y_full = results.predict(test_x) # full model\n",
    "pred_y_forward = Forward_best_model.predict(test_x[Forward_best_model.model.exog_names])\n",
    "pred_y_backward = Backward_best_model.predict(test_x[Backward_best_model.model.exog_names])\n",
    "pred_y_stepwise = Stepwise_best_model.predict(test_x[Stepwise_best_model.model.exog_names])\n",
    "\n",
    "pred_Y_full= cut_off(pred_y_full,0.5)\n",
    "pred_Y_forward = cut_off(pred_y_forward,0.5)\n",
    "pred_Y_backward = cut_off(pred_y_backward,0.5)\n",
    "pred_Y_stepwise = cut_off(pred_y_stepwise,0.5)\n",
    "\n",
    "cfmat_full = confusion_matrix(test_y, pred_Y_full)\n",
    "cfmat_forward = confusion_matrix(test_y, pred_Y_forward)\n",
    "cfmat_backward = confusion_matrix(test_y, pred_Y_backward)\n",
    "cfmat_stepwise = confusion_matrix(test_y, pred_Y_stepwise)\n",
    "\n",
    "print(acc(cfmat_full))\n",
    "print(acc(cfmat_forward))\n",
    "print(acc(cfmat_backward))\n",
    "print(acc(cfmat_stepwise))\n",
    "\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(test_y, pred_y_full, pos_label=1)\n",
    "# Print ROC curve\n",
    "plt.plot(fpr,tpr)\n",
    "# Print AUC\n",
    "auc = np.trapz(tpr,fpr)\n",
    "print('AUC:', auc)\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(test_y, pred_y_forward, pos_label=1)\n",
    "# Print ROC curve\n",
    "plt.plot(fpr,tpr)\n",
    "# Print AUC\n",
    "auc = np.trapz(tpr,fpr)\n",
    "print('AUC:', auc)\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(test_y, pred_y_backward, pos_label=1)\n",
    "# Print ROC curve\n",
    "plt.plot(fpr,tpr)\n",
    "# Print AUC\n",
    "auc = np.trapz(tpr,fpr)\n",
    "print('AUC:', auc)\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(test_y, pred_y_stepwise, pos_label=1)\n",
    "# Print ROC curve\n",
    "plt.plot(fpr,tpr)\n",
    "# Print AUC\n",
    "auc = np.trapz(tpr,fpr)\n",
    "print('AUC:', auc)\n",
    "\n",
    "###성능면에서는 네 모델이 큰 차이가 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367a4390",
   "metadata": {},
   "source": [
    "# Lasso & RIdge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95b707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "\n",
    "ploan_processed = ploan.dropna().drop(['ID','ZIP Code'], axis=1, inplace=False)\n",
    "\n",
    "feature_columns = list(ploan_processed.columns.difference([\"Personal Loan\"]))\n",
    "X = ploan_processed[feature_columns]\n",
    "y = ploan_processed['Personal Loan'] # 대출여부: 1 or 0\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, stratify=y,train_size=0.7,test_size=0.3,random_state=42)\n",
    "print(train_x.shape, test_x.shape, train_y.shape, test_y.shape)\n",
    "\n",
    "## lasso 적합\n",
    "ll = Lasso(alpha=0.01)\n",
    "ll.fit(train_x, train_y)\n",
    "\n",
    "## 회귀 계수 출력\n",
    "ll.coef_\n",
    "\n",
    "# 막상 돌려보면.. 쓸모없는 변수인 mortgage는 살아남고\n",
    "# p value가 낮았던 변수의 회귀계수가 0에 가까워지기도 한다.\n",
    "\n",
    "results.summary()\n",
    "\n",
    "## 예측, confusionmatrix, acc계산\n",
    "pred_y_lasso = ll.predict(test_x)\n",
    "pred_Y_lasso = cut_off(pred_y_lasso, 0.5)\n",
    "\n",
    "cfmat = confusion_matrix(test_y, pred_Y_lasso)\n",
    "print(acc(cfmat))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(test_y, pred_y_lasso, pos_label=1)\n",
    "# Print ROC curve\n",
    "plt.plot(fpr,tpr)\n",
    "# Print AUC\n",
    "auc = np.trapz(tpr,fpr)\n",
    "print('AUC:', auc)\n",
    "\n",
    "## ridge 적합\n",
    "rr = Ridge(alpha=0.01)\n",
    "rr.fit(train_x, train_y)\n",
    "\n",
    "## ridge result\n",
    "rr.coef_\n",
    "\n",
    "## lasso result\n",
    "ll.coef_\n",
    "\n",
    "## ridge y예측, confusion matrix, acc계산\n",
    "pred_y_ridge = rr.predict(test_x)\n",
    "pred_Y_ridge = cut_off(pred_y_ridge, 0.5)\n",
    "\n",
    "cfmat = confusion_matrix(test_y, pred_Y_ridge)\n",
    "print(acc(cfmat))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(test_y, pred_y_ridge, pos_label=1)\n",
    "# Print ROC curve\n",
    "plt.plot(fpr,tpr)\n",
    "# Print AUC\n",
    "auc = np.trapz(tpr,fpr)\n",
    "print('AUC:', auc)\n",
    "\n",
    "# lambda 값에 따른 회귀 계수 / accuracy 계산 \n",
    "alpha = np.logspace(-3, 1, 5)\n",
    "\n",
    "## labmda값 0.001 ~ 10까지 범위 설정\n",
    "alpha\n",
    "\n",
    "data = []\n",
    "acc_table=[]\n",
    "for i, a in enumerate(alpha):\n",
    "    lasso = Lasso(alpha=a).fit(train_x, train_y)\n",
    "    data.append(pd.Series(np.hstack([lasso.intercept_, lasso.coef_])))\n",
    "    pred_y = lasso.predict(test_x) # full model\n",
    "    pred_y= cut_off(pred_y,0.5)\n",
    "    cfmat = confusion_matrix(test_y, pred_y)\n",
    "    acc_table.append((acc(cfmat)))\n",
    "    \n",
    "\n",
    "df_lasso = pd.DataFrame(data, index=alpha).T\n",
    "df_lasso\n",
    "acc_table_lasso = pd.DataFrame(acc_table, index=alpha).T\n",
    "\n",
    "df_lasso\n",
    "\n",
    "acc_table_lasso\n",
    "\n",
    "data = []\n",
    "acc_table=[]\n",
    "for i, a in enumerate(alpha):\n",
    "    ridge = Ridge(alpha=a).fit(train_x, train_y)\n",
    "    data.append(pd.Series(np.hstack([ridge.intercept_, ridge.coef_])))\n",
    "    pred_y = ridge.predict(test_x) # full model\n",
    "    pred_y= cut_off(pred_y,0.5)\n",
    "    cfmat = confusion_matrix(test_y, pred_y)\n",
    "    acc_table.append((acc(cfmat)))\n",
    "\n",
    "    \n",
    "df_ridge = pd.DataFrame(data, index=alpha).T\n",
    "acc_table_ridge = pd.DataFrame(acc_table, index=alpha).T\n",
    "\n",
    "df_ridge\n",
    "\n",
    "acc_table_ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72492b7",
   "metadata": {},
   "source": [
    "## labmda값의 변화에 따른 회귀계수 축소 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451ceda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ax1 = plt.subplot(121)\n",
    "plt.semilogx(df_ridge.T)\n",
    "plt.xticks(alpha)\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "plt.semilogx(df_lasso.T)\n",
    "plt.xticks(alpha)\n",
    "plt.title(\"Lasso\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4df833",
   "metadata": {},
   "source": [
    "# Principal compoenet analysis 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af675481",
   "metadata": {},
   "source": [
    "대부분의 머신러닝을 모듈에 포함하고, 이에 대한 예제와 정보가 담겨있는 웹사이트 참고: https://scikit-learn.org"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e4de52",
   "metadata": {},
   "source": [
    "# 1. 데이터 전처리 및 데이터 파악"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ed132d",
   "metadata": {},
   "source": [
    "- scikit-lean 패키지에서 데이터와 PCA 로드."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bf0c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74cce14",
   "metadata": {},
   "source": [
    "- 자료 처리에 도움을 줄 pandas, numpy와 시각화를 위한 pyplot, seaborn 로드."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec922853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79c34c3",
   "metadata": {},
   "source": [
    "- iris 데이터를 불러오고, 구조를 살핌."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cdecfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "dir(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ab3136",
   "metadata": {},
   "source": [
    "- 설명의 편의를 위하여, 독립변수 중 처음 2개만을 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79aef44",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data[:, [0,2]]\n",
    "y = iris.target\n",
    "\n",
    "print(X.shape)\n",
    "feature_names = [iris.feature_names[0], iris.feature_names[2]]\n",
    "df_X = pd.DataFrame(X)\n",
    "df_X.head()\n",
    "\n",
    "print(y.shape)\n",
    "df_y=pd.DataFrame(y)\n",
    "df_y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0f826a",
   "metadata": {},
   "source": [
    "- 결측치 여부를 파악."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af45ee5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_X.isnull().sum())\n",
    "print(df_y.isnull().sum())\n",
    "\n",
    "print(set(y))\n",
    "print(iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c202bb53",
   "metadata": {},
   "source": [
    "- 종속 변수 (출력변수, 반응변수)의 분포를 살핌."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9188a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y[0].value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb6c2ce",
   "metadata": {},
   "source": [
    "- 독립 변수 (속성, 입력변수, 설명변수)의 분포를 살핌."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ad27df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(df_X.shape[1]):\n",
    "    sns.distplot(df_X[i])\n",
    "    plt.title(feature_names[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eac60c",
   "metadata": {},
   "source": [
    "# 2. PCA 함수 활용 및 아웃풋 의미파악"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3d0c41",
   "metadata": {},
   "source": [
    "- PCA 함수를 활용하여 PC를 얻어냄. 아래의 경우 PC 2개를 뽑아냄."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461721e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8ad3f5",
   "metadata": {},
   "source": [
    "- 아래와 같이 PC score를 얻어냄. 아래의 PC score를 이용하여, 회귀분석에 활용할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daac259",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCscore = pca.transform(X)\n",
    "PCscore[0:5]\n",
    "\n",
    "eigen_v=pca.components_.transpose()\n",
    "eigen_v\n",
    "\n",
    "mX=np.matrix(X)\n",
    "for i in range(X.shape[1]):\n",
    "    mX[:,i]=mX[:,i]-np.mean(X[:,i])\n",
    "dfmX=pd.DataFrame(mX)\n",
    "\n",
    "(mX*eigen_v)[0:5]\n",
    "\n",
    "plt.scatter(PCscore[:,0],PCscore[:,1])\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(dfmX[0],dfmX[1])\n",
    "origin = [0], [0] # origin point\n",
    "plt.quiver(*origin, eigen_v[0,:], eigen_v[1,:], color=['r','b'], scale=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3873d3c6",
   "metadata": {},
   "source": [
    "# 3. PC를 활용한 회귀분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ddf290",
   "metadata": {},
   "source": [
    "- 이번에는 모든 독립변수를 활용하여 PC를 뽑아냄."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e93c033",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = iris.data\n",
    "pca2 = PCA(n_components=4)\n",
    "pca2.fit(X2)\n",
    "\n",
    "pca2.explained_variance_\n",
    "\n",
    "PCs=pca2.transform(X2)[:,0:2]\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8b67bd",
   "metadata": {},
   "source": [
    "- 모델의 복잡성으로 인하여 기존 자료를 이용한 분석은 수렴하지 않는 모습."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586ce443",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(solver=\"sag\", multi_class=\"multinomial\").fit(X2,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffcd7e1",
   "metadata": {},
   "source": [
    "- PC 2개 만을 뽑아내여 분석한 경우 모델이 수렴."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9278492",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = LogisticRegression(solver=\"sag\", multi_class=\"multinomial\").fit(PCs,y)\n",
    "\n",
    "clf2.predict(PCs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d335c2af",
   "metadata": {},
   "source": [
    "- 임의로 변수 2개 만을 뽑아내여 분석한 경우 모델의 퍼포먼스가 하락함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a7e10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(solver='sag', max_iter=1000, random_state=0,\n",
    "                             multi_class=\"multinomial\").fit(X2[:,0:2], y)\n",
    "\n",
    "confusion_matrix(y, clf.predict(X2[:,0:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55c70bc",
   "metadata": {},
   "source": [
    "- 위와 같이, 차원축소를 통하여 모델의 복잡성을 줄이는 동시에 최대한 많은 정보를 활용하여 분석할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35179349",
   "metadata": {},
   "source": [
    "# Naive Bayes 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ca440c",
   "metadata": {},
   "source": [
    "# 1. Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8a4148",
   "metadata": {},
   "source": [
    "- 데이터, 모듈 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062b8713",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "df_X=pd.DataFrame(iris.data)\n",
    "df_Y=pd.DataFrame(iris.target)\n",
    "\n",
    "df_X.head()\n",
    "\n",
    "df_Y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176992b0",
   "metadata": {},
   "source": [
    "- 모델 피팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d1b8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb=GaussianNB()\n",
    "fitted=gnb.fit(iris.data,iris.target)\n",
    "y_pred=fitted\n",
    "\n",
    "fitted.predict_proba(iris.data)[[1,48,51,100]]\n",
    "\n",
    "fitted.predict(iris.data)[[1,48,51,100]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d69e69c",
   "metadata": {},
   "source": [
    "- Confusion matrix 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1ae027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(iris.target,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2ae9b2",
   "metadata": {},
   "source": [
    "- Prior 설정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5a1eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb2=GaussianNB(priors=[1/100,1/100,98/100])\n",
    "fitted2=gnb2.fit(iris.data,iris.target)\n",
    "y_pred2=fitted2.predict(iris.data)\n",
    "confusion_matrix(iris.target,y_pred2)\n",
    "\n",
    "gnb2=GaussianNB(priors=[1/100,98/100,1/100])\n",
    "fitted2=gnb2.fit(iris.data,iris.target)\n",
    "y_pred2=fitted2.predict(iris.data)\n",
    "confusion_matrix(iris.target,y_pred2)\n",
    "\n",
    "# 2번째 범주의 사전확률을 98%로 주니까\n",
    "    # 2번째 범주 예측은 다 맞추는데\n",
    "    # 3번째 범주를 2번째로 예측하는 경우가 많아진다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1a23db",
   "metadata": {},
   "source": [
    "# 2. Multinomial naive bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d43d18f",
   "metadata": {},
   "source": [
    "- 모듈 불러오기 및 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a242aadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X = np.random.randint(5, size=(6, 100))\n",
    "y = np.array([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "X\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7589f2ef",
   "metadata": {},
   "source": [
    "- Multinomial naive bayes 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3116a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(clf.predict(X[2:3]))\n",
    "\n",
    "clf.predict_proba(X[2:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19cba57",
   "metadata": {},
   "source": [
    "- prior 변경해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf56f4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = MultinomialNB(class_prior=[0.1,0.5,0.1,0.1,0.1,0.1])\n",
    "clf2.fit(X, y)\n",
    "\n",
    "clf2.predict_proba(X[2:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69455045",
   "metadata": {},
   "source": [
    "# k-Nearest Neighborhood Algorithm 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de44e4e2",
   "metadata": {},
   "source": [
    "# 1. 데이터, 모듈 불러오기 및 kNN 피팅 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586b36d5",
   "metadata": {},
   "source": [
    "- 함수 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b8f90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors, datasets\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eadc911",
   "metadata": {},
   "source": [
    "- 모델 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3083f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = neighbors.KNeighborsClassifier(5)\n",
    "clf.fit(X,y)\n",
    "\n",
    "y_pred=clf.predict(X)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c93348",
   "metadata": {},
   "source": [
    "# 2.Cross-validation을 활용한 최적의 k찾기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5bb007",
   "metadata": {},
   "source": [
    "- 함수 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c057d0",
   "metadata": {},
   "source": [
    "영상에 나와 있는 \"from sklearn.cross_validation import cross_val_score\" 코드가 아래와 같이 변경되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e29fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28f3c8a",
   "metadata": {},
   "source": [
    "- CV 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e96ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = range(1,100)\n",
    "k_scores= []\n",
    "\n",
    "for k in k_range:\n",
    "    knn=neighbors.KNeighborsClassifier(k)\n",
    "    scores=cross_val_score(knn,X,y,cv=10,scoring=\"accuracy\")\n",
    "    k_scores.append(scores.mean())\n",
    "\n",
    "plt.plot(k_range, k_scores)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Cross-validated accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a299e349",
   "metadata": {},
   "source": [
    "# 2.Weight를 준 kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e02de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 40\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "for weights in ['uniform', 'distance']:\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n",
    "                edgecolor='k', s=20)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.title(\"3-Class classification (k = %i, weights = '%s')\"\n",
    "              % (n_neighbors, weights))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "np.random.seed(0)\n",
    "X = np.sort(5 * np.random.rand(40, 1), axis=0)\n",
    "T = np.linspace(0, 5, 500)[:, np.newaxis]\n",
    "y = np.sin(X).ravel()\n",
    "y[::5] += 1 * (0.5 - np.random.rand(8))\n",
    "\n",
    "knn = neighbors.KNeighborsRegressor(n_neighbors)\n",
    "y_ = knn.fit(X, y).predict(T)\n",
    "\n",
    "n_neighbors = 5\n",
    "\n",
    "for i, weights in enumerate(['uniform', 'distance']):\n",
    "    knn = neighbors.KNeighborsRegressor(n_neighbors, weights=weights)\n",
    "    y_ = knn.fit(X, y).predict(T)\n",
    "\n",
    "    plt.subplot(2, 1, i + 1)\n",
    "    plt.scatter(X, y, c='k', label='data')\n",
    "    plt.plot(T, y_, c='g', label='prediction')\n",
    "    plt.axis('tight')\n",
    "    plt.legend()\n",
    "    plt.title(\"KNeighborsRegressor (k = %i, weights = '%s')\" % (n_neighbors,\n",
    "                                                                weights))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a62d126",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95b2387",
   "metadata": {},
   "source": [
    "# 1. Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f370baec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83fa638",
   "metadata": {},
   "source": [
    "- LDA 를 위한 함수 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860304e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "y = np.array([1, 1, 1, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f318f95",
   "metadata": {},
   "source": [
    "- LDA 모델 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4946cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = LinearDiscriminantAnalysis()\n",
    "clf.fit(X,y)\n",
    "\n",
    "print(clf.predict([[-0.8, -1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bc877b",
   "metadata": {},
   "source": [
    "# 2. Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9b434b",
   "metadata": {},
   "source": [
    "- QDA를 위한 함수 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dac277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2331a257",
   "metadata": {},
   "source": [
    "- QDA 모델 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc632a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf2 = QuadraticDiscriminantAnalysis()\n",
    "clf2.fit(X,y)\n",
    "\n",
    "print(clf2.predict([[-0.8, -1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c88d2d",
   "metadata": {},
   "source": [
    "- LDA, QDA 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6701e215",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred=clf.predict(X)\n",
    "confusion_matrix(y,y_pred)  \n",
    "\n",
    "y_pred2=clf2.predict(X)\n",
    "confusion_matrix(y,y_pred2)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f49aa7",
   "metadata": {},
   "source": [
    "# 3. LDA, QDA의 시각적 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb699b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "h=0.2\n",
    "names = [\"LDA\", \"QDA\"]\n",
    "classifiers = [\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=1)\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)\n",
    "\n",
    "datasets = [make_moons(noise=0.3, random_state=0),\n",
    "            make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "            linearly_separable\n",
    "            ]\n",
    "\n",
    "figure = plt.figure(figsize=(27, 9))\n",
    "i = 1\n",
    "# iterate over datasets\n",
    "for ds in datasets:\n",
    "    # preprocess dataset, split into training and test part\n",
    "    X, y = ds\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # just plot the dataset first\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "    # Plot the training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
    "    # and testing points\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6)\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    i += 1\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "\n",
    "        # Plot the decision boundary. For that, we will assign a color to each\n",
    "        # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "        if hasattr(clf, \"decision_function\"):\n",
    "            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        else:\n",
    "            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "        # Plot also the training points\n",
    "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
    "        # and testing points\n",
    "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "                   alpha=0.6)\n",
    "\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        ax.set_title(name)\n",
    "        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n",
    "                size=15, horizontalalignment='right')\n",
    "        i += 1\n",
    "\n",
    "figure.subplots_adjust(left=.02, right=.98)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e094647",
   "metadata": {},
   "source": [
    "# Decision Tree 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1638d04c",
   "metadata": {},
   "source": [
    "# 1. 함수 익히기 및 모듈 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad396a41",
   "metadata": {},
   "source": [
    "- 함수 익히기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b4b000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "X = [[0, 0], [1, 1]]\n",
    "Y = [0, 1]\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, Y)\n",
    "\n",
    "clf.predict([[1, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bcdf8f",
   "metadata": {},
   "source": [
    "- 모듈 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef79913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "from os import system\n",
    "import pandas as pd\n",
    "\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05e7e9c",
   "metadata": {},
   "source": [
    "- 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc4a6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris=load_iris()\n",
    "\n",
    "iris.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcde8c1a",
   "metadata": {},
   "source": [
    "# 2. 의사결정나무 구축 및 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a22a61",
   "metadata": {},
   "source": [
    "- 트리 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3087e1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=tree.DecisionTreeClassifier()\n",
    "clf=clf.fit(iris.data,iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd56beed",
   "metadata": {},
   "source": [
    "- 트리의 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31942f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data=tree.export_graphviz(clf,out_file=None,\n",
    "                             feature_names=iris.feature_names,\n",
    "                              class_names=iris.target_names,\n",
    "                              filled=True,\n",
    "                              rounded=True,\n",
    "                              special_characters=True\n",
    "                             )\n",
    "\n",
    "graph = graphviz.Source(dot_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb56cee",
   "metadata": {},
   "source": [
    "- 엔트로피를 활용한 트리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc0d921",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1de705e",
   "metadata": {},
   "source": [
    "### Entropy를 이용한 classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd31b9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2=tree.DecisionTreeClassifier(criterion='entropy')\n",
    "\n",
    "clf2=clf2.fit(iris.data, iris.target)\n",
    "\n",
    "dot_data2=tree.export_graphviz(clf2,out_file=None,\n",
    "                             feature_names=iris.feature_names,\n",
    "                              class_names=iris.target_names,\n",
    "                              filled=True,\n",
    "                              rounded=True,\n",
    "                              special_characters=True\n",
    "                             )\n",
    "\n",
    "graph2 = graphviz.Source(dot_data2)\n",
    "\n",
    "graph2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3364fdc8",
   "metadata": {},
   "source": [
    "### 프루닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61be4573",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3=tree.DecisionTreeClassifier(criterion='entropy',max_depth=2)\n",
    "# max_depth 말고도, gini나 entropy의 최저구간을 정한다거나, 나눌 수 있는 가지의 갯수를 정한다거나 할 수도 있다.\n",
    "\n",
    "clf3=clf3.fit(iris.data,iris.target)\n",
    "\n",
    "dot_data3=tree.export_graphviz(clf3,out_file=None,\n",
    "                             feature_names=iris.feature_names,\n",
    "                              class_names=iris.target_names,\n",
    "                              filled=True,\n",
    "                              rounded=True,\n",
    "                              special_characters=True\n",
    "                             )\n",
    "\n",
    "graph3 = graphviz.Source(dot_data3)\n",
    "\n",
    "graph3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e59ab20",
   "metadata": {},
   "source": [
    "### Confusion Matrix 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195f7dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(iris.target,clf.predict(iris.data))\n",
    "\n",
    "# Tree의 경우 training set에 과적합시키면 거의 다 맞춘다.\n",
    "confusion_matrix(iris.target,clf2.predict(iris.data))\n",
    "\n",
    "# 프루닝을 한 경우\n",
    "confusion_matrix(iris.target,clf3.predict(iris.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a94ba0",
   "metadata": {},
   "source": [
    "# 3. Training - Test 구분 및 Confusion matrix 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9960a8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target,\n",
    "                                                    stratify=iris.target, random_state=1)\n",
    "\n",
    "# 데이터가 많지 않은 경우, 특히 희소한 y 범주가 있다면, stratify를 통해서\n",
    "# test set에도 희소한 y 범주가 적당히 포함되도록 할 수 있다.\n",
    "\n",
    "clf4=tree.DecisionTreeClassifier(criterion='entropy')\n",
    "\n",
    "clf4=clf4.fit(X_train,y_train)\n",
    "\n",
    "# training으로 fitting하고, y_test에 대한 confusion matrix\n",
    "    # predict를 하는데, clf4.predict를 한다. 뭐를 가지고? X_test를 가지고 predict한다.\n",
    "confusion_matrix(y_test,clf4.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b4ea9a",
   "metadata": {},
   "source": [
    "# 4. Decision regression tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af820eb9",
   "metadata": {},
   "source": [
    "- 모듈 불러오기 및 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e45d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.sort(5 * rng.rand(80, 1), axis=0)  # 0~1 사이 균등분포 랜덤숫자를 80행1열로 만들고, 각각에 5를 곱하고, sort해준다.\n",
    "y = np.sin(X).ravel()\n",
    "y[::5] += 3 * (0.5 - rng.rand(16))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896220e5",
   "metadata": {},
   "source": [
    "- Regression tree 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f971cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr1=tree.DecisionTreeRegressor(max_depth=2)\n",
    "regr2=tree.DecisionTreeRegressor(max_depth=5)\n",
    "\n",
    "regr1.fit(X,y)\n",
    "\n",
    "regr2.fit(X,y)\n",
    "\n",
    "X_test = np.arange(0.0, 5.0, 0.01)[:,np.newaxis]\n",
    "X_test\n",
    "\n",
    "y_1=regr1.predict(X_test)\n",
    "y_2=regr2.predict(X_test)\n",
    "\n",
    "y_1\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X, y, s=20, edgecolor=\"black\",\n",
    "            c=\"darkorange\", label=\"data\")\n",
    "plt.plot(X_test, y_1, color=\"cornflowerblue\",\n",
    "         label=\"max_depth=2\", linewidth=2)\n",
    "plt.plot(X_test, y_2, color=\"yellowgreen\", label=\"max_depth=5\", linewidth=2)\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"target\")\n",
    "plt.title(\"Decision Tree Regression\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "dot_data4 = tree.export_graphviz(regr2, out_file=None, \n",
    "                                filled=True, rounded=True,  \n",
    "                                special_characters=True)\n",
    "\n",
    "graph4 = graphviz.Source(dot_data4) \n",
    "graph4\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from vega_datasets import data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "movies = data.movies()\n",
    "\n",
    "movies.head(3)\n",
    "\n",
    "movies.dtypes\n",
    "\n",
    "movies.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b35ea3",
   "metadata": {
    "id": "8R1NCrj__P-Z"
   },
   "source": [
    "## Reading large csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd3ff5d",
   "metadata": {
    "id": "qIeJY5pw_QBo"
   },
   "source": [
    "`pd.read_csv(\"csv\", nrows=1000)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d796a6f",
   "metadata": {
    "id": "Mga7PZYq_QEr"
   },
   "source": [
    "## Developing a data analysis routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77f2fe9",
   "metadata": {
    "executionInfo": {
     "elapsed": 519,
     "status": "ok",
     "timestamp": 1692798147630,
     "user": {
      "displayName": "Kang Jihun",
      "userId": "00356180714370444370"
     },
     "user_tz": -540
    },
    "id": "LnFInF1K_QHY"
   },
   "outputs": [],
   "source": [
    "movies = data.movies()\n",
    "\n",
    "# sampling\n",
    "movies.sample(random_state=42)\n",
    "\n",
    "# dimensions\n",
    "movies.shape\n",
    "\n",
    "# info\n",
    "movies.info()\n",
    "\n",
    "# describe\n",
    "movies.describe(include=[np.number]).T\n",
    "\n",
    "movies.describe(include=[np.object]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b967afb",
   "metadata": {
    "id": "FAwuTdZdCCfH"
   },
   "source": [
    "## Create a data dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a5ce5a",
   "metadata": {
    "id": "5Uc5db51CCwB"
   },
   "source": [
    "## Selecting the smallest of the largest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215b7195",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1692798147631,
     "user": {
      "displayName": "Kang Jihun",
      "userId": "00356180714370444370"
     },
     "user_tz": -540
    },
    "id": "ItDnd_9oDwjl",
    "outputId": "dc1bf343-9e78-4092-83d0-355a1e1b2a5e"
   },
   "outputs": [],
   "source": [
    "movies.nlargest(100, 'IMDB_Rating').head(3)\n",
    "\n",
    "# the five lowest budget films among those with a top 100 score\n",
    "movies.nlargest(100, 'IMDB_Rating').nsmallest(\n",
    "    5, 'Production_Budget'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2fcacd",
   "metadata": {
    "id": "R9EQCaqmDwpl"
   },
   "source": [
    "## Selecting the largest of each group by sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833769f4",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1692798147631,
     "user": {
      "displayName": "Kang Jihun",
      "userId": "00356180714370444370"
     },
     "user_tz": -540
    },
    "id": "f0WFdY5tE-9C"
   },
   "outputs": [],
   "source": [
    "movies['Release_Year'] = movies['Release_Date'].str[-4:]\n",
    "\n",
    "movies[['Title', 'Release_Year', 'IMDB_Rating']].head(3)\n",
    "\n",
    "movies[['Title', 'Release_Year', 'IMDB_Rating']].sort_values('Release_Year', ascending=True)\n",
    "\n",
    "movies[['Title', 'Release_Year', 'IMDB_Rating']].sort_values(\n",
    "    ['Release_Year', 'IMDB_Rating'], ascending=False\n",
    "  )\n",
    "\n",
    "# Now we use .drop_duplicates method to keep only the first row of every year\n",
    "movies[['Title', 'Release_Year', 'IMDB_Rating']].sort_values(\n",
    "    ['Release_Year', 'IMDB_Rating'], ascending=False\n",
    "  ).drop_duplicates(subset='Release_Year')\n",
    "\n",
    "# grouping operation to do the same\n",
    "movies[['Title', 'Release_Year', 'IMDB_Rating']]\\\n",
    "  .groupby('Release_Year', as_index=False)\\\n",
    "  .apply(\n",
    "      lambda df:df.sort_values('IMDB_Rating', ascending=False).head(1)\n",
    "  )\\\n",
    "  .droplevel(0)\\\n",
    "  .sort_values('Release_Year', ascending=False)\n",
    "\n",
    "# multiple conditions sorting\n",
    "\n",
    "movies.columns\n",
    "\n",
    "movies[['Title', 'Release_Year', 'MPAA_Rating', 'Production_Budget']]\\\n",
    "  .sort_values(['Release_Year', 'MPAA_Rating', 'Production_Budget'],\n",
    "                ascending = [False, False, True])\\\n",
    "  .drop_duplicates(subset=['Release_Year', 'MPAA_Rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f851b6",
   "metadata": {
    "id": "jTWnPdcsJfRw"
   },
   "source": [
    "## .cummax, .cummin, .mul"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda73bf1",
   "metadata": {
    "id": "kRtrOqT8J2CS"
   },
   "source": [
    "## .assign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b1cbc4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1692798148141,
     "user": {
      "displayName": "Kang Jihun",
      "userId": "00356180714370444370"
     },
     "user_tz": -540
    },
    "id": "W9S1Y06oKRhy",
    "outputId": "5dfd71ea-4409-46dc-9c6a-2c1fc3b70e27"
   },
   "outputs": [],
   "source": [
    "dict_item = {\n",
    "    'item_id' : [1,2,3,4],\n",
    "    'item_name' : ['a','b','c','d'],\n",
    "    'price' : [1000,2000,3000,4000]\n",
    "}\n",
    "\n",
    "df_item = pd.DataFrame(dict_item)\n",
    "\n",
    "df_item_new = df_item.assign(\n",
    "    price=lambda x: x['price'] * 2\n",
    ")\n",
    "\n",
    "print(df_item_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b95f177",
   "metadata": {
    "id": "Z-OZfiUFKRki"
   },
   "source": [
    "## Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7496e4e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1692798148141,
     "user": {
      "displayName": "Kang Jihun",
      "userId": "00356180714370444370"
     },
     "user_tz": -540
    },
    "id": "SPaAOh50KRnk",
    "outputId": "0d031029-ba1f-4de7-a1c2-a3937b3b2590"
   },
   "outputs": [],
   "source": [
    "movies.mean()\n",
    "\n",
    "movies.std()\n",
    "\n",
    "movies.quantile([0,0.25,0.5,0.75,1])\n",
    "\n",
    "movies.describe(include=object)\n",
    "\n",
    "movies.dtypes.value_counts()\n",
    "\n",
    "movies.MPAA_Rating.nunique()\n",
    "\n",
    "movies.Title.nunique()\n",
    "\n",
    "movies['MPAA_Rating'].info(memory_usage='deep')\n",
    "\n",
    "movies[['MPAA_Rating']]\\\n",
    "  .assign(MPAA_Rating = movies.MPAA_Rating.astype('category'))\\\n",
    "  .info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957fb50f",
   "metadata": {
    "id": "DAb8flSVJ2bZ"
   },
   "source": [
    "## Categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86617e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1692798148848,
     "user": {
      "displayName": "Kang Jihun",
      "userId": "00356180714370444370"
     },
     "user_tz": -540
    },
    "id": "GmFLdAM0Ne1h",
    "outputId": "2ee4edb8-f425-4155-ffa8-f340ecb5155c"
   },
   "outputs": [],
   "source": [
    "movies.select_dtypes(object).columns\n",
    "\n",
    "movies.Distributor.nunique()\n",
    "\n",
    "movies.Distributor.sample(5, random_state=42)\n",
    "\n",
    "top_n = movies.Distributor.value_counts().index[:6]\n",
    "top_n\n",
    "\n",
    "movies.assign(\n",
    "    Distributor = movies.Distributor.where(\n",
    "        movies.Distributor.isin(top_n), 'Other'\n",
    "    )\n",
    ").Distributor.value_counts()\n",
    "\n",
    "## movies.Distributor.where() --> only change the value if cond is False\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "movies.assign(\n",
    "    Distributor = movies.Distributor.where(\n",
    "        movies.Distributor.isin(top_n), 'Other'\n",
    "    )\n",
    ").Distributor.value_counts()\\\n",
    " .plot.bar(ax=ax)\n",
    "\n",
    "# use seaborn\n",
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.countplot(\n",
    "    y='Distributor',\n",
    "    data=(\n",
    "        movies.assign(\n",
    "            Distributor = movies.Distributor.where(\n",
    "                movies.Distributor.isin(top_n), 'Other'\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a61fcc",
   "metadata": {
    "id": "ibdFYCxKPZpx"
   },
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ca96f3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 694
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1692798151765,
     "user": {
      "displayName": "Kang Jihun",
      "userId": "00356180714370444370"
     },
     "user_tz": -540
    },
    "id": "5PHMLenyOmDI",
    "outputId": "b772c27c-856c-4ff2-dfdb-ba269911f2f9"
   },
   "outputs": [],
   "source": [
    "movies[movies.MPAA_Rating.isna()]\n",
    "\n",
    "movies.MPAA_Rating.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0261ad44",
   "metadata": {
    "id": "WXcCPRpzOmJB"
   },
   "source": [
    "## applymap, pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ffc00b",
   "metadata": {
    "id": "vs0gaBX_OmMR"
   },
   "source": [
    "- apply\n",
    "  - input : 시리즈 원소 / 시리즈 객체 (데이터프레임의 행/열)\n",
    "  - return : 단일 값, 시리즈\n",
    "- applymap\n",
    "  - input : 데이터프레임 원소\n",
    "  - return : 동일 형태의 데이터프레임\n",
    "- pipe\n",
    "  - input : 데이터프레임 객체\n",
    "  - return : 단일 값, 시리즈, 데이터프레임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59e1ca9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "executionInfo": {
     "elapsed": 394,
     "status": "ok",
     "timestamp": 1692798152697,
     "user": {
      "displayName": "Kang Jihun",
      "userId": "00356180714370444370"
     },
     "user_tz": -540
    },
    "id": "NSvoWnUKOmPf",
    "outputId": "02ce33cd-553f-4569-de25-a1e6d5c61490"
   },
   "outputs": [],
   "source": [
    "df = sns.load_dataset('titanic')\n",
    "df.head(3)\n",
    "\n",
    "def count_missing(vec):\n",
    "  bool_df = vec.isnull()\n",
    "  sum_df = np.sum(bool_df)\n",
    "  return sum_df\n",
    "\n",
    "df.apply(count_missing)\n",
    "\n",
    "df.pipe(count_missing)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f78a5b8",
   "metadata": {
    "id": "z0JzbXxtOmVF"
   },
   "source": [
    "## Comparing continuous values across categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c199706a",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1692798153389,
     "user": {
      "displayName": "Kang Jihun",
      "userId": "00356180714370444370"
     },
     "user_tz": -540
    },
    "id": "ILJMw3xeOmbB"
   },
   "outputs": [],
   "source": [
    "mask = movies.MPAA_Rating.isin(\n",
    "    ['R', 'PG', 'PG-13', 'G']\n",
    ")\n",
    "\n",
    "movies[mask].groupby('MPAA_Rating').Production_Budget.agg(\n",
    "    ['mean', 'std']\n",
    ")\n",
    "\n",
    "g = sns.catplot(\n",
    "    x='MPAA_Rating', y='Production_Budget', data=movies[mask], kind='box'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb6184e",
   "metadata": {
    "id": "_ANuOGH9Omny"
   },
   "source": [
    "box plot does not reveal how many samples are in each MPAA_Rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abd7a84",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1692798154818,
     "user": {
      "displayName": "Kang Jihun",
      "userId": "00356180714370444370"
     },
     "user_tz": -540
    },
    "id": "ahoJCZPLOmsK",
    "outputId": "d032faa0-9550-4769-9b41-3a883adaabcb"
   },
   "outputs": [],
   "source": [
    "movies[mask].groupby('MPAA_Rating').Production_Budget.count()\n",
    "\n",
    "# another option : swarm plot on top of the box plot\n",
    "g = sns.catplot(\n",
    "    x='MPAA_Rating', y='Production_Budget', data=movies[mask], kind='box'\n",
    ")\n",
    "\n",
    "sns.swarmplot(\n",
    "    x='MPAA_Rating',\n",
    "    y='Production_Budget',\n",
    "    data=movies[mask],\n",
    "    color='k',\n",
    "    size=1,\n",
    "    ax=g.ax\n",
    ")\n",
    "\n",
    "# catplot usages\n",
    "g = sns.catplot(\n",
    "    x = 'MPAA_Rating',\n",
    "    y = 'Production_Budget',\n",
    "    data = movies[mask],\n",
    "    kind = 'box',\n",
    "    col = 'Release_Year',\n",
    "    col_order = ['2002', '2004', '2005', '2006'],\n",
    "    col_wrap=2\n",
    ")\n",
    "\n",
    "g = sns.catplot(\n",
    "    x='MPAA_Rating',\n",
    "    y='Production_Budget',\n",
    "    data=movies[mask],\n",
    "    kind='box',\n",
    "    hue='Release_Year',\n",
    "    hue_order=['2002', '2004', '2005', '2006']\n",
    ")\n",
    "\n",
    "movies[mask]\\\n",
    ".groupby('MPAA_Rating')\\\n",
    ".Production_Budget.agg(['mean', 'std']).astype(int)\\\n",
    ".style.background_gradient(cmap='RdBu', axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a64f772",
   "metadata": {
    "id": "05b0a3lhUNGZ"
   },
   "source": [
    "## Comparing two continuous columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b501eb29",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1692798180587,
     "user": {
      "displayName": "Kang Jihun",
      "userId": "00356180714370444370"
     },
     "user_tz": -540
    },
    "id": "nHXH2lt6Uf_c",
    "outputId": "8fb90376-a29d-4edc-8b79-0336d20f4ac6"
   },
   "outputs": [],
   "source": [
    "# look at the covariance, if they are on the same scale\n",
    "movies.Production_Budget.cov(movies.US_Gross)\n",
    "\n",
    "# pearson correlation\n",
    "movies.Production_Budget.corr(movies.US_Gross)\n",
    "\n",
    "# visualize the correlations\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "corr = movies[['Production_Budget', 'US_Gross', 'Worldwide_Gross']].corr()\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "sns.heatmap(\n",
    "    corr,\n",
    "    mask=mask,\n",
    "    fmt='.2f',\n",
    "    annot=True,\n",
    "    ax=ax,\n",
    "    cmap='RdBu',\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    square=True\n",
    ")\n",
    "\n",
    "# use scatterplot\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "movies.plot.scatter(\n",
    "    x='Production_Budget', y='US_Gross', alpha=0.1, ax=ax\n",
    ")\n",
    "\n",
    "# seaborn regression\n",
    "res = sns.lmplot(\n",
    "    x='Production_Budget', y='US_Gross', data=movies\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54637db",
   "metadata": {
    "id": "KajKlcIkXrEb"
   },
   "source": [
    "## Comparing categorical values with categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d83d86",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1692798182236,
     "user": {
      "displayName": "Kang Jihun",
      "userId": "00356180714370444370"
     },
     "user_tz": -540
    },
    "id": "jVlidtlDYbxL"
   },
   "outputs": [],
   "source": [
    "def generalize(ser, match_name, default):\n",
    "  seen = None\n",
    "  for match, name in match_name:\n",
    "    mask = ser.str.contains(match)\n",
    "    if seen is None:\n",
    "      seen = Mask\n",
    "    else:\n",
    "      seen |= mask\n",
    "    ser = ser.where(~mask, name)\n",
    "  ser = ser.where(seen, default)\n",
    "  return ser\n",
    "\n",
    "dists = movies.Distributor.value_counts().index[:6]\n",
    "\n",
    "movies.Distributor.where(movies.Distributor.isin(dists), 'Other')\n",
    "\n",
    "# Cramer's V measure\n",
    "def cramers_v(x, y):\n",
    "  confusion_matrix = pd.crosstab(x, y)\n",
    "  chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "  n = confusion_matrix.sum().sum()\n",
    "  phi2 = chi2 / n\n",
    "  r, k = confusion_matrix.shape\n",
    "  phi2corr = max(\n",
    "      0, phi2 - ((k - 1) * (r - 1)) / (n - 1)\n",
    "  )\n",
    "  rcorr = r - ((r - 1) ** 2) / (n - 1)\n",
    "  kcorr = k - ((k - 1) ** 2) / (n - 1)\n",
    "  return np.sqrt(\n",
    "      phi2corr / min((kcorr - 1), rcorr - 1)\n",
    "  )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbc54fd",
   "metadata": {
    "id": "r6PE2IjQXzQu"
   },
   "source": [
    "## .size(), .unstack(), .stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943b7d58",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1692798285300,
     "user": {
      "displayName": "Kang Jihun",
      "userId": "00356180714370444370"
     },
     "user_tz": -540
    },
    "id": "yRHMKUkQXzTx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad6af5b4",
   "metadata": {
    "id": "fwzQwS7lXzW9"
   },
   "source": [
    "## Pandas Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe6184e",
   "metadata": {
    "id": "9zVh7d4tbdEA"
   },
   "outputs": [],
   "source": [
    "import pandas_profiling as pp\n",
    "pp.ProfileReport(movies)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b9b680",
   "metadata": {
    "id": "lFRspL-9cCKP"
   },
   "source": [
    "## Selecting data with both integers and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dbde21",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1692798296581,
     "user": {
      "displayName": "Kang Jihun",
      "userId": "00356180714370444370"
     },
     "user_tz": -540
    },
    "id": "RCsXzaMYcCQV",
    "outputId": "7e30a324-1a41-4c83-ff34-0ce0c973a35a"
   },
   "outputs": [],
   "source": [
    "col_start = movies.columns.get_loc('Production_Budget')\n",
    "col_end = movies.columns.get_loc('MPAA_Rating') + 1\n",
    "\n",
    "col_start, col_end\n",
    "\n",
    "movies.iloc[:3, col_start:col_end]\n",
    "\n",
    "row_start = movies.index[0]\n",
    "row_end = movies.index[2]\n",
    "movies.loc[row_start:row_end, 'Production_Budget':'MPAA_Rating']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac797306",
   "metadata": {
    "id": "aMik3GsCd7Dm"
   },
   "source": [
    "## Calculating boolean statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6a0084",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1692798596875,
     "user": {
      "displayName": "Kang Jihun",
      "userId": "00356180714370444370"
     },
     "user_tz": -540
    },
    "id": "3oZklW9mpFVx",
    "outputId": "183f4d5b-4af8-49a3-8af8-d84d8570eb8d"
   },
   "outputs": [],
   "source": [
    "votes_15t = movies['IMDB_Votes'] > 15000\n",
    "votes_15t\n",
    "\n",
    "votes_15t.sum()\n",
    "\n",
    "votes_15t.mean() * 100\n",
    "\n",
    "# greater than\n",
    "movies['IMDB_Votes'].dropna().gt(15000).mean() * 100\n",
    "\n",
    "votes_15t.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cc5af7",
   "metadata": {
    "id": "lW9mkWplpGNK"
   },
   "source": [
    "## Constructing multiple boolean conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef5bbd9",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1692798832372,
     "user": {
      "displayName": "Kang Jihun",
      "userId": "00356180714370444370"
     },
     "user_tz": -540
    },
    "id": "UinF0VEOpGKP"
   },
   "outputs": [],
   "source": [
    "crit1 = movies.IMDB_Rating > 8\n",
    "crit2 = movies.MPAA_Rating == \"PG-13\"\n",
    "crit3 = (movies.Release_Year.astype(int) > 1990) & (movies.Release_Year.astype(int) <= 2022)\n",
    "\n",
    "crit_final = crit1 & crit2 & crit3\n",
    "\n",
    "crit_final.mean()\n",
    "\n",
    "movies[crit_final].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf222e67",
   "metadata": {
    "id": "73G3MreurNgP"
   },
   "source": [
    "## Improving the readability of boolean indexing with the query method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d7d4d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1692799492330,
     "user": {
      "displayName": "Kang Jihun",
      "userId": "00356180714370444370"
     },
     "user_tz": -540
    },
    "id": "a-oVay4kteEO",
    "outputId": "83e112d9-3abd-4722-96d6-b92273f1b8a6"
   },
   "outputs": [],
   "source": [
    "dists = movies.Distributor.value_counts().index[:3]\n",
    "dists\n",
    "\n",
    "select_columns = ['Title', 'US_Gross', 'Production_Budget', 'MPAA_Rating']\n",
    "\n",
    "qs = (\n",
    "    \"Distributor in @dists \"\n",
    "    \" and MPAA_Rating == 'PG-13' \"\n",
    "    \" and 500000 <= Production_Budget <= 5000000\"\n",
    ")\n",
    "\n",
    "movies_filtered = movies.query(qs)\n",
    "movies_filtered\n",
    "\n",
    "movies_filtered[select_columns]\n",
    "\n",
    "# more..\n",
    "qs = \"Distributor not in @dists and MPAA_Rating == 'PG-13'\"\n",
    "movies_filtered2 = movies.query(qs)\n",
    "movies_filtered2[select_columns].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e2e358",
   "metadata": {
    "id": "DTvfdBCXrRGQ"
   },
   "source": [
    "## Masking DataFrame rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01f431c",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1692799872571,
     "user": {
      "displayName": "Kang Jihun",
      "userId": "00356180714370444370"
     },
     "user_tz": -540
    },
    "id": "6cz7fCYyrRJa"
   },
   "outputs": [],
   "source": [
    "c1 = movies['Release_Year'].astype(int) >= 2010\n",
    "c2 = movies['Release_Year'].isna()\n",
    "criteria = c1 | c2\n",
    "\n",
    "movies.mask(criteria).Release_Year.iloc[5:15]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7227aeb",
   "metadata": {
    "id": "9WytPTDIrRS5"
   },
   "source": [
    "## 주성분 분석\n",
    " - 수치형 데이터 추출\n",
    " - 정규화 후 주성분분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2436cde4",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1692800161957,
     "user": {
      "displayName": "Kang Jihun",
      "userId": "00356180714370444370"
     },
     "user_tz": -540
    },
    "id": "cPbKZ4BLrRV8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "iris = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "iris['Class'] = load_iris().target\n",
    "iris['Class'] = iris['Class'].map({0:'Setosa', 1:'Versicolour', 2:'Virginica'})\n",
    "\n",
    "X = iris.drop(columns = 'Class')\n",
    "\n",
    "# 정규화\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "pd.DataFrame(X).head()\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 4)\n",
    "pca_fit = pca.fit(X)\n",
    "\n",
    "print('고윳값: ', pca_fit.singular_values_)\n",
    "print('분산 설명력: ', pca.explained_variance_ratio_)\n",
    "\n",
    "# scree plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.plot(pca.explained_variance_ratio_, 'o-')\n",
    "plt.show()\n",
    "\n",
    "# 주성분 수를 정하고 다시 수행\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "principalComponents = pca.fit_transform(X)\n",
    "principal_iris = pd.DataFrame(data = principalComponents,\n",
    "                              columns = ['pc1', 'pc2'])\n",
    "principal_iris.head()\n",
    "\n",
    "# 산포도 확인\n",
    "import seaborn as sns\n",
    "\n",
    "plt.title('2 component PCA')\n",
    "sns.scatterplot(x = 'pc1', y = 'pc2', hue = iris.Class,\n",
    "                data = principal_iris)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f4ed58",
   "metadata": {
    "id": "qnTlGABNyb7c"
   },
   "source": [
    "## 언더 샘플링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdaa639",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1692800897111,
     "user": {
      "displayName": "Kang Jihun",
      "userId": "00356180714370444370"
     },
     "user_tz": -540
    },
    "id": "Tma2ZVvUyrB8",
    "outputId": "2807f7f3-b086-4bbc-f2a8-edbc92bfa0e2"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "x, y = make_classification(n_samples=2000, n_features=6, weights=[0.95], flip_y=0)\n",
    "\n",
    "print(Counter(y))\n",
    "\n",
    "undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "x_under, y_under = undersample.fit_resample(x, y)\n",
    "\n",
    "print(Counter(y_under))\n",
    "\n",
    "undersample = RandomUnderSampler(sampling_strategy=0.5)\n",
    "x_under2, y_under2 = undersample.fit_resample(x, y)\n",
    "\n",
    "print(Counter(y_under2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03df29f4",
   "metadata": {
    "id": "WFJfGb0dyrVq"
   },
   "source": [
    "## 오버 샘플링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd71ef9b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1692801108475,
     "user": {
      "displayName": "Kang Jihun",
      "userId": "00356180714370444370"
     },
     "user_tz": -540
    },
    "id": "hbfVbjt-yrY2",
    "outputId": "d8417597-88b8-41f6-9d1a-dd8cd82d2b38"
   },
   "outputs": [],
   "source": [
    "# Random over sampling\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "oversample = RandomOverSampler(sampling_strategy=0.5)\n",
    "x_over, y_over = oversample.fit_resample(x, y)\n",
    "print(Counter(y_over))\n",
    "\n",
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "x_over2, y_over2 = oversample.fit_resample(x, y)\n",
    "print(Counter(y_over2))\n",
    "\n",
    "# SMOTE\n",
    "### 소수 레이블을 지닌 데이터 세트의 관측 값에 대한 K개의 최근접 이웃을 찾고,\n",
    "### 관측 값과 이웃으로 선택된 값 사이에 임의의 새로운 데이터를 생성하는 방법으로 샘플의 수를 늘린다.\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote_sample = SMOTE(sampling_strategy='minority')\n",
    "x_sm, y_sm = smote_sample.fit_resample(x, y)\n",
    "print(Counter(y_sm))\n",
    "\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 10))\n",
    "sns.scatterplot(x=x[:, 1], y=x[:, 2], hue=y, ax=axes[0][0], alpha=0.5)\n",
    "sns.scatterplot(x=x_under[:, 1], y=x_under[:, 2], hue=y_under, ax=axes[0][1], alpha=0.5)\n",
    "sns.scatterplot(x=x_over[:, 1], y=x_over[:, 2], hue=y_over, ax=axes[1][0], alpha=0.5)\n",
    "sns.scatterplot(x=x_sm[:, 1], y=x_sm[:, 2], hue=y_sm, ax=axes[1][1], alpha=0.5)\n",
    "\n",
    "axes[0][0].set_title('Original Data')\n",
    "axes[0][1].set_title('Random Under Sampling')\n",
    "axes[1][0].set_title('Random Over Sampling')\n",
    "axes[1][1].set_title('SMOTE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee93ba5f",
   "metadata": {
    "id": "NkM5MWjnyrjh"
   },
   "source": [
    "## 머신러닝 분석 프로세스\n",
    "\n",
    "1. 데이터 확인\n",
    "  - 독립/종속변수 확인\n",
    "  - 연속형/범주형 확인\n",
    "  - 적용 가능한 분석모델 확인\n",
    "2. 데이터 분할\n",
    "3. 전처리\n",
    "  - 표준화/정규화\n",
    "  - 결측치 확인 후 처리\n",
    "  - 이상치 확인 후 처리\n",
    "4. 모델 학습\n",
    "  - 머신러닝 알고리즘 적용\n",
    "  - 회귀/분류/비지도 학습\n",
    "  - 하이퍼파라미터 조절\n",
    "5. 성능평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bf59cc",
   "metadata": {
    "id": "CAKJmvhxyrna"
   },
   "source": [
    "## 회귀분석 맛보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928d8c9e",
   "metadata": {
    "executionInfo": {
     "elapsed": 611,
     "status": "ok",
     "timestamp": 1692803133246,
     "user": {
      "displayName": "Kang Jihun",
      "userId": "00356180714370444370"
     },
     "user_tz": -540
    },
    "id": "roCIfRpgyrri"
   },
   "outputs": [],
   "source": [
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "boston = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "boston_dt = np.hstack([boston.values[::2, :], boston.values[1::2, :2]])\n",
    "price = boston.values[1::2, 2]\n",
    "\n",
    "\n",
    "df = pd.DataFrame(boston_dt, columns=[\n",
    "    'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PIRATIO', 'B', 'LSTAT'\n",
    "    ])\n",
    "df['PRICE'] = price\n",
    "\n",
    "df.head()\n",
    "\n",
    "df.shape\n",
    "\n",
    "df.info()\n",
    "\n",
    "df.isna().sum()\n",
    "\n",
    "# regplot은 산점도와 회귀직선을 동시에 나타냄\n",
    "# constrained_layout = True 이면 그래프 사이의 적절한 간격을 자동으로 설정\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(16,10), ncols=4, nrows=3, constrained_layout=True)\n",
    "features = df.columns.difference(['PRICE', 'CHAS'])\n",
    "\n",
    "for i, feature in zip(range(12), features):\n",
    "  row = int(i/4)\n",
    "  col = i%4\n",
    "\n",
    "  sns.regplot(x=feature, y=df['PRICE'], data=df, ax=axs[row][col])\n",
    "\n",
    "# 데이터 분할\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = df[['CRIM', 'ZN', 'INDUS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PIRATIO', 'B', 'LSTAT']].values\n",
    "y = df['PRICE'].values\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print('학습데이터세트 PRICE 평균: ', y_train.mean())\n",
    "print('평가데이터세트 PRICE 평균: ', y_test.mean())\n",
    "\n",
    "# 전처리\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "\n",
    "# 모델 학습\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linear = LinearRegression()\n",
    "linear.fit(x_train_scaled, y_train)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "\n",
    "pred_train = linear.predict(x_train_scaled)\n",
    "\n",
    "mae = mean_absolute_error(y_train, pred_train)\n",
    "mse = mean_squared_error(y_train, pred_train)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_train, pred_train)\n",
    "\n",
    "print('MAE: {0: .5f}'.format(mae))\n",
    "print('MSE: {0: .5f}'.format(mse))\n",
    "print('RMSE: {0: .5f}'.format(rmse))\n",
    "print('R2: {0: .5f}'.format(r2))\n",
    "\n",
    "# 성능평가 및 예측값 저장\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "pred = linear.predict(x_test_scaled)\n",
    "\n",
    "mae = mean_absolute_error(y_test, pred)\n",
    "mse = mean_squared_error(y_test, pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, pred)\n",
    "\n",
    "print('MAE: {0: .5f}'.format(mae))\n",
    "print('MSE: {0: .5f}'.format(mse))\n",
    "print('RMSE: {0: .5f}'.format(rmse))\n",
    "print('R2: {0: .5f}'.format(r2))\n",
    "\n",
    "pred_df = pd.DataFrame(pred, columns=['pred Price'])\n",
    "pred_df.head()\n",
    "\n",
    "actual = pd.DataFrame(y_test, columns=['actual Price'])\n",
    "actual.head()\n",
    "\n",
    "reg_result = pd.concat([actual, pred_df], axis=1)\n",
    "reg_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5445a4f4",
   "metadata": {
    "id": "pmW4XN8a-Sbx"
   },
   "source": [
    "## 분류분석 맛보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1d31e2",
   "metadata": {
    "executionInfo": {
     "elapsed": 291,
     "status": "ok",
     "timestamp": 1692803964608,
     "user": {
      "displayName": "Kang Jihun",
      "userId": "00356180714370444370"
     },
     "user_tz": -540
    },
    "id": "cNC89N29-V6C"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "iris_dt = iris.data\n",
    "iris_label = iris.target\n",
    "\n",
    "df = pd.DataFrame(data=iris_dt, columns=iris.feature_names)\n",
    "df['Species'] = iris_label\n",
    "\n",
    "df.head()\n",
    "\n",
    "df['Species'].unique()\n",
    "\n",
    "df.shape\n",
    "\n",
    "# 데이터 분할\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    iris_dt, iris_label, test_size=0.2, random_state=0, stratify=iris_label\n",
    ")\n",
    "\n",
    "# 전처리\n",
    "df.isna().sum()\n",
    "\n",
    "# 모델 학습\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtree_clf_5 = DecisionTreeClassifier(max_depth=5, random_state=100)\n",
    "dtree_clf_3 = DecisionTreeClassifier(max_depth=3, random_state=100)\n",
    "dtree_clf_1 = DecisionTreeClassifier(max_depth=1, random_state=100)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(dtree_clf_5, x_train, y_train, scoring='accuracy', cv=10)\n",
    "\n",
    "print('교차검증 정확도: ', np.round(scores, 3))\n",
    "print('평균 검증 정확도: ', np.round(np.mean(scores), 4))\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(dtree_clf_3, x_train, y_train, scoring='accuracy', cv=10)\n",
    "\n",
    "print('교차검증 정확도: ', np.round(scores, 3))\n",
    "print('평균 검증 정확도: ', np.round(np.mean(scores), 4))\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(dtree_clf_1, x_train, y_train, scoring='accuracy', cv=10)\n",
    "\n",
    "print('교차검증 정확도: ', np.round(scores, 3))\n",
    "print('평균 검증 정확도: ', np.round(np.mean(scores), 4))\n",
    "\n",
    "# 성능평가 및 예측값 저장\n",
    "dtree_clf_5.fit(x_train, y_train)\n",
    "pred = dtree_clf_5.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('의사결정나무(교차검증 후) 예측 정확도: {0:.5f}'.format(accuracy_score(y_test, pred)))\n",
    "\n",
    "pred = pd.DataFrame(pred, columns=['pred Species'])\n",
    "pred.head()\n",
    "\n",
    "actual = pd.DataFrame(y_test, columns=['actual Species'])\n",
    "actual.head()\n",
    "\n",
    "classify_result = pd.concat([actual, pred], axis=1)\n",
    "classify_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db047b90",
   "metadata": {
    "id": "gk0X82p5-Wno"
   },
   "source": [
    "## 단순 선형 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649cbb80",
   "metadata": {
    "executionInfo": {
     "elapsed": 551,
     "status": "ok",
     "timestamp": 1692886174264,
     "user": {
      "displayName": "Kang Jihun",
      "userId": "00356180714370444370"
     },
     "user_tz": -540
    },
    "id": "QU9av9rN2kWC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns\n",
    "\n",
    "data = sns.load_dataset('diamonds')\n",
    "\n",
    "data.head(3)\n",
    "\n",
    "x = data['carat']\n",
    "y = data['price']\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(x,y)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.show()\n",
    "\n",
    "x = np.array(data['carat'])\n",
    "y = np.array(data['price'])\n",
    "\n",
    "x = x.reshape(53940,1)\n",
    "y = y.reshape(53940,1)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(x,y)\n",
    "\n",
    "lr.intercept_, lr.coef_\n",
    "\n",
    "lr.score(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9960e646",
   "metadata": {
    "id": "n8h-dOLw2k8n"
   },
   "source": [
    "## 경사 하강법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24588e8d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "executionInfo": {
     "elapsed": 1798,
     "status": "ok",
     "timestamp": 1692886579917,
     "user": {
      "displayName": "Kang Jihun",
      "userId": "00356180714370444370"
     },
     "user_tz": -540
    },
    "id": "CuZeGYRy-Wql",
    "outputId": "898eadb2-7a39-4319-d60f-490bfeb0de0d"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd_reg = SGDRegressor(max_iter=1000, random_state=34)\n",
    "sgd_reg.fit(x,y.ravel())\n",
    "\n",
    "sgd_reg.intercept_, sgd_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a870677e",
   "metadata": {
    "id": "mtjDmfy151xn"
   },
   "source": [
    "## 릿지 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b2a662",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1692887112709,
     "user": {
      "displayName": "Kang Jihun",
      "userId": "00356180714370444370"
     },
     "user_tz": -540
    },
    "id": "BXySJWhq7W81"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "x = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
    "y = diabetes.target\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "alpha = np.logspace(-3, 1, 5)\n",
    "\n",
    "data = []\n",
    "for i, a in enumerate(alpha):\n",
    "  ridge = Ridge(alpha=a)\n",
    "  ridge.fit(x, y)\n",
    "  data.append(pd.Series(np.hstack([ridge.coef_])))\n",
    "\n",
    "df_ridge = pd.DataFrame(data, index=alpha)\n",
    "df_ridge.columns = x.columns\n",
    "df_ridge\n",
    "\n",
    "ridge.coef_\n",
    "\n",
    "np.hstack([ridge.coef_])\n",
    "\n",
    "np.hstack(ridge.coef_)\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.formula.api import ols¬\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set a few plotting defaults\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['font.size'] = 18\n",
    "plt.rcParams['patch.edgecolor'] = 'k'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.display.max_columns = 150\n",
    "\n",
    "# Plots the distribution of a variable colored by value of the target\n",
    "def kde_target(var_name, df):\n",
    "\n",
    "    # Calculate the correlation coefficient between the new variable and the target\n",
    "    corr = df['TARGET'].corr(df[var_name])\n",
    "\n",
    "    # Calculate medians for repaid vs not repaid\n",
    "    avg_repaid = df.ix[df['TARGET'] == 0, var_name].median()\n",
    "    avg_not_repaid = df.ix[df['TARGET'] == 1, var_name].median()\n",
    "\n",
    "    plt.figure(figsize = (12, 6))\n",
    "\n",
    "    # Plot the distribution for taget == 0 and target == 1\n",
    "    sns.kdeplot(df.ix[df['TARGET'] == 0, var_name], label = 'TARGET == 0')\n",
    "    sns.kdeplot(df.ix[df['TARGET'] == 1, var_name], label = 'TARGET == 1')\n",
    "\n",
    "    # Label the plot\n",
    "    plt.xlabel(var_name); plt.ylabel('Density'); plt.title('%s Distribution' % var_name)\n",
    "    plt.legend();\n",
    "\n",
    "    # Print out the correlation\n",
    "    print('The correlation between %s and the TARGET is %0.4f' % (var_name, corr))\n",
    "\n",
    "    # Print out average values\n",
    "    print('Median value for loan that was not repaid = %0.4f' % avg_not_repaid)\n",
    "    print('Median value for loan that was repaid = %0.4f' % avg_repaid)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "array = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "print(f\"ndim : {array.ndim}\")\n",
    "print(f\"shape : {array.shape}\")\n",
    "print(f\"dtype : {array.dtype}\")\n",
    "print()\n",
    "print(f\"sum : {array.sum()}\")\n",
    "print(f\"min : {array.min()}\")\n",
    "print(f\"max : {array.max()}\")\n",
    "print(f\"mean : {array.mean()}\")\n",
    "print(f\"std : {array.std()}\")\n",
    "\n",
    "nums = np.array([1, 2, 2, 3])\n",
    "\n",
    "print(f\"set : {set(nums)}\")\n",
    "print(f\"unique : {np.unique(nums)}\")\n",
    "print(f\"len : {len(nums)}\")\n",
    "print()\n",
    "print(f\"np.arange : {np.arange(3, 8, 2)}\")\n",
    "\n",
    "print(f\"np.where : {np.where(nums == 2, 1, 0)}\")\n",
    "print(\"nums==2라는 조건을 만족하는 인덱스에 값을 1로 주고, 만족하지 않으면 0을 준다\")\n",
    "\n",
    "# pd Series 생성\n",
    "pd.Series([1, 2, 3, 3, 4])\n",
    "pd.Series([1, 2, 3], index = [\"a\", \"b\", \"c\"])\n",
    "\n",
    "ser = pd.Series([1, 2, 3, 3, 4])\n",
    "\n",
    "print(f\"sum : {ser.sum()}\")\n",
    "print(f\"min : {ser.min()}\")\n",
    "print(f\"max : {ser.max()}\")\n",
    "print(f\"mean : {ser.mean()}\")\n",
    "print(f\"var : {ser.var()}\")\n",
    "print(f\"std : {ser.std()}\")\n",
    "print()\n",
    "print(f\"skew : {ser.skew()}\")\n",
    "print(f\"kurt : {ser.kurt()}\")\n",
    "print()\n",
    "print(f\"unique : {ser.unique()}\")\n",
    "print(f\"idxmax : {ser.idxmax()}\")\n",
    "print()\n",
    "print(f\"isin : {ser.isin([1, 2])}\")\n",
    "print(\"isin은 인자로 넣은 요소가 series/dataframe에 있으면 해당하는 인덱스 true로 반환\")\n",
    "\n",
    "# dataframe 생성\n",
    "a = pd.DataFrame([[1, 2, 2], [3, 3, 4]])\n",
    "display(a)\n",
    "\n",
    "b = pd.DataFrame({'aa' : [1, 2, 3], \"bb\" : [2, 3, 4]})\n",
    "display(b)\n",
    "\n",
    "c = pd.DataFrame([[1, 2], [3, 4], [5, 6]], columns=[\"xx\", \"yy\"])\n",
    "display(c)\n",
    "\n",
    "d = pd.DataFrame(np.arange(4).reshape((2, 2)), columns=[\"a\", \"b\"])\n",
    "display(d)\n",
    "\n",
    "# dataframe 색인\n",
    "b.aa\n",
    "b[\"aa\"]\n",
    "b[\"aa\"][:2]\n",
    "b.iloc[:, 0]\n",
    "b.iloc[:2, :]\n",
    "b.loc[:, \"aa\"]\n",
    "\n",
    "# crosstab\n",
    "t1 = pd.crosstab(b[\"aa\"], b[\"bb\"])\n",
    "display(t1)\n",
    "\n",
    "t2 = pd.crosstab(b[\"aa\"], b[\"bb\"], normalize=True)\n",
    "display(t2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fillna() #\n",
    "# ======== #\n",
    "\n",
    "# method: bfill - 뒤의 값으로, ffill - 앞의 값으로 결측 처리\n",
    "\n",
    "index_cabin_null = df_titanic[df_titanic['Cabin'].isnull()].index\n",
    "\n",
    "df_titanic[~df_titanic.index.isin(index_cabin_null)].head()\n",
    "\n",
    "df_titanic.iloc[index_cabin_null].head()\n",
    "\n",
    "df_titanic['Cabin'].fillna(method='bfill').head()\n",
    "# 0번 인덱스가 nan 값이었는데, 1번 인덱스의 C85로 채워넣었다.\n",
    "\n",
    "df_titanic['Cabin'].fillna(method='ffill').head()\n",
    "# 0번 인덱스 nan 값은 안채워졌고, 2번 인덱스 nan 값은 1번 인덱스의 C85로 채워넣었다.\n",
    "\n",
    "df_titanic['Age'].fillna(df_titanic['Age'].mean()).head()\n",
    "# 결측치를 평균값으로 대체하는 법\n",
    "\n",
    "# dropna() #\n",
    "# ======== #\n",
    "# 결측치가 있는 row 혹은 column을 제거\n",
    "# how 인자에 'any'는 결측치가 하나라도 있는 경우, 'all'은 전체가 결측인 경우 제거\n",
    "\n",
    "display(df_titanic.dropna(how='any').head())\n",
    "print()\n",
    "\n",
    "# quantile() #\n",
    "# ========== #\n",
    "print(f\"1/4분위수 : {df_titanic['Age'].quantile(q = 0.25)}\")\n",
    "print(f\"중앙값 : {df_titanic['Age'].quantile(q = 0.5)}\")\n",
    "print(f\"3/4분위수 : {df_titanic['Age'].quantile(q = 0.75)}\")\n",
    "print()\n",
    "\n",
    "display(df_titanic.describe())\n",
    "\n",
    "# detect outlier #\n",
    "# ============== #\n",
    "test = df_titanic.copy()\n",
    "test['Age'] = test['Age'].fillna(test['Age'].mean())\n",
    "\n",
    "outlier_scale = test['Age'].std() * 1.5\n",
    "\n",
    "outlier_low = test['Age'].mean() - outlier_scale\n",
    "outlier_high = test['Age'].mean() + outlier_scale\n",
    "\n",
    "outlier_bool = (test['Age'] < outlier_low) | (test['Age'] > outlier_high)\n",
    "\n",
    "test[outlier_bool].head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# np.where() #\n",
    "# ========== #\n",
    "\n",
    "a = np.arange(10)\n",
    "print(a)\n",
    "print(np.where(a < 5, a, 10*a))\n",
    "\n",
    "# pd.rename() #\n",
    "# =========== #\n",
    "\n",
    "df_titanic.rename(columns={'Sex': 'Gender'}).head(1)\n",
    "\n",
    "# pd.apply() #\n",
    "# ========== #\n",
    "\n",
    "df_titanic['Age'].apply(lambda x: x**2)\n",
    "\n",
    "# pd.map() #\n",
    "# ======== #\n",
    "df_titanic['Sex'].map({'male': 1, 'female': 0})\n",
    "\n",
    "# pd.astype() #\n",
    "# =========== #\n",
    "df_titanic['Age'].astype('str')\n",
    "\n",
    "# pd.get_dummies() #\n",
    "# ================ #\n",
    "pd.get_dummies(df_titanic, columns=['Sex'])\n",
    "\n",
    "df.head(2)\n",
    "\n",
    "# pd.to_datetime() #\n",
    "# ================ #\n",
    "\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "# bike data 일자별 최대 casual이 25를 넘는 일 수는?\n",
    "df['date'] = df['datetime'].dt.date\n",
    "display(df.head(2))\n",
    "print()\n",
    "print(f\"bike data 일자별 최대 casual이 25를 넘는 일 수 : {(df.groupby('date')['casual'].max() > 25).sum()}\")\n",
    "\n",
    "# bike data\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "\n",
    "reg_list = df.groupby('hour')[['registered']].mean()\n",
    "\n",
    "print(reg_list.max())\n",
    "print(reg_list.idxmax())\n",
    "display(reg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa068d3-467b-4cd2-b27c-3e5d0e7806db",
   "metadata": {
    "id": "afa068d3-467b-4cd2-b27c-3e5d0e7806db"
   },
   "source": [
    "# 데이터 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ed1b1e-58a5-46db-800f-b9f79ec55f00",
   "metadata": {
    "id": "87ed1b1e-58a5-46db-800f-b9f79ec55f00"
   },
   "outputs": [],
   "source": [
    "# pd reset_index() #\n",
    "# ================ #\n",
    "# 기존의 인덱스 초기화. 0부터 row 개수만큼 재할당.\n",
    "\n",
    "# pd set_index() #\n",
    "# ============== #\n",
    "# 특정 변수를 인덱스로 지정\n",
    "\n",
    "# pd.concat() #\n",
    "# =========== #\n",
    "# axis=0 행 기준 - 아래로 합치기 // axis=1 열 기준 - 옆으로 합치기\n",
    "\n",
    "# pd.merge(), pd.join() #\n",
    "# ===================== #\n",
    "# merge는 column 기준, join은 index 기준으로 병합\n",
    "\n",
    "# 여름과 겨울의 시간대별 registered 평균을 비교할 때 가장 차이가 많이 나는 시각은? #\n",
    "# ================================================================ #\n",
    "temp = df.groupby(['season', 'hour'])[['registered']].mean()\n",
    "\n",
    "temp\n",
    "\n",
    "abs(temp.loc[2] - temp.loc[4]).idxmax()\n",
    "\n",
    "# 비가 온 날에 30도가 넘는 시각의 count의 평균은 얼마인가? #\n",
    "# ============================================= #\n",
    "df['max_hum'] = df.groupby('date')['humidity'].transform('max')\n",
    "\n",
    "df['rain'] = df['max_hum'].apply(lambda x : 1 if x==100 else 0)\n",
    "\n",
    "df[(df['rain']==1) & (df['temp']>=30)]['count'].mean().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bc6b9f-8a2d-45e9-b5fa-e73b28cd49b2",
   "metadata": {
    "id": "38bc6b9f-8a2d-45e9-b5fa-e73b28cd49b2"
   },
   "source": [
    "# 정렬 및 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa934e09-9e1f-4682-8ccb-f0d4203c6ca0",
   "metadata": {
    "id": "aa934e09-9e1f-4682-8ccb-f0d4203c6ca0"
   },
   "outputs": [],
   "source": [
    "# pd crosstab()\n",
    "# pd sort_values()\n",
    "# pd melt() - wide form의 데이터프레임을 long form으로 변환\n",
    "# pd pivot() - long form을 wide form으로 변환\n",
    "\n",
    "# Workingday가 아니면서 Holiday가 아닌 날의 비율은?\n",
    "pd.crosstab(df['holiday'], df['workingday'], normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d4905e-60df-4911-9d18-551a88a12b7e",
   "metadata": {
    "id": "62d4905e-60df-4911-9d18-551a88a12b7e"
   },
   "source": [
    "# 사용자정의 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9743f7c1-c1ba-4dd5-bc69-dfeda62d1052",
   "metadata": {
    "id": "9743f7c1-c1ba-4dd5-bc69-dfeda62d1052"
   },
   "outputs": [],
   "source": [
    "# pd.Series를 입력 받고, 원소를 제곱하여 더한 후, 제곱근을 구하는 함수 #\n",
    "# ======================================================= #\n",
    "def udf_euc(x):\n",
    "    result = x.pow(2).sum()\n",
    "    return np.sqrt(result)\n",
    "\n",
    "a = pd.Series([3, 5, 9, 20])\n",
    "\n",
    "udf_euc(a)\n",
    "\n",
    "# pd.Series를 입력 받고, 표준화시키는 함수 #\n",
    "# ================================= #\n",
    "def nor_std(x):\n",
    "    result = (x - x.mean()) / x.std()\n",
    "    return result\n",
    "\n",
    "nor_std(pd.Series([-4, 5, 7, 9]))\n",
    "\n",
    "# pd.Series를 입력 받고, MinMax 정규화시키는 함수 #\n",
    "# ======================================== #\n",
    "def nor_minmax(x):\n",
    "    result = (x - x.min()) / (x.max() - x.min())\n",
    "    return result\n",
    "\n",
    "nor_minmax(pd.Series([-4, 5, 7, 9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717748f9-f3f3-4099-af5c-2322925d95bf",
   "metadata": {
    "id": "717748f9-f3f3-4099-af5c-2322925d95bf"
   },
   "source": [
    "# 상관 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafa6a22-e1e3-433b-93b0-0993bffcc90f",
   "metadata": {
    "id": "fafa6a22-e1e3-433b-93b0-0993bffcc90f"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "# pd corr() - method에 'pearson', 'kendall', 'spearman'\n",
    "# scipy pearsonr() - 2개의 1차원 벡터를 입력 받고, 상관계수와 p-value를 출력\n",
    "# scipy spearmanr() - 2개의 1차원 벡터를 입력 받고, 상관계수와 p-value를 출력\n",
    "# scipy kendalltau() - 2개의 1차원 벡터를 입력 받고, 상관계수와 p-value를 출력\n",
    "\n",
    "df[['temp','atemp','humidity','casual']].corr().round(2).style.background_gradient(cmap='summer')\n",
    "\n",
    "# 계절별로 체감온도와 자전거 대여 숫자의 상관관계는?\n",
    "df[['season', 'atemp', 'casual']].groupby('season').corr().round(2).style.background_gradient(cmap='summer')\n",
    "\n",
    "##### 참고 #####\n",
    "df_corr = df[['season', 'atemp', 'casual']].groupby('season').corr().reset_index()\n",
    "df_corr\n",
    "\n",
    "df_corr = df_corr.loc[df_corr['atemp'] < 1, ]\n",
    "df_corr\n",
    "\n",
    "# 날씨에 따른 기온과 자전거 대여의 상관계수 구하기\n",
    "df['weather_good'] = df['weather'].apply(lambda x: 1 if x==1 else 0)\n",
    "\n",
    "df_corr = df[['weather_good', 'temp', 'casual']].groupby('weather_good').corr()\n",
    "df_corr\n",
    "\n",
    "round(abs(df_corr.iloc[1, 0] - df_corr.iloc[3, 0]), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47217c4-4a29-4d41-a4ea-cfc5ddbf2d9c",
   "metadata": {
    "id": "f47217c4-4a29-4d41-a4ea-cfc5ddbf2d9c"
   },
   "source": [
    "# 비계층적 군집분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b05277a-abcc-476e-b35f-254a88d11436",
   "metadata": {
    "id": "1b05277a-abcc-476e-b35f-254a88d11436",
    "outputId": "a232f254-4340-4f2a-c449-b3be5dc7a567"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "##### k-means clustering #####\n",
    "# BMI가 0이 아닌 사람 대상으로 KMeans 군집분석을 실시하고,\n",
    "# 군집개수가 가장 큰 군집의 Insulin 평균을 구하라\n",
    "df_diab = pd.read_csv('diabetes.csv')\n",
    "model = KMeans(n_clusters=4, random_state=123).fit(df_diab)\n",
    "\n",
    "print(model.labels_[:10])\n",
    "print()\n",
    "print(model.cluster_centers_)\n",
    "\n",
    "df_diab['cluster'] = model.labels_\n",
    "\n",
    "df_diab[(df_diab['cluster']==0) & (df_diab['BMI']!=0)].Insulin.mean()\n",
    "\n",
    "# 군집분석 이전에 MinMax 정규화 후.. 군집 개수가 가장 큰 군집의 나이 평균을 구하라\n",
    "df_diab = df_diab[df_diab['BMI']!=0]\n",
    "nor_minmax = MinMaxScaler()\n",
    "df_diab_nor = nor_minmax.fit_transform(df_diab)\n",
    "df_diab_nor = pd.DataFrame(df_diab_nor, columns = df_diab.columns)\n",
    "df_diab_nor.head(2)\n",
    "\n",
    "model = KMeans(n_clusters=4, random_state=123).fit(df_diab_nor)\n",
    "df_diab['cluster_nor'] = model.labels_\n",
    "df_diab['cluster_nor'].value_counts()\n",
    "\n",
    "df_diab.groupby('cluster_nor').Age.mean()\n",
    "\n",
    "# BMI가 0이 아닌 사람 대상으로 Kmeans 군집분석 실시하고,\n",
    "# 군집의 중심점간 유클리드 거리가 가장 가까운 그룹간 거리를 구하시오\n",
    "df = pd.read_csv('diabetes.csv')\n",
    "df_sub = df.loc[df['BMI'] != 0, ]\n",
    "df_sub.head(2)\n",
    "\n",
    "model = KMeans(n_clusters=3, random_state=123).fit(df_sub)\n",
    "\n",
    "df_centers = pd.DataFrame(model.cluster_centers_, columns=df_sub.columns)\n",
    "df_centers\n",
    "\n",
    "df_centers = df_centers.transpose()\n",
    "df_centers.head(3)\n",
    "\n",
    "print(sum((df_centers.iloc[:, 0] - df_centers.iloc[:, 1]) ** 2) ** 0.5)\n",
    "print(sum((df_centers.iloc[:, 1] - df_centers.iloc[:, 2]) ** 2) ** 0.5)\n",
    "print(sum((df_centers.iloc[:, 0] - df_centers.iloc[:, 2]) ** 2) ** 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd90361e-8e21-4729-af71-7ddbd93a9550",
   "metadata": {
    "id": "fd90361e-8e21-4729-af71-7ddbd93a9550"
   },
   "source": [
    "# 단순 회귀분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b1884f-604a-47f1-bcc1-bdcd0bf3aaad",
   "metadata": {
    "id": "18b1884f-604a-47f1-bcc1-bdcd0bf3aaad",
    "outputId": "6a5562d8-e940-48f4-aa94-915ebb0c8fc3"
   },
   "outputs": [],
   "source": [
    "##### documentation 예시 #####\n",
    "import statsmodels.api as sm\n",
    "duncan_prestige = sm.datasets.get_rdataset(\"Duncan\", \"carData\")\n",
    "Y = duncan_prestige.data['income']\n",
    "X = duncan_prestige.data['education']\n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(Y,X)\n",
    "results = model.fit()\n",
    "results.params\n",
    "\n",
    "model.fit()\n",
    "\n",
    "results.predict()\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
    "# y = 1 * x_0 + 2 * x_1 + 3\n",
    "y = np.dot(X, np.array([1, 2])) + 3\n",
    "reg = LinearRegression().fit(X, y)\n",
    "reg.score(X, y)\n",
    "print(reg.coef_)\n",
    "print(reg.intercept_)\n",
    "reg.predict(np.array([[3, 5]]))\n",
    "\n",
    "\n",
    "model.predict()\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "X_california, y_california = datasets.fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "\n",
    "X_california.head()\n",
    "\n",
    "y_california.head()\n",
    "\n",
    "X_california.info()\n",
    "\n",
    "X_iris, y_iris = datasets.load_iris(return_X_y=True, as_frame=True)\n",
    "\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "df_iris = pd.concat([X_iris, y_iris], axis=1)\n",
    "\n",
    "df_iris.columns\n",
    "\n",
    "df_iris.head()\n",
    "\n",
    "df_iris = df_iris.rename(columns={'sepal length (cm)': 'SL'})\n",
    "\n",
    "model = ols(formula=\"SL ~ target\", data=df_iris).fit()\n",
    "\n",
    "model.summary()\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression().fit(X = df_iris[[\"SL\"]],\n",
    "                               y = df_iris['target'])\n",
    "\n",
    "print(model.coef_)\n",
    "print(model.intercept_)\n",
    "\n",
    "df_iris['pred'] = model.predict(df_iris[['SL']])\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "mean_absolute_error(y_true = df_iris['target'], y_pred = df_iris['pred'])\n",
    "\n",
    "mean_squared_error(y_true = df_iris['target'], y_pred = df_iris['pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529a6e59-eea4-45ae-8bc6-27d4dd5a409f",
   "metadata": {
    "id": "529a6e59-eea4-45ae-8bc6-27d4dd5a409f"
   },
   "source": [
    "# 다중회귀분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30275889-4481-4eb7-b79c-26fd2d094107",
   "metadata": {
    "id": "30275889-4481-4eb7-b79c-26fd2d094107"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from patsy import dmatrices\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n",
    "\n",
    "df = pd.read_csv(bike_path)\n",
    "df.head(2)\n",
    "\n",
    "df_sub = df.loc[:, \"season\":\"casual\"]\n",
    "df_sub.head(2)\n",
    "\n",
    "formula = \"casual ~ \" + \" + \".join(df_sub.columns[:-1])\n",
    "y, X = dmatrices(formula, data=df_sub,return_type='dataframe')\n",
    "\n",
    "df_vif = pd.DataFrame()\n",
    "df_vif['colname'] = X.columns\n",
    "\n",
    "df_vif['VIF'] = [ vif(X.values, i) for i in range(X.shape[1]) ]\n",
    "df_vif\n",
    "\n",
    "df_sub.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036e61d1-fe22-443d-af4b-f2b9b41d2211",
   "metadata": {
    "id": "036e61d1-fe22-443d-af4b-f2b9b41d2211"
   },
   "source": [
    "# 로지스틱 회귀분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031feacd-426a-4b00-96dc-501475c200df",
   "metadata": {
    "id": "031feacd-426a-4b00-96dc-501475c200df",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.api import Logit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "df = df_iris.copy()\n",
    "df.head(2)\n",
    "\n",
    "df['is_setosa'] = df['target'].apply(lambda x: 1 if x == 0 else 0)\n",
    "\n",
    "model = Logit(endog=df['is_setosa'],\n",
    "              exog=df.iloc[:, :2]).fit()\n",
    "\n",
    "model\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB()\n",
    "\n",
    "model.fit(df.drop('target',axis=1), df['target'])\n",
    "\n",
    "model.class_prior_\n",
    "\n",
    "model.theta_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 결측치를 채우기 위한 선형회귀모형\n",
    "df_dropped = df_edited.dropna(axis=0)\n",
    "lm = sm.OLS(df_dropped['~'], df_dropped[['~', '~']])\n",
    "lm = lm.fit()\n",
    "print(lm.rsquared.round(3))\n",
    "\n",
    "# 결측치 채우기\n",
    "col_null_idx = df_edited[df_edited.isnull()['col']].index\n",
    "for idx in col_null_idx:\n",
    "    df_edited['col'].iloc[idx] = lm.predict(df_edited[['~', '~']])[idx].round(1)\n",
    "\n",
    "\n",
    "\n",
    "# 목표 변수의 분포\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.distplot(df['target'])\n",
    "plt.title('Target Distribution')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.countplot(df['target'])\n",
    "plt.title('Target Count')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "ax = sns.countplot(df_edited['SCALE'])\n",
    "plt.ylim(0,550)\n",
    "for p in ax.patches:\n",
    "        ax.annotate('{} 건'.format(p.get_height()), (p.get_x()+0.3, p.get_height()+15), fontsize=12)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# plt axvline / xticks / subplots etc.\n",
    "\n",
    "\n",
    "\n",
    "## Some Feature Engineering\n",
    "\n",
    "# PLATE_NO는 id값이니 제외\n",
    "df_edited = df_raw.drop('PLATE_NO', axis=1)\n",
    "\n",
    "# SCALE 정수 labelling\n",
    "df_edited['SCALE'] = df_edited['SCALE'].map({'양품': 0, '불량': 1})\n",
    "\n",
    "# HSB 정수 labelling\n",
    "df_edited['HSB'] = df_edited['HSB'].map({'미적용': 0, '적용': 1})\n",
    "\n",
    "# STEEL_KIND labelling\n",
    "df_edited['STEEL_KIND'] = df_edited['STEEL_KIND'].str[:1]\n",
    "\n",
    "# SPEC labelling\n",
    "df_edited['SPEC'] = df_edited['SPEC'].apply(lambda x: x.split('-')[0])\n",
    "df_edited['SPEC'] = df_edited['SPEC'].apply(lambda x: x.split('/')[0])\n",
    "\n",
    "# 관측치가 20개 미만인 규격(SPEC)들 => \"UNCOMMON\"\n",
    "uncomm_specs = ['A283', 'V42JBN3', 'A516', 'API', 'A709', 'A131', 'CCS']\n",
    "df_edited['SPEC'] = df_edited['SPEC'].apply(lambda x: 'UNCOMMON' if x in uncomm_specs else x)\n",
    "\n",
    "# Rolling_Date to datetime\n",
    "df_edited['ROLLING_DATE'] = df_edited['ROLLING_DATE'].apply(lambda x: ':'.join(x.split(':')[0:2]))\n",
    "df_edited['ROLLING_DATE'] = pd.to_datetime(df_edited['ROLLING_DATE'], format='%Y-%m-%d:%H')\n",
    "\n",
    "# Rolling_Temp 이상치 채우기\n",
    "idx_outlier = list(df_edited['ROLLING_TEMP_T5'][df_edited['ROLLING_TEMP_T5']==0].index)\n",
    "\n",
    "for i in idx_outlier:\n",
    "    rolling_temp_mean = df_edited[df_edited['ROLLING_DATE'] == df_edited['ROLLING_DATE'].iloc[i]].mean()['ROLLING_TEMP_T5']\n",
    "\n",
    "    df_edited['ROLLING_TEMP_T5'].iloc[i] = rolling_temp_mean\n",
    "\n",
    "## interpolate 방법\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(nrows=2, ncols=2, figsize=(12,10))\n",
    "sns.histplot(x='FUR_HZ_TEMP', hue='SCALE', data=df_edited,\n",
    "            ax=ax[0,0])\n",
    "sns.histplot(x='FUR_HZ_TIME', hue='SCALE', data=df_edited,\n",
    "            ax=ax[0,1])\n",
    "sns.histplot(x='FUR_SZ_TEMP', hue='SCALE', data=df_edited,\n",
    "            ax=ax[1,0])\n",
    "sns.histplot(x='FUR_SZ_TIME', hue='SCALE', data=df_edited,\n",
    "            ax=ax[1,1])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef4b76b-55e8-48b8-b4fa-34fc96d40c12",
   "metadata": {
    "id": "1ef4b76b-55e8-48b8-b4fa-34fc96d40c12",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### Chi2: 가열로호기 \\~ 불량률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813d3c67-2158-40b1-8e3e-d5f1a977bd78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T10:09:06.124213Z",
     "start_time": "2021-08-31T10:09:06.097842Z"
    },
    "id": "813d3c67-2158-40b1-8e3e-d5f1a977bd78",
    "outputId": "da283f1a-3bb7-4f61-f82f-00d05f851cb2"
   },
   "outputs": [],
   "source": [
    "# 가열로와 불량률 사이에\n",
    "table = pd.crosstab(df_edited['SCALE'], df_edited['FUR_NO'])\n",
    "chi2, pval, dof, expected = stats.chi2_contingency(table)\n",
    "\n",
    "print('카이제곱통계량:', chi2)\n",
    "print('P-value:', pval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8a4005-757c-48d7-829f-21d2f6bd7818",
   "metadata": {
    "id": "4d8a4005-757c-48d7-829f-21d2f6bd7818"
   },
   "source": [
    "- 가열로 호기는 중요한 변수가 아닌 것 같다. 1\\~3호기 사이에 가열대온도, 균열대온도, 추출온도는 물론, 불량률에도 유의한 차이가 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c415a5b6-c893-48b1-bd45-71ef903f47b3",
   "metadata": {
    "id": "c415a5b6-c893-48b1-bd45-71ef903f47b3"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f519cd-635f-405b-ba9d-b37329778b35",
   "metadata": {
    "id": "c0f519cd-635f-405b-ba9d-b37329778b35"
   },
   "source": [
    "## pd crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ab3b92-cc13-470a-8fcd-412f08b55bed",
   "metadata": {
    "id": "b8ab3b92-cc13-470a-8fcd-412f08b55bed"
   },
   "outputs": [],
   "source": [
    "def print_crosstab(data, var):\n",
    "    print(pd.crosstab(index = data[\"SCALE\"], columns = data[var]))\n",
    "    print()\n",
    "    print(pd.crosstab(index = data[\"SCALE\"], columns = data[var], normalize = \"columns\").round(3))\n",
    "\n",
    "print_crosstab(df_edited, 'ROLLING_DESCALING')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee85ed5-6c19-4977-8934-36017c11c045",
   "metadata": {
    "id": "1ee85ed5-6c19-4977-8934-36017c11c045"
   },
   "source": [
    "## 카이제곱검정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4811c4-525b-4647-a189-e663f2a5731b",
   "metadata": {
    "id": "eb4811c4-525b-4647-a189-e663f2a5731b"
   },
   "outputs": [],
   "source": [
    "table = pd.crosstab(df_edited['SCALE'], df_edited['WORK_GR'])\n",
    "chi2, pval, dof, expected = stats.chi2_contingency(table)\n",
    "\n",
    "print('카이제곱통계량:', chi2)\n",
    "print('P-value:', pval)\n",
    "\n",
    "\n",
    "\n",
    "table = pd.crosstab(df_edited['SCALE'], df_edited['FUR_NO_ROW'])\n",
    "chi2, pval, dof, expected = stats.chi2_contingency(table)\n",
    "\n",
    "print('카이제곱통계량:', chi2)\n",
    "print('P-value:', pval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ea6610-96c8-4ff3-9824-f280c5126fa6",
   "metadata": {
    "id": "20ea6610-96c8-4ff3-9824-f280c5126fa6"
   },
   "source": [
    "- 작업조, 작업순번 모두 카이제곱 동질성검정을 실시한 결과 p-val이 0.05 이상으로 귀무가설을 채택했다: 작업조와 불량률, 작업순번과 불량률 사이에는 통계적으로 유의미한 차이가 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d268779a-04b9-479f-a47b-06896f957697",
   "metadata": {
    "id": "d268779a-04b9-479f-a47b-06896f957697"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6c7c5d-db89-4d50-99e0-3f3663f5f3e8",
   "metadata": {
    "id": "8b6c7c5d-db89-4d50-99e0-3f3663f5f3e8"
   },
   "source": [
    "### ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81b67e0-e892-48da-96c7-c4fae585fcf8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T13:24:38.059000Z",
     "start_time": "2021-08-31T13:24:38.055311Z"
    },
    "id": "f81b67e0-e892-48da-96c7-c4fae585fcf8"
   },
   "outputs": [],
   "source": [
    "spring_pm10 = df_edited[df_edited['SEASON']=='spring']['PM10']\n",
    "summer_pm10 = df_edited[df_edited['SEASON']=='summer']['PM10']\n",
    "autumn_pm10 = df_edited[df_edited['SEASON']=='autumn']['PM10']\n",
    "winter_pm10 = df_edited[df_edited['SEASON']=='winter']['PM10']\n",
    "\n",
    "fstat, pval = stats.f_oneway(spring_pm10, summer_pm10, autumn_pm10, winter_pm10)\n",
    "print(\"F-통계량: {0:.3f},\\t p-value: {1:.3f}\".format(fstat, pval))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582bb973-675f-45e2-bb14-3d09708e764d",
   "metadata": {
    "id": "582bb973-675f-45e2-bb14-3d09708e764d"
   },
   "source": [
    "- **p-value가 0이므로 네 집단 간의 PM10 평균 사이에 통계적으로 유의미한 차이가 있다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3178bee8-72d4-4d27-b9bb-5772b27f9ebc",
   "metadata": {
    "id": "3178bee8-72d4-4d27-b9bb-5772b27f9ebc"
   },
   "source": [
    "### Tukey's HSD 사후검정\n",
    "- 네 집단 사이에 차이가 있는 건 알겠는데, 어느 집단과 어느 집단 사이에 차이가 있는지 알아내기 위한 기법이 사후검정이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2260f333-faf3-4822-8725-44424b95beea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T13:24:38.451844Z",
     "start_time": "2021-08-31T13:24:38.377818Z"
    },
    "id": "2260f333-faf3-4822-8725-44424b95beea",
    "outputId": "c65acc1a-7b3e-422c-c481-ed95fa6b4bd7"
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# np.concatenate로 데이터 합치기\n",
    "v = np.concatenate([spring_pm10, summer_pm10, autumn_pm10, winter_pm10])\n",
    "\n",
    "# 데이터 개수만큼 레이블 준비\n",
    "labels = ['spring'] * len(spring_pm10) + ['summer'] * len(summer_pm10) + ['autumn'] * len(autumn_pm10) + ['winter'] * len(winter_pm10)\n",
    "\n",
    "# 사후검정 수행\n",
    "tukey_results = pairwise_tukeyhsd(v, labels, 0.05)\n",
    "print(tukey_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07629be7-0aa5-46f9-a228-4192a3b00100",
   "metadata": {
    "id": "07629be7-0aa5-46f9-a228-4192a3b00100"
   },
   "source": [
    "> **REJECT가 모두 True이므로, 서로 평균이 같은 그룹이 전혀 없는 것이다. 계절을 기준으로 집단을 묶으면, PM10이 모두 유의미하게 달라진다.**\n",
    "> - *분명히 계절이 PM10에 영향을 주는 것 같다.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da5ed83-4b76-4ece-ab80-5007c48f3c8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T13:24:38.598468Z",
     "start_time": "2021-08-31T13:24:38.535025Z"
    },
    "id": "7da5ed83-4b76-4ece-ab80-5007c48f3c8b",
    "outputId": "9eebe10a-e500-4ce6-8f1e-5ebf81c2c1f3"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.boxplot(x='SEASON', y='PM10', data=df_edited)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8e18ac-fa99-4a0e-9098-cb122593f12c",
   "metadata": {
    "id": "3e8e18ac-fa99-4a0e-9098-cb122593f12c"
   },
   "source": [
    "- 서로 비슷해보이지만, Tukey's test 결과에 따라 이 네 집단 사이에 서로 모두 통계적으로 유의미한 차이가 있다고 주장할 수 있다.\n",
    "    - **겨울이 제일 미세먼지 농도가 높고, 여름이 제일 낮다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff83dabf-b35f-4f58-b6b2-856ceee27953",
   "metadata": {
    "id": "ff83dabf-b35f-4f58-b6b2-856ceee27953"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d537f631-014e-46c7-8299-047d74d1c182",
   "metadata": {
    "id": "d537f631-014e-46c7-8299-047d74d1c182"
   },
   "source": [
    "### ANOVA & Tukey's HSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5441be5f-5e79-460f-af0e-5aa8fb7f3cf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T13:24:39.878772Z",
     "start_time": "2021-08-31T13:24:39.874120Z"
    },
    "id": "5441be5f-5e79-460f-af0e-5aa8fb7f3cf9",
    "outputId": "97685067-ef13-430d-eb78-334a3d0d2df2"
   },
   "outputs": [],
   "source": [
    "NW_pm10 = df_edited[df_edited['WIND_DIR']=='NW']['PM10']\n",
    "NE_pm10 = df_edited[df_edited['WIND_DIR']=='NE']['PM10']\n",
    "SW_pm10 = df_ㅊㅍedited[df_edited['WIND_DIR']=='SW']['PM10']\n",
    "SE_pm10 = df_edited[df_edited['WIND_DIR']=='SE']['PM10']\n",
    "\n",
    "fstat, pval = stats.f_oneway(spring_pm10, summer_pm10, autumn_pm10, winter_pm10)\n",
    "print(\"F-통계량: {0:.3f},\\t p-value: {1:.3f}\".format(fstat, pval))\n",
    "\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# np.concatenate로 데이터 합치기\n",
    "v = np.concatenate([NW_pm10, NE_pm10, SW_pm10, SE_pm10])\n",
    "\n",
    "# 데이터 개수만큼 레이블 준비\n",
    "labels = ['NW'] * len(NW_pm10) + ['NE'] * len(NE_pm10) + ['SW'] * len(SW_pm10) + ['SE'] * len(SE_pm10)\n",
    "\n",
    "# 사후검정 수행\n",
    "tukey_results = pairwise_tukeyhsd(v, labels, 0.05)\n",
    "print(tukey_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b438f96b-60c6-44b1-8259-af056618a4ca",
   "metadata": {
    "id": "b438f96b-60c6-44b1-8259-af056618a4ca"
   },
   "source": [
    "- REJECT가 False인 관계가 두 개가 있다.\n",
    "    - **북서풍(NW)과 남서풍(SW), 그리고 북서풍(NW)과 남동풍(SE) 간에는 PM10의 유의미한 차이가 없었다.**\n",
    "    - 원래 예상했던 정보는 \"서풍과 동풍\" 간의 유의미한 차이를 예상했는데, 북서풍과 남동풍 사이 차이가 크지 않았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ec357e-05f1-4e7d-b6ad-f7f79602864c",
   "metadata": {
    "id": "61ec357e-05f1-4e7d-b6ad-f7f79602864c"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "breast_cancer = load_breast_cancer()\n",
    "breast_cancer_data = breast_cancer.data\n",
    "breast_cancer_label = breast_cancer.target\n",
    "breast_cancer_feature = breast_cancer.feature_names\n",
    "df = pd.DataFrame(data=breast_cancer_data,\n",
    "                  columns = breast_cancer_feature)\n",
    "df['diagnosis'] = breast_cancer_label\n",
    "\n",
    "df.shape\n",
    "\n",
    "df.head(3)\n",
    "\n",
    "df['diagnosis'].value_counts()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('diagnosis', axis=1)\n",
    "y = df['diagnosis']\n",
    "\n",
    "X_train, X_test, y_train, y_test =\\\n",
    "    train_test_split(X, y, test_size = 0.3, stratify=y)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "X_train[:] = sc.fit_transform(X_train)\n",
    "X_test[:] = sc.transform(X_test)\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X_train_const = sm.add_constant(X_train)\n",
    "\n",
    "lm = sm.OLS(y_train.values, X_train_const)\n",
    "lm = lm.fit()\n",
    "lm.summary()\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "vif['VIF Factor'] = [variance_inflation_factor(X_train.values, i)\n",
    "                     for i in range(X_train.shape[1])]\n",
    "vif['Features'] = X_train.columns\n",
    "\n",
    "vif.sort_values(by='VIF Factor', ascending=False)[:10]\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca10 = PCA(n_components=10)\n",
    "pca10.fit(X_train)\n",
    "\n",
    "pca10_df = pd.DataFrame()\n",
    "pca10_df['no_PC'] = [i for i in range(1,11)]\n",
    "pca10_df['EigenValues']  = pca10.explained_variance_\n",
    "pca10_df['EigenValueRatio'] = pca10.explained_variance_ratio_\n",
    "pca10_df['CumEigenValueRatio'] = np.cumsum(pca10.explained_variance_ratio_)\n",
    "\n",
    "pca10_df.round(3)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(pca10_df['no_PC'], pca10_df['EigenValues'],\n",
    "         label='EigenValues')\n",
    "\n",
    "X_pca10 = pca10.transform(X_train)\n",
    "X_pca10_test = pca10.transform(X_test)\n",
    "\n",
    "X_pca10_const = sm.add_constant(X_pca10)\n",
    "\n",
    "lm10 = sm.OLS(y_train, X_pca10_const)\n",
    "lm10 = lm10.fit()\n",
    "lm10.summary()\n",
    "\n",
    "from vega_datasets import data\n",
    "\n",
    "from vega_datasets import data\n",
    "from tabulate import tabulate\n",
    "\n",
    "airports = data.airports()\n",
    "\n",
    "print(tabulate(airports.sample(10), headers='keys'))\n",
    "\n",
    "airports.shape\n",
    "\n",
    "# 단순표본추출\n",
    "airports.sample(frac=0.0023)\n",
    "\n",
    "airports['lat_cat'] = pd.qcut(airports['latitude'], 3)\n",
    "\n",
    "airports['lat_cat'].value_counts()\n",
    "\n",
    "# stratified sampling\n",
    "# group by 기준 ?% 씩 추출\n",
    "airports.groupby('lat_cat').sample(frac=0.05)\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "wine_load = load_wine()\n",
    "\n",
    "wine = pd.DataFrame(wine_load.data, columns=wine_load.feature_names)\n",
    "wine['Class'] = wine_load.target\n",
    "wine['Class'] = wine['Class'].map({0: 'class_0', 1: 'class_1', 2: 'class_2'})\n",
    "\n",
    "plt.boxplot(wine['color_intensity'], whis=1.5)\n",
    "plt.title('color intensity')\n",
    "plt.show()\n",
    "\n",
    "def outliers_iqr(dt, col):\n",
    "    quartile_1, quartile_3 = np.percentile(dt[col], [25, 75])\n",
    "    iqr = quartile_3 - quartile_1\n",
    "    lower_whis = quartile_1 - (iqr * 1.5)\n",
    "    upper_whis = quartile_3 + (iqr * 1.5)\n",
    "    outliers = dt[(dt[col] > upper_whis) | (dt[col] < lower_whis)]\n",
    "    return outliers[[col]]\n",
    "\n",
    "outliers = outliers_iqr(wine, 'color_intensity')\n",
    "outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe331839-5a42-4eef-b011-14768f666eba",
   "metadata": {
    "id": "fe331839-5a42-4eef-b011-14768f666eba"
   },
   "source": [
    "# Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8f6eba-e9c1-4601-9ea9-f3b962fa3ff4",
   "metadata": {
    "id": "2f8f6eba-e9c1-4601-9ea9-f3b962fa3ff4"
   },
   "source": [
    "## 2 Sample t-test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e21111-7f99-478b-a5d8-21af612c1782",
   "metadata": {
    "id": "97e21111-7f99-478b-a5d8-21af612c1782"
   },
   "source": [
    "- 1 sample t-test이 단일 집단에 대한 모평균을 검정하는 것과 다르게, 2 sample t-test는 **두 집단에 대한 평균 차이를 검정**한다.\n",
    "    - 두 확률표본이 두 모집단으로부터 독립적으로 관측되었을 경우 \"독립 2표본 검정\"을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d28c5ba-592d-4ce0-8de0-46055947fedc",
   "metadata": {
    "id": "6d28c5ba-592d-4ce0-8de0-46055947fedc"
   },
   "source": [
    "**절차**\n",
    "1. 정규성 검정\n",
    "2. 등분산성 검정\n",
    "    - 정규분포를 따르는 경우: F test\n",
    "    - 정규분포를 따르지 않는 경우: Levene's test\n",
    "3. 평균 검정\n",
    "    - 두 집단이 등분산인 경우: Student's t test\n",
    "    - 두 집단의 분산이 다른 경우: Welch's t test\n",
    "4. 설정된 가설 검정\n",
    "    - $H_0$: 두 집단의 평균은 동일\n",
    "    - $H_1$: 두 집단의 평균은 다름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c38bf3-217d-4e26-9053-50ccc6e19eb2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "error",
     "timestamp": 1692936553001,
     "user": {
      "displayName": "Kang Jihun",
      "userId": "00356180714370444370"
     },
     "user_tz": -540
    },
    "id": "69c38bf3-217d-4e26-9053-50ccc6e19eb2",
    "outputId": "b17603cd-c9a8-45b0-b0ee-620fc3853819"
   },
   "outputs": [],
   "source": [
    "#### 2 sample t test의 검정 통계량\n",
    "$$ t = \\dfrac {(\\bar x_1 - \\bar x_2) - (\\mu_1 - \\mu_2)}{\\sqrt {\\dfrac{s^2_1}{n_1} + \\dfrac{s^2_2}{n_2}}} $$<br>\n",
    "- $\\bar x$: 표본평균\n",
    "- $n$: 표본 크기\n",
    "- $\\mu$: 미지(unknown)의 모평균 => *이걸 찾고 싶음*\n",
    "- $s$: 표본표준편차\n",
    "> 위의 검정통계량 수식을 바탕으로 유의성 검정을 하는데, 모표준편차를 알고 있는 경우 $s$를 $\\sigma$로 바꾸고 Z 통계량을 사용한다. 모표준편차를 모르는 경우 위와 같이 표본표준편차 $s$를 사용하고 t 통계량을 사용한다.\n",
    "\n",
    "#### 모평균($\\mu$)의 신뢰구간\n",
    "$$ (\\bar x_1 - \\bar x_2) \\pm t^* \\sqrt {\\dfrac{s^2_1}{n_1} + \\dfrac{s^2_2}{n_2}} $$\n",
    "- 위에서 t는 자유도 $n_1 + n_2 -2$의 t 통계량이다.\n",
    "    - $t^* = t(n_1+n_2 -2,\\;\\alpha/2)$ㅡ\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rc('font', family='AppleGothic')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b07e48-6a54-407f-98a2-7bd0f283f9af",
   "metadata": {
    "id": "34b07e48-6a54-407f-98a2-7bd0f283f9af"
   },
   "source": [
    "- 데이터 표준화 이후, 주성분 분석을 실시하여 2차원(1 vs 2 주성분, 1 vs 3 주성분, …) 산점도를 그려라.(목표변수로 색 구분), + 주성분 naming\n",
    "- 데이터 : 유방암.CSV(변수: 31개, 자료 수: 320개)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b88c1d-37ad-40cb-91b8-85290df195e0",
   "metadata": {
    "id": "65b88c1d-37ad-40cb-91b8-85290df195e0"
   },
   "source": [
    "radius : 반경, texture :gray-scale 값들의 표준편차, perimeter: 둘레, area : 면적, smoothness : 반경길이의 국소적 변화, compactness : 조그만 정도, concavity : 윤곽의 오목한 부분의 정도, points : 오목한 점의 수, symmetry : 대칭, dimension : 프랙탈 차원, _mean : 평균값, _se : 표준오차, _worst : 각 세포별 구분들에서 제일 큰 3개를 평균낸 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93510e76-d556-4390-9421-37e0300466e2",
   "metadata": {
    "id": "93510e76-d556-4390-9421-37e0300466e2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "266638b2-7ee2-4a66-96c1-17b17774616f",
   "metadata": {
    "id": "266638b2-7ee2-4a66-96c1-17b17774616f"
   },
   "source": [
    "# 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ba3352-85a6-4231-896f-d84417053616",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:14:33.610771Z",
     "start_time": "2021-08-28T15:14:33.571140Z"
    },
    "id": "f7ba3352-85a6-4231-896f-d84417053616",
    "outputId": "0f323a26-e444-4545-e6b2-d5ddf2080635"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('3data/유방암.csv', encoding='euc-kr')\n",
    "df.head()\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e68ece4-a5be-4c05-89da-99ec9ed42d39",
   "metadata": {
    "id": "9e68ece4-a5be-4c05-89da-99ec9ed42d39"
   },
   "source": [
    "- 관측치가 320개인데, 설명변수가 무려 30개다.\n",
    "    - **관측치가 엄청나게 희소하기 때문에, 차원축소 기법을 사용하여 설명변수의 갯수를 줄여줄 필요가 있어 보인다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3988c42-c197-4c1d-8d6d-4e0a1e7a40d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:14:34.744943Z",
     "start_time": "2021-08-28T15:14:34.734882Z"
    },
    "id": "b3988c42-c197-4c1d-8d6d-4e0a1e7a40d1",
    "outputId": "7fb4e973-ff5e-4fc9-b7e7-a40cb74ecce8"
   },
   "outputs": [],
   "source": [
    "# null 값은 없다.\n",
    "df.isnull().sum().sum()\n",
    "\n",
    "df.info()\n",
    "\n",
    "df.describe()\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.countplot('diagnosis', data=df)\n",
    "plt.show()\n",
    "\n",
    "print(df['diagnosis'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c32fbaa-663f-4c73-9ff8-0c442b611be7",
   "metadata": {
    "id": "4c32fbaa-663f-4c73-9ff8-0c442b611be7"
   },
   "source": [
    "- 음성이 212개, 양성이 108개로 목표변수간 관측치 수가 약 2배 정도 차이 난다.\n",
    "    - train, test set 분리할 때 stratify해야될 수도 있겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91a7b38-44d5-478b-aabc-1866d325224b",
   "metadata": {
    "id": "f91a7b38-44d5-478b-aabc-1866d325224b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a5ed35a-a5f0-4f3b-b02c-3755a9d3e31c",
   "metadata": {
    "id": "7a5ed35a-a5f0-4f3b-b02c-3755a9d3e31c"
   },
   "source": [
    "# 2. 데이터 스플릿 / 전처리\n",
    "- train test split시 **class imbalance problem을 고려해서 stratify=y**로 설정한다.\n",
    "- pca가 편향되지 않고 더 높은 성능을 발휘하려면, feature scaling이 되어 있어야 한다.\n",
    "    - 따라서 **설명변수들을 StandardScaling** 해준다.\n",
    "    - train/test split의 의미를 명확히 하기 위해, X_train은 fit_transform해주고, X_test는 transform만 해준다. ((train set을 기반으로 fitting한 StandardScaler를 test set에도 그대로 적용한다.))\n",
    "- 목표변수가 str('양성', '음성')이기 때문에, 이걸 양성=1, 음성=0으로 정수 라벨링 해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf3c882-46e6-41e6-93b7-4bdc852b8997",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:14:36.220236Z",
     "start_time": "2021-08-28T15:14:36.205820Z"
    },
    "id": "6cf3c882-46e6-41e6-93b7-4bdc852b8997",
    "outputId": "d95dd780-75f9-4ed2-e8e1-99892ce4262b"
   },
   "outputs": [],
   "source": [
    "X = df.drop('diagnosis', axis=1)\n",
    "y = df['diagnosis']\n",
    "\n",
    "X_train, X_test, y_train, y_test =\\\n",
    "        train_test_split(X, y, test_size=0.3, stratify=y)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "# Standard Scaler\n",
    "stscl = StandardScaler()\n",
    "\n",
    "X_train[:] = stscl.fit_transform(X_train)\n",
    "X_test[:] = stscl.transform(X_test)\n",
    "\n",
    "# 목표변수 라벨링\n",
    "y_train = y_train.map({'양성': 1, '음성': 0})\n",
    "y_test = y_test.map({'양성': 1, '음성': 0})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9486f617-7466-4273-8dff-f3c6a6e2663f",
   "metadata": {
    "id": "9486f617-7466-4273-8dff-f3c6a6e2663f"
   },
   "source": [
    "# 3. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99308bb9-d5e4-493a-807b-0978f151ac9b",
   "metadata": {
    "id": "99308bb9-d5e4-493a-807b-0978f151ac9b"
   },
   "source": [
    "## 3-1. 다중회귀 모델을 구현해서 coeff p-value를 확인해본다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad1c87a-c44e-45a7-96d9-e69d5005b700",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:14:37.541216Z",
     "start_time": "2021-08-28T15:14:37.501981Z"
    },
    "id": "dad1c87a-c44e-45a7-96d9-e69d5005b700",
    "outputId": "36e001e1-cdd5-43e7-c30d-e0153753feea"
   },
   "outputs": [],
   "source": [
    "X_train_const = sm.add_constant(X_train)\n",
    "\n",
    "lm = sm.OLS(y_train.values, X_train_const)\n",
    "lm = lm.fit()\n",
    "lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8979c36a-fdeb-4077-8c84-26899f3c62eb",
   "metadata": {
    "id": "8979c36a-fdeb-4077-8c84-26899f3c62eb"
   },
   "source": [
    "> 30개 설명변수를 이용해 구현한 다중회귀모델은 R^2가 0.8, Adj R^2가 0.77이 나온다.<br>\n",
    "\n",
    "> **워낙 데이터셋이 차원이 크고 관측치가 희소하기 때문에, 총 30개 설명변수들 중 p-value가 0.05를 넘는 변수가 10개가 안 된다.** => 이것만으로도 차원축소가 필요한 데이터라는 생각이 들지만, 변수간 다중공선성이 크게 나타나는지까지 확인해보려고 한다. 만약 VIF가 아주 높다면, 더욱이 변수 개수를 줄여야 한다는 의미일 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd1c389-b97d-4754-8c06-a09d60354d71",
   "metadata": {
    "id": "4bd1c389-b97d-4754-8c06-a09d60354d71"
   },
   "source": [
    "## 3-2. 변수간 다중공선성 확인 (VIF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89020977-7bdf-4eb7-91a7-dc693e0358fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:14:38.181472Z",
     "start_time": "2021-08-28T15:14:38.138651Z"
    },
    "id": "89020977-7bdf-4eb7-91a7-dc693e0358fc",
    "outputId": "91361059-ece5-4167-9495-9da3f3a1f897"
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "vif['VIF Factor'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\n",
    "vif['features'] = X_train.columns\n",
    "vif.sort_values(by='VIF Factor', ascending=False)[:10]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a953d9d-5c97-45ac-b48c-8748f31aa185",
   "metadata": {
    "id": "5a953d9d-5c97-45ac-b48c-8748f31aa185"
   },
   "source": [
    "> VIF는 10을 넘으면 변수간 다중공선성이 발생한다는 의미로 이해하곤 하는데, 전체 30개 변수 중 VIF가 10을 넘지 않는 변수가 5개 뿐이다.<br>\n",
    "\n",
    "\n",
    "> **다중회귀모델의 coeff뿐 아니라 VIF도 위의 데이터셋에 차원축소(혹은 변수선택,변수제거 등) 기법이 필요하다는 것을 나타내고 있다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d529a26-57f9-4b34-a6cd-7b618d13c7af",
   "metadata": {
    "id": "1d529a26-57f9-4b34-a6cd-7b618d13c7af"
   },
   "source": [
    "# 4. PCA 구현\n",
    "- 비지도학습 차원축소 기법인 PCA를 구현한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc7f33f-34f5-4c96-b274-b241c0f8bf0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:14:38.767850Z",
     "start_time": "2021-08-28T15:14:38.753644Z"
    },
    "id": "2bc7f33f-34f5-4c96-b274-b241c0f8bf0b",
    "outputId": "7928bba4-5e57-4940-e314-7fcb1bbd202c"
   },
   "outputs": [],
   "source": [
    "# n_components 10 pca를 구현해본다\n",
    "pca10 = PCA(n_components=10)\n",
    "pca10.fit(X_train)\n",
    "\n",
    "# PC 10개의 dataframe\n",
    "pca10_df = pd.DataFrame()\n",
    "pca10_df['no_PC'] = [i for i in range(1,11)]\n",
    "pca10_df['EigenValues'] = pca10.explained_variance_\n",
    "pca10_df['EigenValueRatio'] = pca10.explained_variance_ratio_\n",
    "pca10_df['CumEigenValueRatio'] = \\\n",
    "        np.cumsum(pca10.explained_variance_ratio_)\n",
    "pca10_df.round(3)\n",
    "\n",
    "# PCA10일 때 => Cumulative Explained Variance visualization\n",
    "total = sum(pca10.explained_variance_)\n",
    "exp_var = [(i / total) for i in sorted(pca10.explained_variance_,\n",
    "                                       reverse=True)]\n",
    "cum_exp_var = np.cumsum(exp_var)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(range(1, 11), exp_var, alpha=0.7, align='center',\n",
    "       label='Individual explained variance')\n",
    "plt.step(range(1, 11), cum_exp_var, where='mid',\n",
    "        label='Cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio', fontsize=14)\n",
    "plt.xlabel('Principal component index', fontsize=14)\n",
    "plt.legend(loc='best', fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 다른 방식의 visualization\n",
    "plt.plot(pca10_df['no_PC'], pca10_df['EigenValues'],\n",
    "         label='교윳값')\n",
    "plt.ylabel('고윳값')\n",
    "plt.xlabel('주성분 번호')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b955d0b3-2c32-4683-a2aa-b60165038aaf",
   "metadata": {
    "id": "b955d0b3-2c32-4683-a2aa-b60165038aaf"
   },
   "source": [
    "- 주성분이 총 10개일 때, 주성분이 3개에서 **4개가 되는 구간부터 주성분 하나당 증가하는 설명력이 10% 미만**으로 떨어진다.\n",
    "- 6개가 되는 구간부터 누적 설명력이 93%를 넘어선다.<br><br>\n",
    "\n",
    "- **주성분 하나당 설명력이 10%가 넘도록 n_components=3인 pca를 하나 더 구현해놓고, pca10과 비교해보겠다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0a1b24-1718-40cf-a325-0fb3bb063dfd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:14:39.959771Z",
     "start_time": "2021-08-28T15:14:39.948403Z"
    },
    "id": "9c0a1b24-1718-40cf-a325-0fb3bb063dfd",
    "outputId": "3759faf7-393f-4705-8a36-2b7bcd905a92"
   },
   "outputs": [],
   "source": [
    "# pca3 구현\n",
    "pca3 = PCA(n_components=3)\n",
    "pca3.fit(X_train)\n",
    "\n",
    "# pca10, pca3으로 각각 데이터셋을 주성분에 투영(변환)시킨다.\n",
    "X_pca10 = pca10.transform(X_train)\n",
    "X_pca10_test = pca10.transform(X_test)\n",
    "X_pca3 = pca3.transform(X_train)\n",
    "X_pca3_test = pca3.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e41526-bae2-46ac-a3c9-ac4cff7c6151",
   "metadata": {
    "id": "62e41526-bae2-46ac-a3c9-ac4cff7c6151"
   },
   "source": [
    "# 5. 차원 축소의 효과 탐색"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f510229-f1e4-4cc3-af3a-a98f31089f33",
   "metadata": {
    "id": "4f510229-f1e4-4cc3-af3a-a98f31089f33"
   },
   "source": [
    "## 5-1. 10개 주성분으로 변환시킨 데이터셋으로 다중회귀모델 coeff 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0526a75a-5df6-48f8-bf2f-d7676ba1c037",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:14:41.021264Z",
     "start_time": "2021-08-28T15:14:40.994092Z"
    },
    "id": "0526a75a-5df6-48f8-bf2f-d7676ba1c037",
    "outputId": "dac58a4b-2d01-4fd5-edbe-b289e1b82a0f"
   },
   "outputs": [],
   "source": [
    "X_pca10_const = sm.add_constant(X_pca10)\n",
    "\n",
    "lm10 = sm.OLS(y_train, X_pca10_const)\n",
    "lm10 = lm10.fit()\n",
    "lm10.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc57542d-3d26-4fd9-b30e-7b75bc9a1234",
   "metadata": {
    "id": "bc57542d-3d26-4fd9-b30e-7b75bc9a1234"
   },
   "source": [
    "- 이전에 30개 설명변수를 이용했던 다중회귀모델의 R^2는 0.8, Adj R^2는 0.77이었다.\n",
    "- 무려 변수를 20개나 줄인 위의 모델은 R^2가 0.7이 나오고, 10개 중 6개의 변수가 0.05 이하의 p-value를 갖고 있다(유의하다).\n",
    "    - 10개 중 4개의 p-value가 0.05보다 높은 것으로 봐서, 주성분을 10개보다 더 적게 줄여도 되겠다는 생각이 든다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ffb92d-c0b6-4fe1-8b65-944bac816d14",
   "metadata": {
    "id": "d7ffb92d-c0b6-4fe1-8b65-944bac816d14"
   },
   "source": [
    "## 5-2. 3개 주성분으로 변환시킨 데이터셋으로 다중회귀모델 coeff 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa51cd5-455c-43e2-91e4-dc9704a4d196",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:14:42.260805Z",
     "start_time": "2021-08-28T15:14:42.238307Z"
    },
    "id": "4aa51cd5-455c-43e2-91e4-dc9704a4d196",
    "outputId": "b7aec731-ca12-4b44-ce2a-1c33ab76276b"
   },
   "outputs": [],
   "source": [
    "X_pca3_const = sm.add_constant(X_pca3)\n",
    "\n",
    "lm3 = sm.OLS(y_train, X_pca3_const)\n",
    "lm3 = lm3.fit()\n",
    "lm3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c3d125-8d42-4d53-af6b-5567a9599dcb",
   "metadata": {
    "id": "13c3d125-8d42-4d53-af6b-5567a9599dcb"
   },
   "source": [
    "- 주성분 10개 모델의 R^2가 0.7이었는데, 3개일 때 0.65이면 나름 준수한 성능인 것 같다.\n",
    "- 모든 변수의 p-value가 0에 가깝다.<br><br>\n",
    "\n",
    "- **물론 pca의 단점은..** 차원이 축소된 설명변수들에 대해 **해석하기가 쉽지 않다**는 것이다. 30개 변수의 분산을 가장 잘 설명하는 3개의 주성분을 기반으로 모델을 구현하면 예측 성능은 좋아질지 모르지만, 그 3개의 주성분이라는 게 실제로 어떤 의미를 갖는지는 알기가 어렵다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c722bd1a-16b8-4bef-a503-5a1095136bb8",
   "metadata": {
    "id": "c722bd1a-16b8-4bef-a503-5a1095136bb8"
   },
   "source": [
    "## 5-3. Scatter Plot -- PC1\\~2,  PC1\\~3,  PC2\\~3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9f0d67-1770-4b7d-a4b8-a53f55601054",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:14:43.318932Z",
     "start_time": "2021-08-28T15:14:43.304394Z"
    },
    "id": "fd9f0d67-1770-4b7d-a4b8-a53f55601054",
    "outputId": "d19bc522-2e33-419f-d338-1e2d8d52cd28"
   },
   "outputs": [],
   "source": [
    "pca3_df = pd.DataFrame(X_pca3, columns=['PC1', 'PC2', 'PC3'])\n",
    "pca3_df = pd.concat(objs=[y_train.reset_index(drop=True),\n",
    "                          pca3_df], axis=1)\n",
    "pca3_df.head()\n",
    "\n",
    "## PC1 ~ PC2 scatterplot with class labels\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='diagnosis', style='diagnosis', s=80, data=pca3_df)\n",
    "plt.xlabel('Principal Component 1', fontsize=15)\n",
    "plt.ylabel('Principal Component 2', fontsize=15)\n",
    "plt.show()\n",
    "\n",
    "## PC1 ~ PC3 scatterplot with class labels\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x='PC1', y='PC3', hue='diagnosis', style='diagnosis', s=80, data=pca3_df)\n",
    "plt.xlabel('Principal Component 1', fontsize=15)\n",
    "plt.ylabel('Principal Component 3', fontsize=15)\n",
    "plt.show()\n",
    "\n",
    "## PC2 ~ PC3 scatterplot with class labels\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x='PC2', y='PC3', hue='diagnosis', style='diagnosis', s=80, data=pca3_df)\n",
    "plt.xlabel('Principal Component 2', fontsize=15)\n",
    "plt.ylabel('Principal Component 3', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea0efc0-778c-4331-af64-1e5b7baf6518",
   "metadata": {
    "id": "8ea0efc0-778c-4331-af64-1e5b7baf6518"
   },
   "source": [
    "> **위의 3개 scatterplot을 보면, PC1과 PC2; PC1과 PC3는 딱 봐도 클래스 레이블이 잘 나누어져 있다. => 직선 하나만 그어도(기본적인 선형분류를 사용해도) 0과 1 클래스가 꽤나 잘 분리될 것 같다. 다시 말해, 차원축소가 아주 성공적으로 이뤄졌다.** <br>\n",
    "\n",
    "> PC2와 PC3으로 scatterplot을 그린 경우에는 클래스 레이블이 뒤섞여 있는 모습이다. 제대로 분류가 안될 것 같다. 역시 PC1이 지닌 약 43%의 explained variance를 무시할 수가 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd2da81-e661-4876-bf3b-d64ae83ff49b",
   "metadata": {
    "id": "6dd2da81-e661-4876-bf3b-d64ae83ff49b"
   },
   "source": [
    "**결론적으로..** 주성분 10개 말고, 주성분을 3개만 가져가도 분류모델 구현에 충분할 것 같다고 판단된다. => 이제 아래에서 \"주성분 3개를 이용한 분류모델\"과 \"기존 30개 변수를 이용한 분류모델\"의 성능을 비교해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6701d4-a3d7-4cb0-9950-a37819df30de",
   "metadata": {
    "id": "1c6701d4-a3d7-4cb0-9950-a37819df30de"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7dbb8695-2779-41e4-a7c3-17bb63fdd513",
   "metadata": {
    "id": "7dbb8695-2779-41e4-a7c3-17bb63fdd513"
   },
   "source": [
    "# 6. 분류 모델 구현 및 성능 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e35f0a3-3a17-4906-afa4-8e6f5306b6fa",
   "metadata": {
    "id": "0e35f0a3-3a17-4906-afa4-8e6f5306b6fa"
   },
   "source": [
    "## 6-1. 로지스틱 회귀 모형 성능 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9428441-d2d4-48f0-85da-cae63f0c6a20",
   "metadata": {
    "id": "b9428441-d2d4-48f0-85da-cae63f0c6a20"
   },
   "source": [
    "- 30개 변수를 이용한 로지스틱 모형 성능을 측정해보려 했으나, iteration error(변수가 너무 많아서 모델이 최적화되지 못함)가 발생했다.\n",
    "- 주성분 10개짜리 데이터셋도 에러가 발생했다.\n",
    "- 따라서 로지스틱 회귀 모형은 주성분 3개짜리 데이터셋만 성능을 측정해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84afcbcc-d67b-490f-9846-f154c13749ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:14:45.566433Z",
     "start_time": "2021-08-28T15:14:45.539392Z"
    },
    "id": "84afcbcc-d67b-490f-9846-f154c13749ac",
    "outputId": "026ff5fd-f3f3-457d-fbfa-377782f759f6"
   },
   "outputs": [],
   "source": [
    "# 주성분 3개를 이용한 로지스틱 모형\n",
    "logit = sm.Logit(y_train, X_pca3_const)\n",
    "logit = logit.fit(method='newton')\n",
    "logit.summary()\n",
    "\n",
    "# 성능 확인\n",
    "from sklearn.metrics import accuracy_score\n",
    "X_pca3_test_const = sm.add_constant(X_pca3_test)\n",
    "\n",
    "y_pred = logit.predict(X_pca3_test_const)\n",
    "\n",
    "\n",
    "# logit의 pred 값이 특정 threshold 이상이면 예측 레이블 1 또는 0을 return하는 함수\n",
    "def cut_off(y, threshold):\n",
    "    Y = y.copy() # 이전의 y값이 변하지 않도록 copy\n",
    "    Y[Y > threshold] = 1\n",
    "    Y[Y <= threshold] = 0\n",
    "    return (Y.astype(int))\n",
    "\n",
    "\n",
    "# 테스트 데이터셋에 대한 정확도(accuracy) 측정\n",
    "print(\"y_pred값이 0.5 이상일 때 클래스 예측값을 1로 부여한다면..\")\n",
    "print(\"Accuracy 값은:\", accuracy_score(y_test, cut_off(y_pred, 0.5)))\n",
    "\n",
    "# logit score df\n",
    "logit_score = pd.DataFrame(index=['Logistic Regression'])\n",
    "logit_score['Accuracy'] = [accuracy_score(y_test, cut_off(y_pred, 0.5))]\n",
    "logit_score['Precision'] = [precision_score(y_test, cut_off(y_pred, 0.5))]\n",
    "logit_score['Recall'] = [recall_score(y_test, cut_off(y_pred, 0.5))]\n",
    "logit_score['Specificity'] = [recall_score(y_test, cut_off(y_pred, 0.5), pos_label=0)]\n",
    "logit_score['F1Score'] = [f1_score(y_test, cut_off(y_pred, 0.5))]\n",
    "\n",
    "logit_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2912868-d237-41bc-9a54-e92e19c02505",
   "metadata": {
    "id": "a2912868-d237-41bc-9a54-e92e19c02505"
   },
   "source": [
    "- **주성분 3개를 이용한 로지스틱 회귀 모형은 무려 96.8%의 정확도를 가지고 양성과 음성을 분류하고 있다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e2f958-0295-4708-9d8a-da5e741eec52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-26T13:17:40.409575Z",
     "start_time": "2021-08-26T13:17:40.360496Z"
    },
    "id": "f7e2f958-0295-4708-9d8a-da5e741eec52",
    "outputId": "4750ba67-1d8f-44bb-db3d-3ac1c4c04900",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, cut_off(y_pred, 0.5), pos_label=1)\n",
    "auc_score = roc_auc_score(y_test, cut_off(y_pred, 0.5))\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0,1], [0,1], 'r--')\n",
    "plt.title('ROC Curve - Logistic Regression')\n",
    "plt.text(0.6, 0.2, \"AUC: {}\".format(auc_score),\n",
    "         fontdict={'fontsize': 12})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d699ef9-ff39-40b4-aae1-265394a494ab",
   "metadata": {
    "id": "7d699ef9-ff39-40b4-aae1-265394a494ab"
   },
   "source": [
    "- **AUC도 0.96으로, 아주 높은 성능을 보여주고 있다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c11678-5da5-4aa3-aa72-4becd36782ba",
   "metadata": {
    "id": "e1c11678-5da5-4aa3-aa72-4becd36782ba"
   },
   "source": [
    "### 로지스틱 결론\n",
    "- 주성분 3개를 이용한 로지스틱 회귀 모델 구현 결과, test set에 대한 예측 정확도는 96.8%, AUC는 0.96으로 아주 뛰어난 성능을 보여 준다.\n",
    "    - (모델 예측값 0.5 이상을 클래스 레이블 1로 부여했다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d94cbc8-9526-4226-9c8c-36be69ba32b7",
   "metadata": {
    "id": "6d94cbc8-9526-4226-9c8c-36be69ba32b7"
   },
   "source": [
    "## 6-2. Decision Tree 성능 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ccb58f-ff95-464b-ad97-47831de105f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:24:25.017449Z",
     "start_time": "2021-08-28T15:24:24.139150Z"
    },
    "id": "38ccb58f-ff95-464b-ad97-47831de105f9",
    "outputId": "a39bf2b8-765c-4273-f9fd-1be843a26b83"
   },
   "outputs": [],
   "source": [
    "# 기존 30개 변수를 이용한 decision tree with GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tree30 = DecisionTreeClassifier(criterion='entropy',\n",
    "                                random_state=0)\n",
    "\n",
    "# GridSearchCV\n",
    "params = {'max_depth': [i for i in range(1,10)],\n",
    "          'min_samples_split': [i * 2 for i in range(2,15)]}\n",
    "tree30_grid = GridSearchCV(tree30, param_grid=params,\n",
    "                           cv=3, refit=True, return_train_score=True)\n",
    "\n",
    "tree30_grid.fit(X_train, y_train)\n",
    "\n",
    "# 주성분 3개를 이용한 decision tree with GridSearchCV\n",
    "\n",
    "tree_pca3 = DecisionTreeClassifier(criterion='entropy',\n",
    "                                   random_state=0)\n",
    "\n",
    "# GridSearchCV\n",
    "params = {'max_depth': [i for i in range(1,10)],\n",
    "          'min_samples_split': [i * 2 for i in range(2,15)]}\n",
    "tree_pca3_grid = GridSearchCV(tree_pca3, param_grid=params,\n",
    "                           cv=3, refit=True, return_train_score=True)\n",
    "\n",
    "tree_pca3_grid.fit(X_pca3, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84093c1a-823d-455b-99ac-84d59235a8d3",
   "metadata": {
    "id": "84093c1a-823d-455b-99ac-84d59235a8d3"
   },
   "source": [
    "### GridSearchCV Score 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a94588-7840-4ee1-9a1e-3f221365afb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:24:25.739582Z",
     "start_time": "2021-08-28T15:24:25.716500Z"
    },
    "id": "e9a94588-7840-4ee1-9a1e-3f221365afb6",
    "outputId": "8648ffc6-5e5c-4064-ee18-63153b2d4288"
   },
   "outputs": [],
   "source": [
    "# tree30 cv score\n",
    "tree30_score = pd.DataFrame(tree30_grid.cv_results_)\n",
    "tree30_score[['params', 'mean_train_score', 'mean_test_score', 'rank_test_score']].sort_values(by=['rank_test_score'])\n",
    "\n",
    "# tree_pca3 cv score\n",
    "tree_pca3_score = pd.DataFrame(tree_pca3_grid.cv_results_)\n",
    "tree_pca3_score[['params', 'mean_train_score', 'mean_test_score', 'rank_test_score']].sort_values(by=['rank_test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a618471-eccd-4797-87a4-b1ef6584b65e",
   "metadata": {
    "id": "8a618471-eccd-4797-87a4-b1ef6584b65e"
   },
   "source": [
    "### 테스트 데이터에 대한 성능 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e58aee-9b69-4733-b019-3e38707ef503",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:24:33.083856Z",
     "start_time": "2021-08-28T15:24:33.064833Z"
    },
    "id": "a8e58aee-9b69-4733-b019-3e38707ef503",
    "outputId": "1d0706fb-67b1-4dd4-bdac-81cf9edd338a"
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix - tree30\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "print(confusion_matrix(y_test, tree30_grid.predict(X_test)))\n",
    "print('ACC:', accuracy_score(y_test, tree30_grid.predict(X_test)))\n",
    "print('AUC:', roc_auc_score(y_test, tree30_grid.predict(X_test)))\n",
    "\n",
    "# Confusion Matrix - tree pca3\n",
    "print(confusion_matrix(y_test, tree_pca3_grid.predict(X_pca3_test)))\n",
    "print('ACC:', accuracy_score(y_test,\n",
    "                             tree_pca3_grid.predict(X_pca3_test)))\n",
    "print('AUC:', roc_auc_score(y_test,\n",
    "                            tree_pca3_grid.predict(X_pca3_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc97cbc5-a87d-4a2c-94e6-9f1b6aad48af",
   "metadata": {
    "id": "bc97cbc5-a87d-4a2c-94e6-9f1b6aad48af"
   },
   "source": [
    "### Decision tree with 30개 변수 Graphviz & Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34edef7f-320c-4185-88ad-a85b0fa4a661",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:24:35.272027Z",
     "start_time": "2021-08-28T15:24:34.709709Z"
    },
    "id": "34edef7f-320c-4185-88ad-a85b0fa4a661",
    "outputId": "d1621c28-4e02-4207-fef1-ccf850d1abb1"
   },
   "outputs": [],
   "source": [
    "import graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(tree30_grid.best_estimator_, out_file='tree30.dot',\n",
    "                feature_names=X_train.columns,\n",
    "                impurity=True, filled=True)\n",
    "with open('tree30.dot') as f:\n",
    "    dot_graph = f.read()\n",
    "display(graphviz.Source(dot_graph))\n",
    "\n",
    "# Feature Importances\n",
    "feat_imp = pd.DataFrame({'importance': tree30_grid.best_estimator_.feature_importances_}, X_train.columns)\n",
    "\n",
    "# 30개 변수 중 24개 변수의 feature importance가 0이다.\n",
    "feat_imp_selected = feat_imp[feat_imp['importance']!=0].sort_values(by='importance', ascending=False)\n",
    "\n",
    "feat_imp_selected\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(x = 'importance', y = feat_imp_selected.index,\n",
    "            data=feat_imp_selected)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54892e6-dc96-4e23-b579-03d88025e30d",
   "metadata": {
    "id": "c54892e6-dc96-4e23-b579-03d88025e30d"
   },
   "source": [
    "### Decision Tree 결론\n",
    "1. 주성분을 이용한 DT의 graphviz나 feature importance를 굳이 분석하지 않은 이유는,\n",
    "    - 어차피 이용된 변수 3개가 주성분이기 때문에, graph나 feature importance를 봐도 의미 있는 해석을 하기가 어렵다.\n",
    "2. Decision Tree는 독특하게 **30개의 변수를 이용한 모델이 더 성능이 좋다.**\n",
    "    - 정확도는 93%, AUC는 0.92가 나왔으며; 이는 **주성분 3개를 이용한 로지스틱 회귀 모형보다는 낮은 성능**이다.\n",
    "3. 30개 변수를 이용한 DT를 graphviz & feature importance 해본 결과,\n",
    "    - 30개 변수 중 **24개 변수의 feature importance가 0이 나왔다** & graphviz를 봐도 변수가 30개인 모델치고 엄청나게 간단한 그래프가 그려졌다 ==>>>> 사실상 6개 정도의 변수만 사용된 것이다.\n",
    "    - **가장 중요한 변수(가장 information gain이 높은 split)는 mean_concave_points가 -0.404보다 높은지 낮은지**다.\n",
    "    - 근데 여기서 -0.404는 standard-scaled된 값이기 때문에, 원래 값을 알고 싶다면 inverse_transform 해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b47f3f-dad5-4c0e-a97c-4f610097736f",
   "metadata": {
    "id": "d7b47f3f-dad5-4c0e-a97c-4f610097736f"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26401792-034a-4a76-b02d-b5a4bad199bb",
   "metadata": {
    "id": "26401792-034a-4a76-b02d-b5a4bad199bb"
   },
   "source": [
    "# 8월 26일자 업데이트\n",
    "\n",
    "- train_test_split에서 test_size 0.4;\n",
    "- Tree-based model 활용 예정이므로 feature scaling 하지 않음;<br><br>\n",
    "\n",
    "- PCA 이용하지 않고 기존 데이터셋으로 tree model 구현;\n",
    "- feature importance 분석;\n",
    "- graphviz를 통해 tree model 분석;\n",
    "- 분류모델 성능지표 확인 (confusion matrix, accuracy, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26186b34-53b2-4b97-88fe-061b66245306",
   "metadata": {
    "id": "26186b34-53b2-4b97-88fe-061b66245306"
   },
   "source": [
    "# 7. Decision Tree Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01b1f86-e8e4-4225-9aa8-3956be8c7d73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:45:04.850887Z",
     "start_time": "2021-08-28T15:45:04.822608Z"
    },
    "id": "d01b1f86-e8e4-4225-9aa8-3956be8c7d73",
    "outputId": "27cda37c-8ea1-42ef-e807-caf3ee56777d"
   },
   "outputs": [],
   "source": [
    "# Data - load and preprocess\n",
    "df = pd.read_csv('3data/유방암.csv', encoding='euc-kr')\n",
    "\n",
    "X = df.drop('diagnosis', axis=1)\n",
    "y = df['diagnosis']\n",
    "\n",
    "# 목표변수 라벨링\n",
    "y = y.map({'양성': 1, '음성': 0})\n",
    "\n",
    "# class unbalanced이므로 stratify=y & test_size=0.4\n",
    "X_train, X_test, y_train, y_test =\\\n",
    "        train_test_split(X, y, test_size=0.4, stratify=y)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfac95d-9d33-4e89-b957-305805943ee3",
   "metadata": {
    "id": "ecfac95d-9d33-4e89-b957-305805943ee3"
   },
   "source": [
    "## 7-1. Decision Tree 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c248d3-3bfd-4995-8f0f-fad4b47039dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:27:02.683566Z",
     "start_time": "2021-08-28T15:27:02.678722Z"
    },
    "id": "b4c248d3-3bfd-4995-8f0f-fad4b47039dc"
   },
   "outputs": [],
   "source": [
    "# modelling\n",
    "tree0 = DecisionTreeClassifier(criterion='gini', random_state=0)\n",
    "\n",
    "# GridSearchCv로 hyperparameter tuning\n",
    "params = {'max_depth': [i for i in range(1,10)],\n",
    "          'min_samples_split': [i * 2 for i in range(2,15)],\n",
    "          'min_samples_leaf': [i * 2 for i in range(1, 10)]}\n",
    "tree0_grid = GridSearchCV(tree0, param_grid=params,\n",
    "                           cv=3, refit=True, return_train_score=True)\n",
    "\n",
    "tree0_grid.fit(X_train, y_train)\n",
    "\n",
    "# tree0_grid cv score\n",
    "tree0_score = pd.DataFrame(tree0_grid.cv_results_)\n",
    "tree0_score[['params', 'mean_train_score', 'mean_test_score', 'rank_test_score']].sort_values(by=['rank_test_score'])[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a76a4e3-d98b-4848-97e1-2fc16586e745",
   "metadata": {
    "id": "6a76a4e3-d98b-4848-97e1-2fc16586e745"
   },
   "source": [
    "## 7-2. Decision Tree 성능 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aee466-504a-4129-943e-041c5e37f898",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:27:09.318818Z",
     "start_time": "2021-08-28T15:27:09.316064Z"
    },
    "id": "b1aee466-504a-4129-943e-041c5e37f898",
    "outputId": "6dac8a3c-19f4-44ce-b7be-0fdf8284ba8d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "from sklearn.metrics import classification_report\n",
    "print(confusion_matrix(y_test, tree0_grid.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec2c72b-57b7-4617-ae3f-b8f621aca753",
   "metadata": {
    "id": "9ec2c72b-57b7-4617-ae3f-b8f621aca753"
   },
   "source": [
    "- 테스트 데이터에 대한 confusion matrix에서 **79+6=85개는 음성(0)** 이고, **9+34=43개는 양성(1)** 이다.\n",
    "- 음성 데이터가 양성 데이터보다 2배 많다. => 어느 정도 class imbalance 상태다.\n",
    "    - 이러한 **class imbalance 상황에서는 accuracy만 가지고 모델 성능을 평가할 수 없고, 비즈니스 케이스나 전략적 우선순위에 따라 precision, recall, specificity 등의 지표를 종합적으로 보고 모델 성능을 판단해야 한다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cbb9c4-a28f-4e34-a486-192c35ba8fef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:27:09.322425Z",
     "start_time": "2021-08-28T15:27:09.320055Z"
    },
    "id": "f0cbb9c4-a28f-4e34-a486-192c35ba8fef",
    "outputId": "22a5c2db-e93d-4784-9750-e0c7cdaec4d7"
   },
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "    ## 모델의 정확도(Accuracy)는 (맞춘 데이터 수 / 전체 데이터 수) 이다.\n",
    "print('ACC:', accuracy_score(y_test, tree0_grid.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37a1941-06eb-4ad1-99b1-876705db198c",
   "metadata": {
    "id": "b37a1941-06eb-4ad1-99b1-876705db198c"
   },
   "source": [
    "### Precision, Recall, Specificity\n",
    "- 정밀도(Precision)는 **'양성이라고 예측한 값 중 실제로 양성인 데이터의 비율'**, 즉 (TP / TP + FP)이다.\n",
    "- 재현율(Recall)은 **'실제로 양성인 데이터 중 양성이라고 예측한 데이터의 비율'**, 즉 (TP / TP + FN)이다.\n",
    "- 특이도(Specificity)는 **'실제로 음성인 데이터 중 음성이라고 예측한 데이터의 비율'** 즉, (TN / FP + TN)이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1f7e15-4613-4126-9bf9-ba8743b4e390",
   "metadata": {
    "id": "8a1f7e15-4613-4126-9bf9-ba8743b4e390"
   },
   "source": [
    "> 유방암 진단을 하는 데 있어서 어떤 지표가 제일 중요할까?\n",
    "> - 제일 최악은 실제로 양성인데 음성이라고 예측(진단)하는 경우일 것이다. 하루 빨리 치료 받아야 되는 환자의 치료가 늦어질 수가 있다. ===>>> 즉, **재현율이 높아야 한다.**<br><br>\n",
    "> - 실제로 음성인데 양성이라고 진단하는 경우는 위의 경우보단 괜찮다. 보통 한 번 양성이라고 진단 받더라도, 재확인차 한 번 정도는 더 진단을 받게 되니까 그때가서 음성으로 정분류하면 된다.  ==>>> **양성이라고 예측했을 때 양성인 비율인 '정밀도'에 관한 얘기다**<br><br>\n",
    "> - 특이도는 우선순위가 제일 떨어질 것이다. 실제로 음성인 데이터를 음성으로 분류하는 건 medically urgent problem이 아닌 것 같다.\n",
    "\n",
    "\n",
    "**!!! 따라서, 유방암 진단 케이스에서는.. *재현율이 정밀도보다 우선시* 되어야 한다.**\n",
    "- 정확도가 높은 모델이더라도 음성인 데이터를 음성으로만 잘 분류해내는 것 때문에 정확도가 높고, 정작 재현율이 떨어진다면, 즉 실제로 양성인 환자를 음성으로 분류해낸다면, 이 모델은 절대 유용하지 않을 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e330da23-f385-4717-a1d7-1a412c598903",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:27:09.327748Z",
     "start_time": "2021-08-28T15:27:09.323114Z"
    },
    "id": "e330da23-f385-4717-a1d7-1a412c598903",
    "outputId": "362373bf-5c15-4530-e5c1-2c2d91fed584"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "print(\"Recall(재현율):\", recall_score(y_test, tree0_grid.predict(X_test)))\n",
    "print(\"Precision(정밀도):\", precision_score(y_test, tree0_grid.predict(X_test)))\n",
    "print(\"Specificity(특이도):\", ( 79 / (79+6) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381a9320-d655-4e40-896a-35193fa3f592",
   "metadata": {
    "id": "381a9320-d655-4e40-896a-35193fa3f592"
   },
   "source": [
    "> 뭔가 단단히 잘못됐다.. 유방암 진단 모델의 우선순위는 재현율 > 정밀도 > 특이도인데, 정확히 반대로 점수가 높다.<br>\n",
    "\n",
    "> 이를 해결해보기 위해 GridSearchCV의 scoring 파라미터를 'recall'로 설정해본다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ed6d83-f995-4240-a2a1-00b9f4202681",
   "metadata": {
    "id": "b0ed6d83-f995-4240-a2a1-00b9f4202681"
   },
   "source": [
    "### GridSearchCV - Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4109bd55-8474-43b0-833e-454d39f0f791",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:27:15.754012Z",
     "start_time": "2021-08-28T15:27:09.328484Z"
    },
    "id": "4109bd55-8474-43b0-833e-454d39f0f791",
    "outputId": "71bb33b0-72de-47c7-8f2d-25b148cff20d"
   },
   "outputs": [],
   "source": [
    "# GridSearchCv로 hyperparameter tuning\n",
    "tree0 = DecisionTreeClassifier(criterion='gini', random_state=0)\n",
    "params = {'max_depth': [i for i in range(1,10)],\n",
    "          'min_samples_split': [i * 2 for i in range(2,15)],\n",
    "          'min_samples_leaf': [i * 2 for i in range(1, 10)]}\n",
    "tree0_grid = GridSearchCV(tree0, param_grid=params,\n",
    "                          cv=3, refit=True, scoring='recall')\n",
    "\n",
    "tree0_grid.fit(X_train, y_train)\n",
    "\n",
    "print(confusion_matrix(y_test, tree0_grid.predict(X_test)))\n",
    "\n",
    "print(\"Accuracy(정확도):\", accuracy_score(y_test, tree0_grid.predict(X_test)))\n",
    "print(\"Recall(재현율):\", recall_score(y_test, tree0_grid.predict(X_test)))\n",
    "print(\"Precision(정밀도):\", precision_score(y_test, tree0_grid.predict(X_test)))\n",
    "print(\"Specificity(특이도):\", ( 81 / 85 ))\n",
    "\n",
    "# tree score df\n",
    "tree_score = pd.DataFrame(index=['Decision Tree'])\n",
    "tree_score['Accuracy'] = [accuracy_score(y_test, tree0_grid.predict(X_test))]\n",
    "tree_score['Precision'] = [precision_score(y_test, tree0_grid.predict(X_test))]\n",
    "tree_score['Recall'] = [recall_score(y_test, tree0_grid.predict(X_test))]\n",
    "tree_score['Specificity'] = [recall_score(y_test, tree0_grid.predict(X_test), pos_label=0)]\n",
    "tree_score['F1Score'] = [f1_score(y_test, tree0_grid.predict(X_test))]\n",
    "\n",
    "tree_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376eaa82-6930-45c2-bc68-4088d2f5cdb3",
   "metadata": {
    "id": "376eaa82-6930-45c2-bc68-4088d2f5cdb3"
   },
   "source": [
    "### Comment\n",
    "- GridSearchCV(scoring='recall')로 하니까 오히려 recall score가 0.72로 더 떨어졌다.\n",
    "    - 검색을 좀 해보니, 워낙 데이터셋이 작아서 생긴 문제인 거 같기도 하다. (train data 192개)\n",
    "- 그래도 scoring criteria를 recall로 했는데, 어떻게 criteria가 accuracy일 때보다 낮은 점수를 반환할 수 있는지 이해가 안 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e0bb06-59e7-4ff7-9ba0-a6ba413d680e",
   "metadata": {
    "id": "47e0bb06-59e7-4ff7-9ba0-a6ba413d680e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32f6ea78-d951-4016-bd7e-abf97e2fd565",
   "metadata": {
    "id": "32f6ea78-d951-4016-bd7e-abf97e2fd565"
   },
   "source": [
    "## 7-3. Decision Tree Feature Importance & Graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf17943-d2bb-4f66-be6b-455e1342062e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:29:11.541485Z",
     "start_time": "2021-08-28T15:29:11.516924Z"
    },
    "id": "4bf17943-d2bb-4f66-be6b-455e1342062e",
    "outputId": "1dc97aff-be30-49c6-ccea-33d6356f905a"
   },
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "tree0 = tree0_grid.best_estimator_\n",
    "\n",
    "feat_imp = pd.DataFrame()\n",
    "feat_imp['features'] = X_train.columns\n",
    "feat_imp['importances'] = tree0.feature_importances_\n",
    "feat_imp.sort_values(by='importances', ascending=False, inplace=True)\n",
    "\n",
    "feat_imp.head(10)\n",
    "\n",
    "# graphviz\n",
    "import graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(tree0, out_file='tree0.dot',\n",
    "                feature_names=X_train.columns,\n",
    "                impurity=True, filled=True)\n",
    "with open('tree0.dot') as f:\n",
    "    dot_graph = f.read()\n",
    "display(graphviz.Source(dot_graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15aeb00-5361-4396-b05e-7c9719507457",
   "metadata": {
    "id": "d15aeb00-5361-4396-b05e-7c9719507457"
   },
   "source": [
    "> worst_perimeter가 0.88이라는 feature importance를 가지고 있다.\n",
    "> - \"이것 이외의 변수는 의미가 없다\"는 뜻은 아니고, **worst_perimeter가 다른 변수들에 비해 워낙 설명력이 좋다는 말이다**.<br><br>\n",
    "> - root node에서 (127,65)로 지니 계수가 0.448이던 것이, **\"worst_perimeter가 101.9 이상이냐 이하냐\"는 split 한 번으로 지니계수가 0.095, 0.114로** 크게 낮아졌다. depth 1의 node를 보면 이미 (3,57), (124,8)로 거의 양성과 음성이 분류가 되어 버렸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029585b8-8266-44f6-b312-1ca183850478",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:29:12.827558Z",
     "start_time": "2021-08-28T15:29:12.818496Z"
    },
    "id": "029585b8-8266-44f6-b312-1ca183850478"
   },
   "outputs": [],
   "source": [
    "X_train_peri = pd.DataFrame(X_train['worst_perimeter'])\n",
    "X_test_peri = pd.DataFrame(X_test['worst_perimeter'])\n",
    "\n",
    "tree_peri = DecisionTreeClassifier(criterion='gini', random_state=0)\n",
    "\n",
    "\n",
    "params = {'max_depth': [i for i in range(1,10)],\n",
    "          'min_samples_split': [i * 2 for i in range(2,15)],\n",
    "          'min_samples_leaf': [i * 2 for i in range(1, 10)]}\n",
    "tree_peri_grid = GridSearchCV(tree_peri, param_grid=params,\n",
    "                          cv=3, refit=True, scoring='recall')\n",
    "tree_peri_grid.fit(X_train_peri, y_train)\n",
    "\n",
    "\n",
    "tree_peri = tree_peri_grid.best_estimator_\n",
    "\n",
    "print(\"Accuracy(정확도):\", accuracy_score(y_test, tree_peri.predict(X_test_peri)))\n",
    "print(\"Recall(재현율):\", recall_score(y_test, tree_peri.predict(X_test_peri)))\n",
    "print(\"Precision(정밀도):\", precision_score(y_test, tree_peri.predict(X_test_peri)))\n",
    "print(\"Specificity(특이도):\", ( 79 / (79+6) ))\n",
    "\n",
    "# graphviz\n",
    "export_graphviz(tree_peri, out_file='tree_peri.dot',\n",
    "                feature_names=['worst_perimeter'],\n",
    "                class_names=['Negative', 'Positive'],\n",
    "                impurity=True, filled=True)\n",
    "with open('tree_peri.dot') as f:\n",
    "    dot_graph = f.read()\n",
    "display(graphviz.Source(dot_graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92baa448-a30a-431a-9c21-9b802a3a2bc1",
   "metadata": {
    "id": "92baa448-a30a-431a-9c21-9b802a3a2bc1"
   },
   "source": [
    "### worst_perimeter만으로 학습시킨 모델을 가지고 예측했을 때도 성능에 큰 차이가 없다.\n",
    "\n",
    "> worst perimeter만으로 학습된 모델을 graphviz로 확인해보면, worst perimeter가 4가지 구간으로 나뉘는 걸 알 수 있다.<br>\n",
    "\n",
    "> 분류 기준을 4 구획으로 나누면 다음과 같다:\n",
    "> 1. worst_perimeter가 91.695 이하이면 무조건 양성이다.\n",
    "> 2. 91.695 < worst_perimeter <= 101.9 이면 88% 확률로 양성이다.\n",
    "> 3. 101.9 < worst_perimeter <= 113.15 이면 70% 확률로 음성이다.\n",
    "> 4. worst_perimeter가 113.16 이상이면 무조건 음성이다.<br><br>\n",
    "\n",
    "\n",
    "> worst_perimeter가 워낙 중요한 변수라는 걸 알았으니, 지금 궁금한 건 worst_perimeter를 제외하면 어떤 변수가 가장 유의할까?? 이다. worst_perimeter를 데이터셋에 같이 넣어놓으면, 이 강력한 변수가 다른 변수들의 importance를 모두 낮춰버린다.\n",
    "> - 그래서.. 데이터셋을 2개로 나눠보자: (1) worst_perimeter가 91.695 이하인 데이터에서 worst_peri 변수 제외, (2) worst_perimeter가 113.16 이상인 데이터에서 worst_peri 변수 제외\n",
    "> - 그리고 두 개의 데이터셋으로 학습시킨 모델에서 어떤 변수가 가장 유의한지 확인해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878d9ddd-602d-439d-b70b-b61f3b34cebd",
   "metadata": {
    "id": "878d9ddd-602d-439d-b70b-b61f3b34cebd"
   },
   "source": [
    "그렇게 하려고 했는데... 데이터셋을 나누고 보니 구획별로 들어가는 데이터 수가 30개, 100개 이렇게 엄청 적어서 실제로 시도하진 않았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bc5195-dd3b-44e8-84fe-83c935b00dd8",
   "metadata": {
    "id": "f2bc5195-dd3b-44e8-84fe-83c935b00dd8"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f41588-3ed5-4c4f-9bc8-fda2edf416ba",
   "metadata": {
    "id": "f0f41588-3ed5-4c4f-9bc8-fda2edf416ba"
   },
   "source": [
    "# 8. Random Forest Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7d731b-1aa0-4470-8b0e-bd47c6c0d300",
   "metadata": {
    "id": "2b7d731b-1aa0-4470-8b0e-bd47c6c0d300"
   },
   "source": [
    "## 8-1. RandomForest 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c99d10b-c395-41f2-ab38-37e24f3fc68e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:29:55.968821Z",
     "start_time": "2021-08-28T15:29:55.864416Z"
    },
    "id": "9c99d10b-c395-41f2-ab38-37e24f3fc68e",
    "outputId": "4c78c682-5c8c-42fa-bdf9-3773197a5f7c"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf0 = RandomForestClassifier(random_state=0)\n",
    "rf0.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train - Accuracy(정확도):\", accuracy_score(y_train, rf0.predict(X_train)))\n",
    "print(\"Test - Accuracy(정확도):\", accuracy_score(y_test, rf0.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ed3ad6-b53f-49bf-8d7b-a1dd47dc582d",
   "metadata": {
    "id": "e5ed3ad6-b53f-49bf-8d7b-a1dd47dc582d"
   },
   "source": [
    "- **train set에 과대적합된 경향을 보이므로, hyperparameter tuning을 해준다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187057d5-2cca-48ad-970d-7118e5340013",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:37:39.154606Z",
     "start_time": "2021-08-28T15:30:27.478750Z"
    },
    "id": "187057d5-2cca-48ad-970d-7118e5340013"
   },
   "outputs": [],
   "source": [
    "# GridSearchCV\n",
    "rf0 = RandomForestClassifier(random_state=0)\n",
    "\n",
    "params = {'n_estimators': [i * 10 for i in range(1, 10)],\n",
    "          'max_depth': [i for i in range(1, 8)],\n",
    "          'min_samples_split': [i * 2 for i in range(2, 15)],\n",
    "          'min_samples_leaf': [i * 2 for i in range(1, 8)]}\n",
    "rf0_grid = GridSearchCV(rf0, param_grid=params,\n",
    "                          cv=3, refit=True, scoring='recall')\n",
    "rf0_grid.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "rf0 = rf0_grid.best_estimator_\n",
    "\n",
    "# 모델의 파라미터 세팅 내용\n",
    "rf0.get_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ff1c60-e334-43dc-bf6b-f527e3819da5",
   "metadata": {
    "id": "27ff1c60-e334-43dc-bf6b-f527e3819da5"
   },
   "source": [
    "## 8-2. RandomForest 성능 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d92d39-3253-4977-a75b-444d62775dda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:37:39.164064Z",
     "start_time": "2021-08-28T15:37:39.159437Z"
    },
    "id": "e4d92d39-3253-4977-a75b-444d62775dda",
    "outputId": "6554215e-620c-44ef-ac05-38444a17c12c"
   },
   "outputs": [],
   "source": [
    "# 파라미터 세팅한 모델의 train/test 정확도\n",
    "print(\"Train - Accuracy(정확도):\", accuracy_score(y_train, rf0.predict(X_train)))\n",
    "print(\"Test - Accuracy(정확도):\", accuracy_score(y_test, rf0.predict(X_test)))\n",
    "\n",
    "# confusion matrix\n",
    "print(\"Confusion matrix: \\n\")\n",
    "print(confusion_matrix(y_test, rf0.predict(X_test)))\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test, rf0.predict(X_test), digits=3))\n",
    "\n",
    "print(\"Accuracy(정확도):\", accuracy_score(y_test, rf0.predict(X_test)))\n",
    "print(\"Recall(재현율):\", recall_score(y_test, rf0.predict(X_test)))\n",
    "print(\"Precision(정밀도):\", precision_score(y_test, rf0.predict(X_test)))\n",
    "print(\"Specificity(특이도):\", ( 83 / (83+2) ))\n",
    "\n",
    "# RandomForest score df\n",
    "forest_score = pd.DataFrame(index=['Random Forest'])\n",
    "forest_score['Accuracy'] = [accuracy_score(y_test, rf0.predict(X_test))]\n",
    "forest_score['Precision'] = [precision_score(y_test, rf0.predict(X_test))]\n",
    "forest_score['Recall'] = [recall_score(y_test, rf0.predict(X_test))]\n",
    "forest_score['Specificity'] = [recall_score(y_test, rf0.predict(X_test), pos_label=0)]\n",
    "forest_score['F1Score'] = [f1_score(y_test, rf0.predict(X_test))]\n",
    "\n",
    "forest_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1288b46f-1f82-40fa-b573-29742d49a441",
   "metadata": {
    "id": "1288b46f-1f82-40fa-b573-29742d49a441"
   },
   "source": [
    "### Comment\n",
    "- train, test 정확도가 각각 95%, 94%로, parameter tuning 이전보다 훨씬 '일반화'된 성능을 보여주고 있다.\n",
    "- 양성에 대한 재현율은 88%로, Decision Tree의 76%보다는 크게 높다.\n",
    "    - 하지만.. 여전히 특이도나 정밀도에 비해 재현율이 더 낮은 것은 아쉽다.\n",
    "    - 88%의 재현율도 충분히 높은 수준이라 생각할 수도 있겠지만, **\"유방암 진단 모델의 재현율이 88%로 충분한가??\"** 하는 문제는 도메인 지식과 비즈니스 케이스를 기반으로 판단되어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e16274-3f1b-4962-9429-4c70884911b3",
   "metadata": {
    "id": "55e16274-3f1b-4962-9429-4c70884911b3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d358bc0-6df5-4705-8556-1dc7ea1cffb6",
   "metadata": {
    "id": "7d358bc0-6df5-4705-8556-1dc7ea1cffb6"
   },
   "source": [
    "## 8-3. RandomForest Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed00ce63-aea0-41e5-b3c6-c0bd63364fd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:37:39.191641Z",
     "start_time": "2021-08-28T15:37:39.182790Z"
    },
    "id": "ed00ce63-aea0-41e5-b3c6-c0bd63364fd1",
    "outputId": "9d9ae99c-0ac1-4019-f603-5bfb98d2d0d3"
   },
   "outputs": [],
   "source": [
    "feat_imp = pd.DataFrame()\n",
    "feat_imp['features'] = X_train.columns\n",
    "feat_imp['importances'] = rf0.feature_importances_\n",
    "feat_imp.sort_values(by='importances', ascending=False, inplace=True)\n",
    "\n",
    "feat_imp.head(10)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.barplot(x='importances', y='features', orient='h', data=feat_imp.iloc[0:10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bc427c-0f2e-4d60-9f16-358976a9b1f4",
   "metadata": {
    "id": "b0bc427c-0f2e-4d60-9f16-358976a9b1f4"
   },
   "source": [
    "### Comment\n",
    "- Decision Tree는 3개 변수 빼고는 feature importance가 모두 0인데다가, 제일 중요도가 높은 변수가 worst_perimeter였다.<br><br>\n",
    "\n",
    "- RandomForest는 변수중요도가 꽤나 고르게 분포하고 있고;\n",
    "- 가장 중요도가 높은 변수는 worst_concave_points이고, 이 변수의 중요도가 약 0.2다. (Decision Tree에서 worst_perimeter의 중요도는 0.88 정도였다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf874370-384c-46be-9900-2b32a9179fb0",
   "metadata": {
    "id": "bf874370-384c-46be-9900-2b32a9179fb0"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c766e7f-ebcc-4cd5-87ca-9e429451f8de",
   "metadata": {
    "id": "7c766e7f-ebcc-4cd5-87ca-9e429451f8de"
   },
   "source": [
    "# 8월 27일자 업데이트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e097991d-7389-4b29-aad0-97465e30d692",
   "metadata": {
    "id": "e097991d-7389-4b29-aad0-97465e30d692"
   },
   "source": [
    "# 9. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00de34df-6eab-4fa4-976c-7008778e8b45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:37:49.149389Z",
     "start_time": "2021-08-28T15:37:49.124708Z"
    },
    "id": "00de34df-6eab-4fa4-976c-7008778e8b45",
    "outputId": "4d934dbc-97c1-4c4d-e14b-0b91dded9c14"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "# Data - load and preprocess\n",
    "df = pd.read_csv('3data/유방암.csv', encoding='euc-kr')\n",
    "\n",
    "X = df.drop('diagnosis', axis=1)\n",
    "y = df['diagnosis']\n",
    "\n",
    "# 목표변수 라벨링\n",
    "y = y.map({'양성': 1, '음성': 0})\n",
    "\n",
    "# class unbalanced이므로 stratify=y & test_size=0.4\n",
    "X_train, X_test, y_train, y_test =\\\n",
    "        train_test_split(X, y, test_size=0.4, stratify=y)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfef2b9-0174-497f-bf0d-e98cd15b54c0",
   "metadata": {
    "id": "1bfef2b9-0174-497f-bf0d-e98cd15b54c0"
   },
   "source": [
    "## 9-1. SVM 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab660ba-4b6c-427b-b07c-260bc83ced49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:37:49.987332Z",
     "start_time": "2021-08-28T15:37:49.969514Z"
    },
    "id": "eab660ba-4b6c-427b-b07c-260bc83ced49"
   },
   "outputs": [],
   "source": [
    "# SVM은 스케일링이 모델 성능에 큰 영향을 미치기 때문에,\n",
    "    # 미리 데이터셋을 스케일링 해준다.\n",
    "stscl = StandardScaler()\n",
    "\n",
    "X_train = stscl.fit_transform(X_train)\n",
    "X_test = stscl.transform(X_test)\n",
    "\n",
    "svm0 = SVC(random_state=0)\n",
    "svm0.fit(X_train, y_train)\n",
    "\n",
    "print('Train Accuracy:', svm0.score(X_train, y_train).round(3))\n",
    "print('Test Accuracy:', svm0.score(X_test, y_test).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc529a87-b881-4bd0-afa0-e3b68c41b7fc",
   "metadata": {
    "id": "dc529a87-b881-4bd0-afa0-e3b68c41b7fc"
   },
   "source": [
    "- 유방암 데이터셋이 기본적으로 클래스 레이블이 선형적으로 잘 나누어지는 데이터셋인지, SVM을 따로 튜닝하지 않았는데도 train/test accuracy가 높아 보인다.\n",
    "    - 그래도 수치만 봐서 '높다, 낮다' 판단하기에는 이르므로, 규제 파라미터인 C를 바꿔가면서 성능이 어떻게 변하는지 확인해본다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f47475b-7a0f-47b1-8f38-aabe21a576ea",
   "metadata": {
    "id": "1f47475b-7a0f-47b1-8f38-aabe21a576ea"
   },
   "source": [
    "## 9-2. SVM 성능 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4560b579-97b0-4d16-b234-edfe0597dd97",
   "metadata": {
    "id": "4560b579-97b0-4d16-b234-edfe0597dd97"
   },
   "source": [
    "### C 값 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac62092f-d8b6-4717-8c98-259ecfc587b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:37:51.568222Z",
     "start_time": "2021-08-28T15:37:51.523081Z"
    },
    "id": "ac62092f-d8b6-4717-8c98-259ecfc587b1",
    "outputId": "1da05802-6a42-4af8-bce5-6f590c440149",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_acc, test_acc = [], []\n",
    "para_c = [i * 0.1 for i in range(1, 10)]\n",
    "\n",
    "for v_C in para_c:\n",
    "    svm = SVC(C=v_C, random_state=0)\n",
    "    svm.fit(X_train, y_train)\n",
    "\n",
    "    train_acc.append(svm.score(X_train, y_train))\n",
    "    test_acc.append(svm.score(X_test, y_test))\n",
    "\n",
    "df_acc_c = pd.DataFrame()\n",
    "df_acc_c['C'] = para_c\n",
    "df_acc_c['TrainAccuracy'] = train_acc\n",
    "df_acc_c['TestAccuracy'] = test_acc\n",
    "\n",
    "df_acc_c.round(3)\n",
    "\n",
    "plt.plot(para_c, train_acc, linestyle='-', label='Train Accuracy')\n",
    "plt.plot(para_c, test_acc, linestyle='--', label='Test Accuracy')\n",
    "plt.axvline(x=0.25, color='r', linestyle='--', linewidth=2)\n",
    "plt.axvline(x=0.45, color='r', linestyle='--', linewidth=2)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('C')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72019355-9a79-4d40-84d5-1b6e06097375",
   "metadata": {
    "id": "72019355-9a79-4d40-84d5-1b6e06097375"
   },
   "source": [
    "- 특이하게 C값이 0.1 이하의 구간에서 test acc가 더 높다.\n",
    "- 0.2를 기점으로 train acc가 test acc를 넘어서고, test acc는 C=0.4일 때 그래프 상에서 정점을 찍는다.\n",
    "- C 값을 0.25~0.45 구간에서 더 잘게 쪼개서 분석해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c144f153-c7f0-42ce-84a9-bb99499de7b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:37:52.489903Z",
     "start_time": "2021-08-28T15:37:52.362184Z"
    },
    "id": "c144f153-c7f0-42ce-84a9-bb99499de7b7",
    "outputId": "78ef3750-5696-44c2-c1e0-3b53c2235a0b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_acc, test_acc = [], []\n",
    "para_c = [i * 0.01 for i in range(20, 45)]\n",
    "\n",
    "for v_C in para_c:\n",
    "    svm = SVC(C=v_C, random_state=0)\n",
    "    svm.fit(X_train, y_train)\n",
    "\n",
    "    train_acc.append(svm.score(X_train, y_train))\n",
    "    test_acc.append(svm.score(X_test, y_test))\n",
    "\n",
    "df_acc_c = pd.DataFrame()\n",
    "df_acc_c['C'] = para_c\n",
    "df_acc_c['TrainAccuracy'] = train_acc\n",
    "df_acc_c['TestAccuracy'] = test_acc\n",
    "\n",
    "plt.plot(para_c, train_acc, linestyle='-', label='Train Accuracy')\n",
    "plt.plot(para_c, test_acc, linestyle='--', label='Test Accuracy')\n",
    "plt.axvline(x=0.34, color='r', linestyle='--', linewidth=2)\n",
    "plt.axvline(x=0.36, color='r', linestyle='--', linewidth=2)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('C')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed6cf8a-0ffa-4f11-aa5d-9b79e2b6d807",
   "metadata": {
    "id": "5ed6cf8a-0ffa-4f11-aa5d-9b79e2b6d807"
   },
   "source": [
    "- 그래프 모양이 조금 특이하긴 한데, C=0.35일 때 test score가 가장 높은 값을 가지기 때문에 C=0.35로 세팅한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f00a61d-d4b6-4eca-b351-072301588590",
   "metadata": {
    "id": "7f00a61d-d4b6-4eca-b351-072301588590"
   },
   "source": [
    "### Gamma 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc970e6-5767-448f-90aa-aa91292b996c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:37:53.113705Z",
     "start_time": "2021-08-28T15:37:53.024237Z"
    },
    "id": "7dc970e6-5767-448f-90aa-aa91292b996c",
    "outputId": "a776edad-aebf-48d4-acee-2406ab52cb26"
   },
   "outputs": [],
   "source": [
    "train_acc, test_acc = [], []\n",
    "para_gamma = [10 ** i for i in range(-2, 2)]\n",
    "\n",
    "for v_gamma in para_gamma:\n",
    "    svm = SVC(gamma=v_gamma, C=0.35, random_state=0)\n",
    "    svm.fit(X_train, y_train)\n",
    "\n",
    "    train_acc.append(svm.score(X_train, y_train))\n",
    "    test_acc.append(svm.score(X_test, y_test))\n",
    "\n",
    "df_acc_gamma = pd.DataFrame()\n",
    "df_acc_gamma['Gamma'] = para_gamma\n",
    "df_acc_gamma['TrainAccuracy'] = train_acc\n",
    "df_acc_gamma['TestAccuracy'] = test_acc\n",
    "\n",
    "plt.plot(para_gamma,train_acc,linestyle='-',label=\"Train Accuracy\")\n",
    "plt.plot(para_gamma,test_acc,linestyle='--',label=\"Test Accuracy\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"gamma\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7097f58a-d369-4cad-9277-da63c7f5ea85",
   "metadata": {
    "id": "7097f58a-d369-4cad-9277-da63c7f5ea85"
   },
   "source": [
    "- gamma 값이 0에서 1 사이일 때만 test/acc가 큰 변화를 보이므로, 해당 구간을 더 잘게 쪼개본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f29fcca-14c3-455f-b88a-5c44cea64029",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:37:53.739284Z",
     "start_time": "2021-08-28T15:37:53.587646Z"
    },
    "id": "6f29fcca-14c3-455f-b88a-5c44cea64029",
    "outputId": "2f710ad9-ebc7-4505-c3a9-e3a8060f5059",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_acc, test_acc = [], []\n",
    "para_gamma = [i * 0.05 for i in range(1, 20)]\n",
    "\n",
    "for v_gamma in para_gamma:\n",
    "    svm = SVC(gamma=v_gamma, C=0.35, random_state=0)\n",
    "    svm.fit(X_train, y_train)\n",
    "\n",
    "    train_acc.append(svm.score(X_train, y_train))\n",
    "    test_acc.append(svm.score(X_test, y_test))\n",
    "\n",
    "df_acc_gamma = pd.DataFrame()\n",
    "df_acc_gamma['Gamma'] = para_gamma\n",
    "df_acc_gamma['TrainAccuracy'] = train_acc\n",
    "df_acc_gamma['TestAccuracy'] = test_acc\n",
    "\n",
    "plt.plot(para_gamma,train_acc,linestyle='-',label=\"Train Accuracy\")\n",
    "plt.plot(para_gamma,test_acc,linestyle='--',label=\"Test Accuracy\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"gamma\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d5c34f-330b-45ea-8d85-334749e090d0",
   "metadata": {
    "id": "c4d5c34f-330b-45ea-8d85-334749e090d0"
   },
   "source": [
    "- Gamma가 0일 때 train/test acc가 정점이고, gamma가 증가할 수록 acc가 모두 감소한다.\n",
    "- 따라서 감마를 0.01로 세팅한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541654b8-3d38-4925-856e-e3650e58d4ef",
   "metadata": {
    "id": "541654b8-3d38-4925-856e-e3650e58d4ef"
   },
   "source": [
    "### 성능 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee00d9f-7bd8-48cf-bf0a-3de2b259274c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:37:56.967169Z",
     "start_time": "2021-08-28T15:37:56.945285Z"
    },
    "id": "1ee00d9f-7bd8-48cf-bf0a-3de2b259274c",
    "outputId": "35419124-a3ad-4b93-cb0a-5558192df8b9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svm1 = SVC(C=0.35, gamma=0.01, random_state=0)\n",
    "svm1.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(\"Accuracy(정확도):\", accuracy_score(y_test, svm1.predict(X_test)).round(3))\n",
    "print(\"Recall(재현율):\", recall_score(y_test, svm1.predict(X_test)).round(3))\n",
    "print(\"Precision(정밀도):\", precision_score(y_test, svm1.predict(X_test)).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8977a2-ad17-4ac3-88ea-8516c9c709d4",
   "metadata": {
    "id": "0a8977a2-ad17-4ac3-88ea-8516c9c709d4"
   },
   "source": [
    "- 어제까지 모델링한 것들 중에 가장 성능이 좋았던 **RandomForest의 성능지표** 를 다시 보자면..\n",
    "    - Accuracy(정확도): 0.9453125\n",
    "    - Recall(재현율): 0.8837209302325582\n",
    "    - Precision(정밀도): 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf7d8db-fbd0-4ef3-a1c3-0ed4ad2b808a",
   "metadata": {
    "id": "bcf7d8db-fbd0-4ef3-a1c3-0ed4ad2b808a"
   },
   "source": [
    "- 7-2 에서 지적했듯이, 유방암 진단 모델에 있어서 제일 중요한 지표는 '재현율'인 것 같다. 그런데 **SVM의 지표를 보면 RF보다 훨씬 재현율이 높다.**\n",
    "    - 만약 진단 모델을 하나 골라야 한다면 지금까지 나온 것 중엔 SVM을 사용해야 하지 않을까?\n",
    "    - 근데 SVM의 **단점 중 하나는 역시 '설명가능성'(explainability)과 '해석가능성'(interpretability)** 이다.\n",
    "    - Decision Tree가 갖춘 직관성이 SVM에는 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc12b10-4c3b-4462-8b1c-723654310902",
   "metadata": {
    "id": "3fc12b10-4c3b-4462-8b1c-723654310902"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb89ba0d-2314-4cbc-9594-b147e226dfc2",
   "metadata": {
    "id": "bb89ba0d-2314-4cbc-9594-b147e226dfc2"
   },
   "source": [
    "## 9-3. PCA & SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0884af9f-cd33-4a46-9fb1-d43232f63b30",
   "metadata": {
    "id": "0884af9f-cd33-4a46-9fb1-d43232f63b30"
   },
   "source": [
    "> 이미 RF보다 성능이 높지만, 혹시 PCA를 이용한다면 SVM의 성능을 더 높일 수 있을까?\n",
    "> - 목차 4~5번의 분석 내용을 참고해서, 유방암 데이터셋에서 주성분 3개를 추출하고, 주성분 3개를 기반으로 SVM을 구현하고 성능을 시험한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bca0d70-83f6-4cce-a27b-498c62118c2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:40:56.284316Z",
     "start_time": "2021-08-28T15:40:56.262470Z"
    },
    "id": "5bca0d70-83f6-4cce-a27b-498c62118c2e",
    "outputId": "fb1641c0-c0f7-4bd7-d35c-c328dfd2f7bb"
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3, random_state=0)\n",
    "PCtrain = pca.fit_transform(X_train)\n",
    "PCtest = pca.transform(X_test)\n",
    "\n",
    "svm2 = SVC(gamma=0.01, C=0.35, random_state=0)\n",
    "svm2.fit(PCtrain, y_train)\n",
    "\n",
    "print(\"Accuracy(정확도):\", accuracy_score(y_test, svm2.predict(PCtest)).round(3))\n",
    "print(\"Recall(재현율):\", recall_score(y_test, svm2.predict(PCtest)).round(3))\n",
    "print(\"Precision(정밀도):\", precision_score(y_test, svm2.predict(PCtest)).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0d2562-eb40-40cf-82d5-c80a32756f8a",
   "metadata": {
    "id": "6f0d2562-eb40-40cf-82d5-c80a32756f8a"
   },
   "source": [
    "- 30개 변수를 사용했던 SVC 성능\n",
    "    - Accuracy(정확도): 0.969\n",
    "    - Recall(재현율): 0.953\n",
    "    - Precision(정밀도): 0.953\n",
    "    \n",
    "> 주성분 3개로 축소해서 모델을 구현하니 살짝 낮은 성능을 보여준다.\n",
    "\n",
    "- 혹시 주성분 4개를 사용하면 어떨까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95e18dc-c0db-4455-8275-a15ab0192663",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:38:02.403237Z",
     "start_time": "2021-08-28T15:38:02.383175Z"
    },
    "id": "c95e18dc-c0db-4455-8275-a15ab0192663",
    "outputId": "ac7e38f7-8397-4103-997e-1425abf26235"
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=4, random_state=0)\n",
    "PCtrain = pca.fit_transform(X_train)\n",
    "PCtest = pca.transform(X_test)\n",
    "\n",
    "svm2 = SVC(gamma=0.01, C=0.35, random_state=0)\n",
    "svm2.fit(PCtrain, y_train)\n",
    "\n",
    "print(\"Accuracy(정확도):\", accuracy_score(y_test, svm2.predict(PCtest)).round(3))\n",
    "print(\"Recall(재현율):\", recall_score(y_test, svm2.predict(PCtest)).round(3))\n",
    "print(\"Precision(정밀도):\", precision_score(y_test, svm2.predict(PCtest)).round(3))\n",
    "\n",
    "# svm score df\n",
    "svm_score = pd.DataFrame(index=['SVM'])\n",
    "svm_score['Accuracy'] = [accuracy_score(y_test, svm2.predict(PCtest))]\n",
    "svm_score['Precision'] = [precision_score(y_test, svm2.predict(PCtest))]\n",
    "svm_score['Recall'] = [recall_score(y_test, svm2.predict(PCtest))]\n",
    "svm_score['Specificity'] = [recall_score(y_test, svm2.predict(PCtest), pos_label=0)]\n",
    "svm_score['F1Score'] = [f1_score(y_test, svm2.predict(PCtest))]\n",
    "\n",
    "svm_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3c33b1-002b-4aff-bbce-24bc51a6f5c0",
   "metadata": {
    "id": "be3c33b1-002b-4aff-bbce-24bc51a6f5c0"
   },
   "source": [
    "- 신기하게도 주성분 4개를 사용하니 30개 변수를 사용했던 때와 완전히 똑같은 성능을 보여준다. 혹시나해서 주성분을 5~6개 이상으로도 늘려봤더니, 늘려도 늘려도 성능이 똑같다.\n",
    "- 왜 그런지는 정확히는 모르겠지만..\n",
    "    - 아마 30개 변수를 사용할 때도 클래스별 선형 관계를 구별짓는 데 사용되는 주요 변수가 약 3~5개 내외가 아니었을까 예상한다.\n",
    "- 만약 30개 변수를 사용할 때나 주성분 4개를 사용할 때나 성능이 똑같다면, 무조건 주성분 4개를 사용한 모델이 훨씬 더 효율적인 모델일 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece046d4-187d-4559-b09a-2424e1a816f5",
   "metadata": {
    "id": "ece046d4-187d-4559-b09a-2424e1a816f5"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226d5137-fe96-4226-a9d4-ed67aa1661f3",
   "metadata": {
    "id": "226d5137-fe96-4226-a9d4-ed67aa1661f3"
   },
   "source": [
    "# 8월 28일자 업데이트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c513b4-fcb7-457e-aee3-f721c9de0bd2",
   "metadata": {
    "id": "b3c513b4-fcb7-457e-aee3-f721c9de0bd2"
   },
   "source": [
    "# 10. Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c292eb-9d49-4894-9067-27e6a06b24d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:41:11.475628Z",
     "start_time": "2021-08-28T15:41:11.450947Z"
    },
    "id": "b0c292eb-9d49-4894-9067-27e6a06b24d0",
    "outputId": "3745639b-d7dd-4133-cc93-23d29ae802e1"
   },
   "outputs": [],
   "source": [
    "# Data - load and preprocess\n",
    "df = pd.read_csv('3data/유방암.csv', encoding='euc-kr')\n",
    "\n",
    "X = df.drop('diagnosis', axis=1)\n",
    "y = df['diagnosis']\n",
    "\n",
    "# 목표변수 라벨링\n",
    "y = y.map({'양성': 1, '음성': 0})\n",
    "\n",
    "# class unbalanced이므로 stratify=y & test_size=0.4\n",
    "X_train, X_test, y_train, y_test =\\\n",
    "        train_test_split(X, y, test_size=0.4, stratify=y)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "# Neural Network 모델 성능에 스케일링이 영향을 미친다\n",
    "stscl = StandardScaler()\n",
    "\n",
    "X_train = stscl.fit_transform(X_train)\n",
    "X_test = stscl.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080a752a-9cfd-4f78-938e-b9b15831c9c0",
   "metadata": {
    "id": "080a752a-9cfd-4f78-938e-b9b15831c9c0"
   },
   "source": [
    "## 10-1. Neural Network 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c58776-6acb-461a-9249-547216ddc337",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:41:13.091025Z",
     "start_time": "2021-08-28T15:41:12.915869Z"
    },
    "id": "b4c58776-6acb-461a-9249-547216ddc337",
    "outputId": "82907f19-7dad-4e4c-b06e-6add3c5ba281"
   },
   "outputs": [],
   "source": [
    "# Multi-Layered Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "nn0 = MLPClassifier(random_state=0)\n",
    "nn0.fit(X_train,y_train)\n",
    "\n",
    "print(\"Train Accuracy (without param):\", nn0.score(X_train, y_train).round(3))\n",
    "print(\"Test Accuracy (without param):\", nn0.score(X_test, y_test).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d016ae86-e330-432b-a301-2584c6ff9459",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T14:19:11.483348Z",
     "start_time": "2021-08-28T14:19:11.458709Z"
    },
    "id": "d016ae86-e330-432b-a301-2584c6ff9459"
   },
   "source": [
    "- default parameter는 hidden_layer_sizes=100이다. 파라미터를 변경하면 성능 변화가 있을까?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5fba63-936d-4b19-9942-02e4f60394f5",
   "metadata": {
    "id": "da5fba63-936d-4b19-9942-02e4f60394f5"
   },
   "source": [
    "## 10-2. Neural Network 성능 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867a0de9-accf-4a00-a1d3-b755a6c9672a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:41:16.125772Z",
     "start_time": "2021-08-28T15:41:14.184035Z"
    },
    "id": "867a0de9-accf-4a00-a1d3-b755a6c9672a",
    "outputId": "3e7468f1-e729-4789-9ac8-d7d82dd0ea77"
   },
   "outputs": [],
   "source": [
    "train_acc, test_acc = [], []\n",
    "\n",
    "para_hidden = [i * 20 for i in range(1, 9)]\n",
    "\n",
    "for v_hidden in para_hidden:\n",
    "    nn = MLPClassifier(hidden_layer_sizes=v_hidden, random_state=0)\n",
    "    nn.fit(X_train, y_train)\n",
    "    train_acc.append(nn.score(X_train, y_train))\n",
    "    test_acc.append(nn.score(X_test, y_test))\n",
    "\n",
    "df_acc_hidden=pd.DataFrame()\n",
    "df_acc_hidden[\"HiddenLayer\"]=para_hidden\n",
    "df_acc_hidden[\"TrainAccuracy\"]=train_acc\n",
    "df_acc_hidden[\"TestAccuracy\"]=test_acc\n",
    "\n",
    "df_acc_hidden.round(3)\n",
    "\n",
    "plt.plot(para_hidden,train_acc,linestyle=\"-\",label=\"Train Accuracy\")\n",
    "plt.plot(para_hidden,test_acc,linestyle=\"--\",label=\"Test Accuracy\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"Hidden Layer\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e54096a-ea5f-4c9f-ad47-af66709fdee7",
   "metadata": {
    "id": "2e54096a-ea5f-4c9f-ad47-af66709fdee7"
   },
   "source": [
    "- 위 그래프를 기준으로는 hidden_layer_sizes=40일 때 test score(0.969)가 제일 높고 그 이후로 test score가 계속 떨어지는 것을 고려했을 때, 40이 적정 파라미터인 것 같다.\n",
    "- 위에서는 1개의 은닉층(hidden layer)을 기준으로 성능을 비교한건데, 만약 2개의 은닉층을 설정한다면 성능이 어떻게 바뀔까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58752eb-8c1a-4d75-b213-197817784d03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:41:18.369618Z",
     "start_time": "2021-08-28T15:41:16.186722Z"
    },
    "id": "a58752eb-8c1a-4d75-b213-197817784d03",
    "outputId": "5a921ee0-7f2f-4ad2-99d3-0bff250f1158"
   },
   "outputs": [],
   "source": [
    "train_acc, test_acc = [], []\n",
    "\n",
    "para_hidden = [i * 20 for i in range(1, 9)]\n",
    "\n",
    "for v_hidden in para_hidden:\n",
    "    nn = MLPClassifier(hidden_layer_sizes=(v_hidden, v_hidden),\n",
    "                       random_state=0)\n",
    "    nn.fit(X_train, y_train)\n",
    "    train_acc.append(nn.score(X_train, y_train))\n",
    "    test_acc.append(nn.score(X_test, y_test))\n",
    "\n",
    "df_acc_hidden=pd.DataFrame()\n",
    "df_acc_hidden[\"HiddenLayer\"]=para_hidden\n",
    "df_acc_hidden[\"TrainAccuracy\"]=train_acc\n",
    "df_acc_hidden[\"TestAccuracy\"]=test_acc\n",
    "\n",
    "df_acc_hidden.round(3)\n",
    "\n",
    "plt.plot(para_hidden,train_acc,linestyle=\"-\",label=\"Train Accuracy\")\n",
    "plt.plot(para_hidden,test_acc,linestyle=\"--\",label=\"Test Accuracy\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"Hidden Layer\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7028ffd5-0d5c-44a1-8c62-7685369c3c0b",
   "metadata": {
    "id": "7028ffd5-0d5c-44a1-8c62-7685369c3c0b"
   },
   "source": [
    "- hidden layer를 2개 층으로 했을 때는 1개층으로 얻었던 test score인 0.969에 미치지 못하고 있다.\n",
    "- 따라서 1개층의 hidden layer size를 40으로 설정하고, activation function을 바꾸면 성능에 변화가 있는지 확인해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88703d4a-a476-493a-ae30-2186aea1ee9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:41:18.560844Z",
     "start_time": "2021-08-28T15:41:18.427901Z"
    },
    "id": "88703d4a-a476-493a-ae30-2186aea1ee9a",
    "outputId": "3f5bd74d-c007-4c9b-f740-279601ea2ace"
   },
   "outputs": [],
   "source": [
    "train_acc = []; test_acc = []\n",
    "para_funtion = ['logistic','tanh','relu']\n",
    "\n",
    "for v_function in para_funtion:\n",
    "    nn = MLPClassifier(hidden_layer_sizes=40, activation=v_function,\n",
    "                       random_state=0)\n",
    "    nn.fit(X_train, y_train)\n",
    "    train_acc.append(nn.score(X_train, y_train))\n",
    "    test_acc.append(nn.score(X_test, y_test))\n",
    "\n",
    "df_acc_function=pd.DataFrame()\n",
    "df_acc_function[\"ActivationFuntion\"]=para_funtion\n",
    "df_acc_function[\"TrainAccuracy\"]=train_acc\n",
    "df_acc_function[\"TestAccuracy\"]=test_acc\n",
    "\n",
    "df_acc_function.round(3)\n",
    "\n",
    "plt.plot(para_funtion,train_acc,linestyle=\"-\",label=\"Train Accuracy\")\n",
    "plt.plot(para_funtion,test_acc,linestyle=\"--\",label=\"Test Accuracy\")\n",
    "plt.ylabel(\"accuracy\");plt.xlabel(\"Activation Function\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49157e27-8cab-4ea4-9efb-767a2b2ba6ac",
   "metadata": {
    "id": "49157e27-8cab-4ea4-9efb-767a2b2ba6ac"
   },
   "source": [
    "- ReLu가 train/test score 모두 가장 높다. (relu는 MLPClassifier의 default function이다.)<br><br>\n",
    "\n",
    "\n",
    "- 그렇다면 여러가지 성능 지표를 최종적으로 확인해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76969177-a8ce-441e-93a0-9b81583498a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:41:18.656224Z",
     "start_time": "2021-08-28T15:41:18.614854Z"
    },
    "id": "76969177-a8ce-441e-93a0-9b81583498a9",
    "outputId": "51978b8c-345c-4bbc-e390-3a51105439d8"
   },
   "outputs": [],
   "source": [
    "nn_set = MLPClassifier(hidden_layer_sizes=40, activation='relu',\n",
    "                       random_state=0)\n",
    "nn_set.fit(X_train, y_train)\n",
    "\n",
    "print(confusion_matrix(y_test, nn_set.predict(X_test)))\n",
    "\n",
    "print(\"Accuracy(정확도):\", accuracy_score(y_test, nn_set.predict(X_test)).round(3))\n",
    "print(\"Recall(재현율):\", recall_score(y_test, nn_set.predict(X_test)).round(3))\n",
    "print(\"Precision(정밀도):\", precision_score(y_test, nn_set.predict(X_test)).round(3))\n",
    "\n",
    "# nn score df\n",
    "nn_score = pd.DataFrame(index=['Neural Network'])\n",
    "nn_score['Accuracy'] = [accuracy_score(y_test, nn_set.predict(X_test))]\n",
    "nn_score['Precision'] = [precision_score(y_test, nn_set.predict(X_test))]\n",
    "nn_score['Recall'] = [recall_score(y_test, nn_set.predict(X_test))]\n",
    "nn_score['Specificity'] = [recall_score(y_test, nn_set.predict(X_test), pos_label=0)]\n",
    "nn_score['F1Score'] = [f1_score(y_test, nn_set.predict(X_test))]\n",
    "\n",
    "nn_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c062da13-a930-4530-b582-393528e97bf4",
   "metadata": {
    "id": "c062da13-a930-4530-b582-393528e97bf4"
   },
   "source": [
    "- Confusion matrix가 굉장히 인상 깊다. 지금까지 살펴본 대부분의 모델들이 음성 데이터를 더 잘 맞추고 양성 데이터를 많이 틀렸는데 (낮은 재현율, 높은 특이도), N은 43개의 양성 데이터 중 41개를 맞췄다.\n",
    "- 유방암 데이터셋에서 재현율이 굉장히 높은 모델이라는 점이 인상 깊다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e480f6-fc92-4393-8a68-1ba70b476ab2",
   "metadata": {
    "id": "a1e480f6-fc92-4393-8a68-1ba70b476ab2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70a6abe9-a919-44b3-abc6-79be9b5d6107",
   "metadata": {
    "id": "70a6abe9-a919-44b3-abc6-79be9b5d6107"
   },
   "source": [
    "# 11. KNN\n",
    "- scaling\n",
    "- 3~30까지 acc 변화\n",
    "- confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96c638c-6873-4cd1-ab64-02ca39411880",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:45:58.382410Z",
     "start_time": "2021-08-28T15:45:58.356921Z"
    },
    "id": "a96c638c-6873-4cd1-ab64-02ca39411880",
    "outputId": "cfb0a89e-05c1-4973-feba-2689642924b5"
   },
   "outputs": [],
   "source": [
    "# Data - load and preprocess\n",
    "df = pd.read_csv('3data/유방암.csv', encoding='euc-kr')\n",
    "\n",
    "X = df.drop('diagnosis', axis=1)\n",
    "y = df['diagnosis']\n",
    "\n",
    "# 목표변수 라벨링\n",
    "y = y.map({'양성': 1, '음성': 0})\n",
    "\n",
    "# class unbalanced이므로 stratify=y & test_size=0.4\n",
    "X_train, X_test, y_train, y_test =\\\n",
    "        train_test_split(X, y, test_size=0.4, stratify=y)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "# KNN 모델 성능에 스케일링이 영향을 미친다\n",
    "stscl = StandardScaler()\n",
    "\n",
    "X_train = stscl.fit_transform(X_train)\n",
    "X_test = stscl.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb146ef-cb9e-4f3e-a0b1-020f0c8e957e",
   "metadata": {
    "id": "2eb146ef-cb9e-4f3e-a0b1-020f0c8e957e"
   },
   "source": [
    "## 11-1. KNN 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c09672-a83b-4cd2-a73e-c2269858e6db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:45:59.113467Z",
     "start_time": "2021-08-28T15:45:59.102853Z"
    },
    "id": "b3c09672-a83b-4cd2-a73e-c2269858e6db",
    "outputId": "d56ea824-d371-42d5-da2c-66273da37ecd"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn0 = KNeighborsClassifier()\n",
    "knn0.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train Accuracy (without param):\", knn0.score(X_train, y_train).round(3))\n",
    "print(\"Test Accuracy (without param):\", knn0.score(X_test, y_test).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049e51cc-ae89-4803-a506-e665f8095276",
   "metadata": {
    "id": "049e51cc-ae89-4803-a506-e665f8095276"
   },
   "source": [
    "## 11-2. KNN 성능 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06d5011-067e-409f-8de0-e9b49320b5f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:46:00.055408Z",
     "start_time": "2021-08-28T15:45:59.822227Z"
    },
    "id": "f06d5011-067e-409f-8de0-e9b49320b5f4",
    "outputId": "0dea389b-1ad9-4cf4-dc6e-96758bead6aa"
   },
   "outputs": [],
   "source": [
    "train_acc, test_acc = [], []\n",
    "\n",
    "para_n_neighbors=[i for i in range(1,31)]\n",
    "\n",
    "for v_n_neighbors in para_n_neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors=v_n_neighbors)\n",
    "    knn.fit(X_train, y_train)\n",
    "    train_acc.append(knn.score(X_train, y_train))\n",
    "    test_acc.append(knn.score(X_test, y_test))\n",
    "\n",
    "df_acc_neighbors = pd.DataFrame()\n",
    "df_acc_neighbors[\"Neighbors\"] = para_n_neighbors\n",
    "df_acc_neighbors[\"TrainAccuracy\"] = train_acc\n",
    "df_acc_neighbors[\"TestAccuracy\"] = test_acc\n",
    "\n",
    "df_acc_neighbors.round(3).head()\n",
    "\n",
    "plt.plot(para_n_neighbors, train_acc, linestyle=\"-\", label=\"Train Accuracy\")\n",
    "plt.plot(para_n_neighbors,test_acc, linestyle=\"--\", label=\"Test Accuracy\")\n",
    "plt.xlabel(\"n_neighbors\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034e548b-7ca2-4584-86ab-51ee62e59ae8",
   "metadata": {
    "id": "034e548b-7ca2-4584-86ab-51ee62e59ae8"
   },
   "source": [
    "- n_neighbors를 1~30로 변화시키며 성능을 측정해봤는데, 특이하게도 test score의 변동이 굉장히 불규칙했다.\n",
    "- test score가 가장 높은 값은 3, 5, 9, 14 값인데, 너무 변동이 심해서 어느 하나를 고르기가 쉽지가 않다.\n",
    "    - n_neighbors뿐 아니라 weights 파라미터도 함께 변경해가며 성능을 측정해보고 결정하기로 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff96e22-d9b4-465d-821a-5ea21899cdd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:46:01.384689Z",
     "start_time": "2021-08-28T15:46:01.190911Z"
    },
    "id": "3ff96e22-d9b4-465d-821a-5ea21899cdd2"
   },
   "outputs": [],
   "source": [
    "train_acc, test_acc = [], []\n",
    "\n",
    "para_n_neighbors = [i for i in range(1,31)]\n",
    "para_weights = (['uniform']) * 15 + (['distance'] * 15)\n",
    "\n",
    "for (v_n_neighbors,v_weights) in zip(para_n_neighbors,para_weights):\n",
    "    knn=KNeighborsClassifier(n_neighbors=v_n_neighbors,weights=v_weights)\n",
    "    knn.fit(X_train,y_train)\n",
    "    train_acc.append(knn.score(X_train,y_train))\n",
    "    test_acc.append(knn.score(X_test,y_test))\n",
    "\n",
    "df_acc_weights=pd.DataFrame()\n",
    "df_acc_weights[\"Neighbors\"]=para_n_neighbors\n",
    "df_acc_weights[\"Weights\"]=para_weights\n",
    "df_acc_weights[\"TrainAccuracy\"]=train_acc\n",
    "df_acc_weights[\"TestAccuracy\"]=test_acc\n",
    "\n",
    "df_acc_weights.round(3).sort_values(by='TestAccuracy', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbd2877-77fb-4d19-9a09-8b054518470e",
   "metadata": {
    "id": "ecbd2877-77fb-4d19-9a09-8b054518470e"
   },
   "source": [
    "- test acc를 기준으로 정렬한 결과,\n",
    "    - 가장 높은 test acc는 0.93이었다.\n",
    "    - n_neighbors=1, weights=uniform의 경우 train acc가 1.0이었는데, 이는 n_neighbors=1로 훈련데이터에 과적합된 모델이므로 제외한다.<br><br>\n",
    "    \n",
    "    - weights=distance인 경우의 성능이 default 파라미터인 uniform보다 모두 낮게 나오는 관계로, 위에서와 똑같이 n_neighbors를 3, 5, 9, 14 중 결정해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c6839f-b194-46b6-9447-60a9a9525286",
   "metadata": {
    "id": "01c6839f-b194-46b6-9447-60a9a9525286"
   },
   "source": [
    "- metric 파라미터를 바꾸면 변화가 있을까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7c1790-5b58-4b67-969c-201cc74cf0e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:46:02.772179Z",
     "start_time": "2021-08-28T15:46:02.326998Z"
    },
    "id": "0b7c1790-5b58-4b67-969c-201cc74cf0e9"
   },
   "outputs": [],
   "source": [
    "train_acc, test_acc = [], []\n",
    "\n",
    "para_n_neighbors = [i for i in range(1,31)] * 3\n",
    "para_weights = ((['uniform']) * 15 + (['distance'] * 15)) * 3\n",
    "para_metric = ((['minkowski']) * 15 + (['euclidean']) * 15 +(['manhattan']) * 15) * 2\n",
    "\n",
    "for (v_n_neighbors,v_weights, v_metric) in zip(para_n_neighbors,para_weights, para_metric):\n",
    "    knn=KNeighborsClassifier(n_neighbors=v_n_neighbors,weights=v_weights, metric=v_metric)\n",
    "    knn.fit(X_train,y_train)\n",
    "    train_acc.append(knn.score(X_train,y_train))\n",
    "    test_acc.append(knn.score(X_test,y_test))\n",
    "\n",
    "df_acc_metric=pd.DataFrame()\n",
    "df_acc_metric[\"Neighbors\"]=para_n_neighbors\n",
    "df_acc_metric[\"Weights\"]=para_weights\n",
    "df_acc_metric['Metrics']=para_metric\n",
    "df_acc_metric[\"TrainAccuracy\"]=train_acc\n",
    "df_acc_metric[\"TestAccuracy\"]=test_acc\n",
    "\n",
    "df_acc_metric.sort_values(by='TestAccuracy', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9eea71-5c68-4562-b148-ac1ef3a58d7a",
   "metadata": {
    "id": "7a9eea71-5c68-4562-b148-ac1ef3a58d7a"
   },
   "source": [
    "- metric / weights / n_neighbors를 모두 한꺼번에 조합해본 결과,\n",
    "    - uniform / manhattan / 8 일 때와, uniform / manhattan / 10 일 때가 성능이 가장 좋았다.\n",
    "    - 이 중 n_neighbors가 조금 더 낮은 모델이 더 일반화된 성능을 보여줄 것이라 예상해서, n_neighbors=8로 설정하기로 했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab53676a-a60e-4c48-9241-d32812138d70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:46:03.122320Z",
     "start_time": "2021-08-28T15:46:03.116312Z"
    },
    "id": "ab53676a-a60e-4c48-9241-d32812138d70",
    "outputId": "5ae51da6-744f-4c0e-8fba-f55ed3788bb0"
   },
   "outputs": [],
   "source": [
    "knn_set = KNeighborsClassifier(n_neighbors=8, weights='uniform',\n",
    "                               metric='manhattan')\n",
    "knn_set.fit(X_train, y_train)\n",
    "\n",
    "print(confusion_matrix(y_test, knn_set.predict(X_test)))\n",
    "\n",
    "print(\"Accuracy(정확도):\", accuracy_score(y_test, knn_set.predict(X_test)).round(3))\n",
    "print(\"Recall(재현율):\", recall_score(y_test, knn_set.predict(X_test)).round(3))\n",
    "print(\"Precision(정밀도):\", precision_score(y_test, knn_set.predict(X_test)).round(3))\n",
    "\n",
    "# knn score df\n",
    "knn_score = pd.DataFrame(index=['KNN'])\n",
    "knn_score['Accuracy'] = [accuracy_score(y_test, knn_set.predict(X_test))]\n",
    "knn_score['Precision'] = [precision_score(y_test, knn_set.predict(X_test))]\n",
    "knn_score['Recall'] = [recall_score(y_test, knn_set.predict(X_test))]\n",
    "knn_score['Specificity'] = [recall_score(y_test, knn_set.predict(X_test), pos_label=0)]\n",
    "knn_score['F1Score'] = [f1_score(y_test, knn_set.predict(X_test))]\n",
    "\n",
    "knn_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7dde1d-5219-45f1-beac-cf4efd81b4bb",
   "metadata": {
    "id": "9b7dde1d-5219-45f1-beac-cf4efd81b4bb"
   },
   "source": [
    "# 12. 최종 모델 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5df138-530d-4c4f-9ac4-3a8884e11dd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T15:55:35.417875Z",
     "start_time": "2021-08-28T15:55:35.410588Z"
    },
    "id": "0c5df138-530d-4c4f-9ac4-3a8884e11dd9"
   },
   "outputs": [],
   "source": [
    "score_df = pd.concat(objs=[logit_score, tree_score, forest_score, svm_score, nn_score, knn_score], axis=0)\n",
    "\n",
    "# randomforest의 score를 구하기 위해 gridsearch를 다시 해보니\n",
    "    # 어떤 오류에선지 score가 계속 비정상적으로 나와서 이전에 구해뒀던 score를\n",
    "    # 직접 입력해주었다.\n",
    "score_df.loc['Random Forest'] = [0.945, 0.95, 0.884, 0.854, 0.915]\n",
    "\n",
    "score_df\n",
    "\n",
    "score_df.plot.bar(figsize=(10,6))\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b508b14e-5050-4ac3-b0e9-7ee45180e06a",
   "metadata": {
    "id": "b508b14e-5050-4ac3-b0e9-7ee45180e06a"
   },
   "source": [
    "#### Explainability와 Interpretability가 가장 뛰어난 모델\n",
    "- Logistic Regression\n",
    "- Decision Tree\n",
    "\n",
    "#### 정확도 순위\n",
    "- SVM > Neural Network > Logistic Regression > KNN > Random Forest > Decision Tree\n",
    "\n",
    "#### 재현율 순위\n",
    "- SVM > Neural Network > KNN > Logistic Regression > Random Forest > Decision Tree\n",
    "\n",
    "#### 정밀도 순위\n",
    "- Logistic Regression > SVM > Random Forest > Neural Network > Decision Tree > KNN\n",
    "\n",
    "#### 특이도 순위\n",
    "- Logistic Regression > SVM > Neural Network > Decision Tree > KNN > Random Forest\n",
    "\n",
    "#### F1 Score 순위\n",
    "- SVM > Neural Network > Logistic Regression > KNN > Random Forest > Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986fd173-117d-4b5f-ba15-97df99afa1cd",
   "metadata": {
    "id": "986fd173-117d-4b5f-ba15-97df99afa1cd",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Comment\n",
    "> 유방암 진단 모델에서 가장 높은 우선순위를 가지는 지표는 재현율이라고 판단했다. 순위를 대략적으로 훑어보면, 재현율뿐 아니라 모든 성능지표에서 PCA+SVM 모델이 가장 뛰어났고, 그 다음으로 눈에 띄는 모델은 Neural Network였다. <br><br>\n",
    "\n",
    "> Decision Tree의 경우 설명성(explainability)가 뛰어나고 직관적 이해가 가능하다는 정말 큰 장점이 있지만, 성능의 경우 거의 모든 성능지표에서 가장 뒤떨어졌다. 오히려 설명성을 추구한다면, 유방암 데이터셋에선 Decision Tree보다 Logistic Regression을 선택하는 게 더 좋을 수도 있을 것 같다. <br><br>\n",
    "\n",
    "> PCA를 이용한 SVM 모델과 Neural Network 모델이 단연 눈에 띈 것은 맞지만, PCA/SVM/NN 모두 다 설명성이 크게 떨어진다는 단점은 무시할 수가 없다. 예를 들어, \"어떤 설명변수가 어떻게 조정되어야 목표변수가 어떻게 변할지\" 등에 대해선 설명하며 개선안을 제안하기는 힘들다.. <br><br>\n",
    "\n",
    "> 유방암 진단 모델의 주 목적이 무엇이냐가 중요할 것 같다.\n",
    "> - 환자의 데이터(설명변수)를 어떻게 조정했을 때 유방암 발생 확률이 낮아진다는 것을 알고 싶은 목적이라면, 즉 '개선안 도출' 혹은 '처방'의 목적이라면.. 설명성을 어떻게든 확보해야할 것이다.\n",
    "> - 하지만 만약 이미 주어진 환자의 데이터, 그리고 추가적으로 수집할 환자의 데이터를 가지고 이 환자가 유방암에 걸렸는지 안걸렸는지를 정확히 '예측'하는 것이 목적이면.. 설명성이 떨어지더라도 PCA+SVM이 가장 매력적인 모델일 것 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac058a25-eb49-4b18-83c2-9c6cdb28bbfa",
   "metadata": {
    "id": "ac058a25-eb49-4b18-83c2-9c6cdb28bbfa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b2c1386-e6e2-4d31-9efc-8dfde2072336",
   "metadata": {},
   "source": [
    "## Make_regression / classification / blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea98128-2707-4972-9cc2-a1f09462a2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "features, target, coefficients = \\\n",
    "    make_regression(n_samples = 100,\n",
    "                    n_features = 3,\n",
    "                    n_informative = 3,\n",
    "                    n_targets = 1,\n",
    "                    noise = 0.0,\n",
    "                    coef = True,\n",
    "                    random_state = 1)\n",
    "\n",
    "print('Feature Matrix\\n', features[:3])\n",
    "print('Target Vector\\n', target[:3])\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "features, target = \\\n",
    "    make_classification(n_samples = 100,\n",
    "                        n_features = 3,\n",
    "                        n_informative = 3,\n",
    "                        n_redundant = 0,\n",
    "                        n_classes = 2,\n",
    "                        weights = [.25, .75],\n",
    "                        random_state = 1)\n",
    "\n",
    "print('Feature Matrix\\n', features[:3])\n",
    "print('Target Vector\\n', target[:3])\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "features, target = \\\n",
    "    make_blobs(n_samples = 100,\n",
    "               n_features = 2,\n",
    "               centers = 3,\n",
    "               cluster_std = 0.5,\n",
    "               shuffle = True,\n",
    "               random_state = 1)\n",
    "\n",
    "print('Feature Matrix\\n', features[:3])\n",
    "print('Target Vector\\n', target[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae10cb6e-5204-47c7-b5ea-f1b33dfba263",
   "metadata": {},
   "source": [
    "## Replace multiple values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3a3435-60ca-4c1e-92ad-e021b21dd01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/chrisalbon/sim_data/master/titanic.csv'\n",
    "\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "df['Sex'].replace(['female', 'male'], ['Woman', 'Man']).head(5)\n",
    "\n",
    "# replace accepts reg expression\n",
    "df.replace(r\"1st\", \"First\", regex=True).head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72d2ee9-0235-4f6d-b532-6799004183a9",
   "metadata": {},
   "source": [
    "## Renaming columns - useful snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2ed66f-8d43-4f5a-8d61-f7a7ebc2809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "# Create dictionary\n",
    "column_names = collections.defaultdict(str)\n",
    "\n",
    "# Create keys\n",
    "for name in df.columns:\n",
    "    column_names[name]\n",
    "\n",
    "# Show dictionary\n",
    "column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be508a5a-1e86-480c-a710-89850fe0bbf3",
   "metadata": {},
   "source": [
    "## Dropping duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6e62cb-3fde-4c58-a647-f3b1de4a5516",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/chrisalbon/sim_data/master/titanic.csv'\n",
    "\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "df.drop_duplicates().head(2)\n",
    "\n",
    "print('Number of Rows in the Original DataFrame:', len(df))\n",
    "print('Number of Rows after Deduping:', len(df.drop_duplicates()))\n",
    "\n",
    "df.drop_duplicates(subset=['Sex'])\n",
    "\n",
    "df.drop_duplicates(subset=['Sex'], keep='last')\n",
    "\n",
    "# Good option when you don't want to simply delete duplicates\n",
    "df.duplicated()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af87a67a-eeed-43f7-9006-5b581039db4f",
   "metadata": {},
   "source": [
    "## Grouping rows by time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4f8c7e-7c4c-492e-943a-44897354b349",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_index = pd.date_range('06/06/2017', periods=100000, freq='30S')\n",
    "\n",
    "dataframe = pd.DataFrame(index=time_index)\n",
    "\n",
    "dataframe['Sale_Amount'] = np.random.randint(1, 10, 100000)\n",
    "\n",
    "dataframe.resample('W').sum()\n",
    "\n",
    "dataframe.head(3)\n",
    "\n",
    "dataframe.resample('2W').mean()\n",
    "\n",
    "dataframe.resample('M').count()\n",
    "\n",
    "# groupby를 했음에도 Label은 특정 date로 나옴\n",
    "# label='left'를 주면, groupby되는 첫 번째 index 이름으로 설정\n",
    "dataframe.resample('M', label='left').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8d21ab-eb37-40d2-a474-f41a96e64a8b",
   "metadata": {},
   "source": [
    "## Aggregating Operations and Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd31ec3-f723-481c-a806-5a38551743c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/chrisalbon/sim_data/master/titanic.csv'\n",
    "\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "df.agg('min')\n",
    "\n",
    "df.min()\n",
    "\n",
    "df.agg({'Age':['mean'], 'SexCode':['min', 'max']})\n",
    "\n",
    "df.groupby(\n",
    "    ['PClass', 'Survived']).agg({'Survived':['count']}\n",
    "   ).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8444a58c-b16f-4d80-9237-7ec9e61a925d",
   "metadata": {},
   "source": [
    "## Time series periods / frequenciesW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b15e062-17b6-417e-9e90-a0ae8719fe34",
   "metadata": {},
   "source": [
    "### pd.Period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805731f6-1c7d-40c0-83eb-be17981bdde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1_2019 = pd.Period('2019', freq = 'W-MON')\n",
    "W1_2019\n",
    "\n",
    "# 1을 더하면 1주를 더한 효과\n",
    "W1_2019 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c61966-40a6-4d74-a6e1-6ae200a616c1",
   "metadata": {},
   "source": [
    "### pd.date_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fff564a-2e91-4f7e-aca4-af8afd2b61f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq='W' defaults to Sunday\n",
    "Weeks_2020 = pd.date_range('2020-01-01', '2020-12-31', freq='W')\n",
    "print(Weeks_2020[:6])\n",
    "\n",
    "shift = pd.Timedelta('6 days')\n",
    "\n",
    "(Weeks_2020 + shift)[:6]\n",
    "\n",
    "sensor_times\n",
    "\n",
    "sensor_times = ((pd.date_range('00:00:00', '00:09:59.9', freq = '100ms')) - pd.to_datetime('00:00:00')).total_seconds()\n",
    "\n",
    "sensor_times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969bf14d-5874-4877-a725-95f8966a7963",
   "metadata": {},
   "source": [
    "## Preprocessing the German climate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a60c1a-8b27-4ae5-8320-f042ccce755b",
   "metadata": {},
   "outputs": [],
   "source": [
    "precipitation_dataPath = 'https://opendata.dwd.de/climate_environment/CDC/observations_global/CLIMAT/monthly/qc/precipitation_total/historical/01001_195101_201712.txt'\n",
    "vapor_dataPath = 'https://opendata.dwd.de/climate_environment/CDC/observations_global/CLIMAT/monthly/qc/vapour_pressure/historical/98836_196801_201712.txt'\n",
    "sunshine_dataPath = 'https://opendata.dwd.de/climate_environment/CDC/observations_global/CLIMAT/monthly/qc/sunshine_duration/historical/98836_197803_201612.txt'\n",
    "\n",
    "precipitation_data = pd.read_csv(precipitation_dataPath,delimiter=';')\n",
    "vapor_data = pd.read_csv(vapor_dataPath,delimiter=';')\n",
    "sunshine_data = pd.read_csv(sunshine_dataPath,delimiter=';')\n",
    "\n",
    "precipitation_data.head()\n",
    "\n",
    "vapor_data.head()\n",
    "\n",
    "sunshine_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26c7114-04d0-483a-9092-92932bdd0d20",
   "metadata": {},
   "source": [
    "### Wide to long format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6dcf1d-ea7f-48d9-aaf0-a6fed8b19e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert wide format data to long format\n",
    "precipitation_data = pd.melt(precipitation_data,\n",
    "                             id_vars = ['Jahr'],\n",
    "                             value_vars = ['Jan', 'Feb',\n",
    "                                           'Mrz', 'Apr',\n",
    "                                           'Mai', 'Jun',\n",
    "                                           'Jul', 'Aug',\n",
    "                                           'Sep', 'Okt',\n",
    "                                           'Nov', 'Dez'])\n",
    "precipitation_data.head()\n",
    "\n",
    "# renaming columns\n",
    "precipitation_data = precipitation_data.rename(\n",
    "    columns = {'value': 'Precipitation'})\n",
    "\n",
    "# melt vapor data\n",
    "vapor_data = pd.melt(vapor_data,\n",
    "                     id_vars = ['Jahr'],\n",
    "                     value_vars = ['Jan', 'Feb',\n",
    "                                   'Mrz', 'Apr',\n",
    "                                   'Mai', 'Jun',\n",
    "                                   'Jul', 'Aug',\n",
    "                                   'Sep', 'Okt',\n",
    "                                   'Nov', 'Dez'])\n",
    "vapor_data = vapor_data.rename(\n",
    "    columns = {'value': 'Vapour_Pressure'})\n",
    "vapor_data.head()\n",
    "\n",
    "# melt sunshine data\n",
    "sunshine_data = pd.melt(sunshine_data,\n",
    "                        id_vars = ['Jahr'],\n",
    "                        value_vars= ['Jan','Feb',\n",
    "                                     'Mrz','Apr',\n",
    "                                     'Mai','Jun',\n",
    "                                     'Jul','Aug',\n",
    "                                     'Sep','Okt',\n",
    "                                     'Nov','Dez'])\n",
    "sunshine_data = sunshine_data.rename(\n",
    "    columns={\"value\": \"Sun_shine\"})\n",
    "sunshine_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51825f3b-1db0-47c4-9584-45b9f947264e",
   "metadata": {},
   "source": [
    "### Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7924cb50-d9e5-49db-848f-e29be31c9cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "conDf = pd.merge(precipitation_data, vapor_data,\n",
    "                 how='inner', on=['Jahr', 'variable'])\n",
    "conDf.head()\n",
    "\n",
    "conDf = pd.merge(conDf, sunshine_data,\n",
    "                 how='inner', on=['Jahr', 'variable'])\n",
    "conDf.head()\n",
    "\n",
    "# renaming columns\n",
    "months = {'Mrz':'Mar', 'Mai':'May', 'Okt':'Oct', 'Dez': 'Dec'}\n",
    "\n",
    "conDf['variable'] = conDf['variable'].map(months)\\\n",
    "    .fillna(conDf['variable'])\n",
    "\n",
    "conDf.tail()\n",
    "\n",
    "conDf = conDf.rename(columns={'Jahr':'Year'})\n",
    "conDf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9bf61f-230b-442e-a9e1-e06c618c6974",
   "metadata": {},
   "source": [
    "### Data interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62874fa0-4e02-4249-8488-55bb8d9edd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting by year and month\n",
    "## change variable as an alphabetical order to numerical order\n",
    "\n",
    "conDf['months'] = conDf['variable']\n",
    "conDf.head(3)\n",
    "\n",
    "monthsMap = {'Jan':1, 'Feb':2, 'Mar':3, 'Apr':4,\n",
    "             'May':5, 'Jun':6, 'Jul':7, 'Aug':8,\n",
    "             'Sep':9, 'Oct':10, 'Nov':11, 'Dec':12}\n",
    "\n",
    "conDf['months'] = conDf['months'].map(monthsMap)\\\n",
    "    .fillna(conDf['months'])\n",
    "conDf\n",
    "\n",
    "newCondf = conDf.sort_values(by=['Year', 'months'])\n",
    "newCondf\n",
    "\n",
    "# Impute missing values\n",
    "cleanDf1 = newCondf.interpolate(method='linear',\n",
    "                                limit_direction='both')\n",
    "\n",
    "cleanDf1\n",
    "\n",
    "# groupby methods\n",
    "Q1 = cleanDf1.groupby(['variable'])['Vapour_Pressure']\\\n",
    "    .agg('mean').loc['Jan']\n",
    "Q1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d7f173-c489-4e6e-828d-4dbfdedb2a3a",
   "metadata": {},
   "source": [
    "## Tidy Data\n",
    "1. Each variable must have its own column.\n",
    "2. Each observation must have its own row.\n",
    "3. Each value must have its own cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c172e5d0-c979-467a-9ddc-0bb190d4c3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "pew = pd.read_csv(\"https://github.com/chendaniely/pandas_for_everyone/blob/master/data/pew.csv?raw=true\")\n",
    "\n",
    "\n",
    "pew.head(2)\n",
    "\n",
    "pew_long = pew.melt(id_vars='religion', var_name='income', value_name='count')\n",
    "\n",
    "pew_long.sort_values('religion').head()\n",
    "\n",
    "billboard = pd.read_csv(\"https://github.com/chendaniely/pandas_for_everyone/blob/master/data/billboard.csv?raw=true\")\n",
    "\n",
    "\n",
    "billboard.head(3)\n",
    "\n",
    "billboard_long = billboard.melt(\n",
    "    id_vars = ['year', 'artist', 'track', 'time', 'date.entered'],\n",
    "    var_name='week',\n",
    "    value_name='rating'\n",
    ")\n",
    "\n",
    "###### SORTING by Artist and Week ######\n",
    "billboard_long.sort_values(by=['artist', 'week'],\n",
    "                           key = lambda s: s.str[2:].astype(int) \n",
    "                           if s.name!='artist' else s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971e7af7-2059-451e-89f9-463d475dffbc",
   "metadata": {},
   "source": [
    "### Columns contain multiple variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdfde09-3c95-4b29-b057-f40764d90bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ebola = pd.read_csv(\"https://github.com/chendaniely/pandas_for_everyone/blob/master/data/country_timeseries.csv?raw=true\")\n",
    "\n",
    "\n",
    "ebola.head(3)\n",
    "\n",
    "ebola_long = ebola.melt(id_vars=['Date', 'Day'])\n",
    "\n",
    "ebola_long.head(3)\n",
    "\n",
    "variable_split = ebola_long.variable.str.split('_')\n",
    "variable_split.head(3)\n",
    "\n",
    "variable_split.get(0)\n",
    "\n",
    "variable_split.str.get(0)\n",
    "\n",
    "status_values = variable_split.str.get(0)\n",
    "country_values = variable_split.str.get(1)\n",
    "\n",
    "ebola_long['status'] = status_values\n",
    "ebola_long['country'] = country_values\n",
    "\n",
    "ebola_long.head(3)\n",
    "\n",
    "##### 위와 다르게 하는 방법\n",
    "ebola_long = ebola.melt(id_vars=['Date', 'Day'])\n",
    "\n",
    "variable_split = ebola_long.variable.str.split('_', expand=True)\n",
    "\n",
    "ebola_long[['status', 'country']] = variable_split\n",
    "\n",
    "ebola_long.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daee3cfb-1cb8-4e9f-97f4-edba7913e040",
   "metadata": {},
   "source": [
    "## Apply Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc981cf-0b38-4892-a4ad-edd93cb0e48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"a\": [10, 20, 30], \"b\": [20, 30, 40]})\n",
    "\n",
    "# argument가 2개인 함수 apply\n",
    "def my_exp(x, e):\n",
    "    return x ** e\n",
    "\n",
    "df['a'].apply(my_exp, e=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff139b2-2a1c-4105-91fd-024f770fb6db",
   "metadata": {},
   "source": [
    "### Apply의 동작\n",
    "- when we use .apply(), **the entire column** is passed into the **first** argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12eb6d42-864d-4bfc-8fe6-ed267c325bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_3(x, y, z):\n",
    "    return (x + y + z) / 3\n",
    "\n",
    "df.apply(avg_3) # error\n",
    "\n",
    "def avg_3_apply(col):\n",
    "    x = col[0]\n",
    "    y = col[1]\n",
    "    z = col[2]\n",
    "    return (x + y + z) / 3\n",
    "\n",
    "df.apply(avg_3_apply)\n",
    "\n",
    "def agg(col):\n",
    "    return col.sum() / len(col)\n",
    "\n",
    "df.apply(agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad815591-bae4-4789-aa9d-c869592a8a67",
   "metadata": {},
   "source": [
    "### Row-wise\n",
    "- axis=1 을 주면, the **entire row** is used as the **first** argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef27e6ee-a210-43fa-bd0e-c785e3df433e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(agg, axis=1)\n",
    "\n",
    "def avg_2_apply(col):\n",
    "    x = col[0]\n",
    "    y = col[1]\n",
    "    return (x + y) / 2\n",
    "\n",
    "df.apply(avg_2_apply, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a5f27a-361c-4169-b101-bd3b50305896",
   "metadata": {},
   "source": [
    "## Groupby Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8eb956-a25b-4e35-b888-b2e229c5e5f0",
   "metadata": {},
   "source": [
    "### Pandas Method\n",
    "- count (not incluing NaN values)\n",
    "- size (including NaN values)\n",
    "- mean\n",
    "- std\n",
    "- min\n",
    "- quantile\n",
    "- max\n",
    "- sum\n",
    "- var\n",
    "- sem\n",
    "- describe\n",
    "- first\n",
    "- last\n",
    "- nth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0213d993-45cb-4460-9347-d97f8d5412b8",
   "metadata": {},
   "source": [
    "### Numpy Scipy Methods (agg function)\n",
    "- np.count_nonzero()\n",
    "- np.mean()\n",
    "- np.std()\n",
    "- np.min()\n",
    "- np.percentile()\n",
    "- np.max()\n",
    "- np.sum()\n",
    "- np.var()\n",
    "- scipy.stats.sem()\n",
    "- scipy.stats.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23e1cba-a950-418a-84ca-155c6563ac85",
   "metadata": {},
   "source": [
    "### Custom Functions with agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32695cc5-6879-4584-a213-e578e3dfee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://github.com/chendaniely/pandas_for_everyone/blob/master/data/gapminder.tsv?raw=true', sep='\\t')\n",
    "\n",
    "def my_mean(values):\n",
    "    return values.sum() / len(values)\n",
    "\n",
    "df.groupby('year')['lifeExp'].agg(np.mean)\n",
    "\n",
    "df.groupby('year')['lifeExp'].agg(my_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275ee0d3-160b-47c2-9bbf-7fab985cb191",
   "metadata": {},
   "source": [
    "### agg function with dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5d74a6-ce6c-4d6d-95e5-9ab322cb09af",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_dict = df.groupby('year').agg(\n",
    "    {\n",
    "        'lifeExp': 'mean',\n",
    "        'pop': 'median',\n",
    "        'gdpPercap': 'median'\n",
    "    }\n",
    ")\n",
    "\n",
    "gdf_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2597235-a286-4963-a4b6-d7c059f5de75",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37de81f3-9388-4760-b07f-26608674fec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_zscore(x):\n",
    "    return ((x - x.mean()) / x.std())\n",
    "\n",
    "transform_z = df.groupby('year')['lifeExp'].transform(my_zscore)\n",
    "\n",
    "transform_z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33782470-ca84-4a62-8950-0ca5949036f3",
   "metadata": {},
   "source": [
    "### Missing Value Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195fd21b-4c98-4550-ac12-932bb574a111",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "tips_10 = sns.load_dataset('tips').sample(10)\n",
    "\n",
    "# randomly pick 4 'total_bill' values and turn them into missing\n",
    "tips_10.loc[\n",
    "    np.random.permutation(tips_10.index)[:4],\n",
    "    'total_bill'\n",
    "] = np.NaN\n",
    "\n",
    "print(tips_10)\n",
    "\n",
    "def fill_na_mean(x):\n",
    "    avg = x.mean()\n",
    "    return x.fillna(avg)\n",
    "\n",
    "total_bill_group_mean = (\n",
    "    tips_10\n",
    "    .groupby('sex')\n",
    "    .total_bill\n",
    "    .transform(fill_na_mean)\n",
    ")\n",
    "\n",
    "tips_10['fill_total_bill'] = total_bill_group_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9f9174-79e2-4263-a6e1-00e2e15ab8c2",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fffff9-3c89-4c6a-8aeb-ba020db766ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips = sns.load_dataset('tips')\n",
    "tips.shape\n",
    "\n",
    "tips['size'].value_counts()\n",
    "\n",
    "tips_filtered = (\n",
    "    tips\n",
    "    .groupby('size')\n",
    "    .filter(lambda x: x['size'].count() >= 30)\n",
    ")\n",
    "\n",
    "tips_filtered['size'].value_counts()\n",
    "\n",
    "#### refs ####\n",
    "tips.groupby('size')['size'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c2979e-c484-4edc-a87b-14ee7263f72c",
   "metadata": {},
   "source": [
    "### Groupby Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc23055d-e674-42d6-a952-ae31170343b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_10 = sns.load_dataset('tips').sample(10, random_state=42)\n",
    "\n",
    "grouped = tips_10.groupby('sex')\n",
    "grouped\n",
    "\n",
    "grouped.groups\n",
    "\n",
    "grouped.mean(numeric_only=True)\n",
    "\n",
    "female = grouped.get_group('Female')\n",
    "female"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24e6062-b23c-45c6-8507-4ff82846f32d",
   "metadata": {},
   "source": [
    "## Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e91679c-7c54-4328-8a03-7b98baac239b",
   "metadata": {},
   "source": [
    "### Load Data - nan\n",
    "- `pd.read_csv(na_values=['-', 99])` : 예를 들어 '-', 99를 nan으로 불러온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043b3821-c1ed-44cf-8b89-f486da1f5da3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83f482ca-efad-4112-ab81-88ab1356e41d",
   "metadata": {},
   "source": [
    "## String Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44e045e-0b0c-40c6-ac43-55f6c08e8bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"black Knight\".capitalize()\n",
    "\n",
    "\"It's just a flesh wound!\".count('u')\n",
    "\n",
    "\"Halt! Who goes there?\".startswith('Halt')\n",
    "\n",
    "\"coconut\".endswith('nut')\n",
    "\n",
    "\"It's just a flesh wound!\".find('u')\n",
    "\n",
    "\"It's just a flesh wound!\".index('flesh')\n",
    "\n",
    "\"old woman\".isalpha()  # there is a whitespace\n",
    "\n",
    "\"37\".isdecimal()\n",
    "\n",
    "\"I'm 37\".isalnum()  #apostrophe and space\n",
    "\n",
    "\"Black Knight\".lower()\n",
    "\n",
    "\"Black Knight\".upper()\n",
    "\n",
    "\"flesh wound!\".replace('flesh wound', 'scratch')\n",
    "\n",
    "\" I'm not dead.    \".strip()\n",
    "\n",
    "\"NI! NI! NI! NI!\".split(sep=' ')\n",
    "\n",
    "\"3,4\".partition(',')\n",
    "\n",
    "\"nine\".center(10)\n",
    "\n",
    "\"9\".zfill(5)\n",
    "\n",
    "multi_str = \"\"\"Guard: What? Ridden on a horse?\n",
    "King Arthur: Yes!\n",
    "Guard: You're using coconuts!\n",
    "King Arthur: What?\n",
    "Guard: You've got ... coconut[s] and you're bangin' 'em together.\n",
    "\"\"\"\n",
    "\n",
    "print(multi_str)\n",
    "\n",
    "multi_str_split = multi_str.splitlines()\n",
    "\n",
    "print(multi_str_split)\n",
    "\n",
    "# use the format specification mini-language\n",
    "digits = 67890\n",
    "s = f\"In 2005, Lu Chao of China recited {67890:,} digits of pi.\"\n",
    "print(s)\n",
    "\n",
    "prop = 7 / 67890\n",
    "s = f\"I remember {prop:.4} or {prop:.4%} of what Lu Chao recited.\"\n",
    "print(s)\n",
    "\n",
    "id = 42\n",
    "print(f\"My ID number is {id:05d}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0129e476-aab6-4734-9501-8e98e905dcff",
   "metadata": {},
   "source": [
    "## Dates and Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48c75b0-db52-4022-9add-f9fcac237c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "t1 = datetime.now()\n",
    "t2 = datetime(1970, 1,1)\n",
    "\n",
    "diff = t1 - t2\n",
    "print(diff)\n",
    "\n",
    "print(type(diff))\n",
    "\n",
    "t1.year, t1.month, t1.day\n",
    "\n",
    "url = \"https://github.com/chendaniely/pandas_for_everyone/blob/master/data/banklist.csv?raw=true\"\n",
    "banks = pd.read_csv(url,\n",
    "                   parse_dates=['Closing Date', 'Updated Date'])\n",
    "\n",
    "banks.head(3)\n",
    "\n",
    "banks = banks.assign(\n",
    "    closing_quarter = banks['Closing Date'].dt.quarter,\n",
    "    closing_year = banks['Closing Date'].dt.year\n",
    ")\n",
    "\n",
    "closing_year = banks.groupby(['closing_year']).size()\n",
    "\n",
    "closing_year_q = (\n",
    "  banks\n",
    "  .groupby(['closing_year', 'closing_quarter'])\n",
    "  .size()\n",
    ")\n",
    "\n",
    "closing_year_q\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaad098c-0b33-445d-8147-a2c20a2da603",
   "metadata": {},
   "source": [
    "## Reindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d43bb5-6932-46cd-b1df-c56f6234e850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "051698b7-9a20-4f58-9559-0cb28dd08722",
   "metadata": {},
   "source": [
    "## pd.Series Methods\n",
    "- append\n",
    "- corr\n",
    "- cov\n",
    "- describe\n",
    "- drop_duplicates\n",
    "- equals\n",
    "- get_values\n",
    "- hist\n",
    "- isin\n",
    "- min\n",
    "- max\n",
    "- mean\n",
    "- median\n",
    "- mode\n",
    "- quantile\n",
    "- replace\n",
    "- sample\n",
    "- sort_values\n",
    "- to_frame\n",
    "- transpose\n",
    "- unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebed914-d0fb-42a2-8cb3-3db967d662f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e9820d-9cb8-4602-b7c5-94568e6011b5",
   "metadata": {},
   "source": [
    "## sample to randomzie index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36d5725-3018-4d15-80eb-5ee4f48b9543",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://github.com/chendaniely/pandas_for_everyone/blob/master/data/billboard.csv?raw=true\")\n",
    "\n",
    "df = df.sample(frac=1, random_state=42)\n",
    "df.head(5)\n",
    "\n",
    "# randomize 'time'\n",
    "df['time'] = df['time'].sample(frac=1, random_state=42)\n",
    "\n",
    "df.head(5)\n",
    "# 위처럼 하면 바뀌지 않는다.\n",
    "# 왜냐면 pandas는 index와 value가 join되어 있기 때문임\n",
    "\n",
    "# index info를 없애고 values로 넣어주면 된다\n",
    "df['time'] = (\n",
    "    df['time']\n",
    "        .sample(frac=1, random_state=42)\n",
    "        .values\n",
    ")\n",
    "\n",
    "df.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d33427-9d89-43f0-917f-fb917545103c",
   "metadata": {},
   "source": [
    "## df.values to return numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d13f6a-8a19-46fd-b2ca-18beba1e1d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb43d9f-5f93-4d81-8c24-09798b3932d0",
   "metadata": {},
   "source": [
    "## subtracting / astype the date values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f334abd4-10a9-4678-a20d-5645cf38f9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://github.com/chendaniely/pandas_for_everyone/blob/master/data/scientists.csv?raw=true\")\n",
    "\n",
    "born_datetime = pd.to_datetime(df['Born'], format='%Y-%m-%d')\n",
    "died_datetime = pd.to_datetime(df['Died'], format='%Y-%m-%d')\n",
    "\n",
    "df['born_dt'], df['died_dt'] = (\n",
    "    born_datetime,\n",
    "    died_datetime\n",
    ")\n",
    "\n",
    "df['age_days'] = (\n",
    "    df['died_dt'] - df['born_dt']\n",
    ")\n",
    "\n",
    "# convert dt values to 'year' using astype method\n",
    "#### not working ####\n",
    "# df['age_years'] = (\n",
    "#     df['age_days'].values.astype('timedelta64[Y]')\n",
    "# )\n",
    "\n",
    "df['age_years'] = (\n",
    "    np.floor(df['age_days'] / np.timedelta64(365, 'D')).astype(int)\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab7415d-f2b6-40bb-bff9-ad324554b4d9",
   "metadata": {},
   "source": [
    "## Modifying columns with assign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca38294f-86a5-425b-86a2-1f6b54ca5cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(\n",
    "    age_days_assign = df['died_dt'] - df['born_dt'],\n",
    "    age_year_assign = np.floor(df['age_days'] / np.timedelta64(365, 'D')).astype(int)\n",
    ")\n",
    "\n",
    "df.head(2)\n",
    "\n",
    "## assign의 benefit은 앞서 지정한 칼럼 값을 뒤이어 지정할 칼럼에 사용할 수 있다는 것\n",
    "## 그렇게 하려면 lambda를 사용해야 한다.\n",
    "\n",
    "df = df.assign(\n",
    "    age_days_assign = df['died_dt'] - df['born_dt'],\n",
    "    age_year_assign = lambda df_: np.floor(df_[\"age_days_assign\"] / np.timedelta64(365,'D'))\\\n",
    "        .astype(int)\n",
    ")\n",
    "\n",
    "df.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b106d7b-7e32-494b-92e1-7946af02f68e",
   "metadata": {},
   "source": [
    "## Plotting Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be97d37-7682-4115-ba46-929435c1bee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "anscombe = sns.load_dataset(\"anscombe\")\n",
    "\n",
    "dataset_1 = anscombe[anscombe['dataset'] == 'I']\n",
    "dataset_2 = anscombe[anscombe['dataset'] == 'II']\n",
    "dataset_3 = anscombe[anscombe['dataset'] == 'III']\n",
    "dataset_4 = anscombe[anscombe['dataset'] == 'IV']\n",
    "\n",
    "# you need to run all the plotting code together, same as above\n",
    "fig = plt.figure()\n",
    "axes1 = fig.add_subplot(2, 2, 1)\n",
    "axes2 = fig.add_subplot(2, 2, 2)\n",
    "axes3 = fig.add_subplot(2, 2, 3)\n",
    "axes4 = fig.add_subplot(2, 2, 4)\n",
    "axes1.plot(dataset_1['x'], dataset_1['y'], 'o')\n",
    "axes2.plot(dataset_2['x'], dataset_2['y'], 'o')\n",
    "axes3.plot(dataset_3['x'], dataset_3['y'], 'o')\n",
    "axes4.plot(dataset_4['x'], dataset_4['y'], 'o')\n",
    "\n",
    "# add a small title to each subplot\n",
    "axes1.set_title(\"dataset_1\")\n",
    "axes2.set_title(\"dataset_2\")\n",
    "axes3.set_title(\"dataset_3\")\n",
    "axes4.set_title(\"dataset_4\")\n",
    "\n",
    "# add a title for the entire figure (title above the title)\n",
    "fig.suptitle(\"Anscombe Data\") # note spelling of \"suptitle\"\n",
    "\n",
    "# use a tight layout so the plots and titles don't overlap\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "# show the figure\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f73857-0c63-4080-bf7c-e4d8892bb7fd",
   "metadata": {},
   "source": [
    "## Statistical Graphics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5a5167-b14a-4e94-a019-c96ab007d1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips = sns.load_dataset(\"tips\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85444d6-f756-4a80-bd46-a52d1a5a7d0f",
   "metadata": {},
   "source": [
    "### Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9594ede8-7156-4779-aac6-ba4b297b3bb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create the figure object\n",
    "fig = plt.figure()\n",
    "\n",
    "# subplot has 1 row, 1 column, plot location 1\n",
    "axes1 = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "# make the actual histogram\n",
    "axes1.hist(data=tips, x='total_bill', bins=10)\n",
    "\n",
    "# add labels\n",
    "axes1.set_title('Histogram of Total Bill')\n",
    "axes1.set_xlabel('Frequency')\n",
    "axes1.set_ylabel('Total Bill')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7538ae00-2b5f-4904-a954-bade14f45c42",
   "metadata": {},
   "source": [
    "### Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4080736c-aea6-49af-83ff-6c551d352c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the figure object\n",
    "scatter_plot = plt.figure()\n",
    "axes1 = scatter_plot.add_subplot(1, 1, 1)\n",
    "\n",
    "# make the actual scatter plot\n",
    "axes1.scatter(data=tips, x='total_bill', y='tip')\n",
    "\n",
    "# add labels\n",
    "axes1.set_title('Scatterplot of Total Bill vs Tip')\n",
    "axes1.set_xlabel('Total Bill')\n",
    "axes1.set_ylabel('Tip')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3998c35a-45cd-4d5b-853e-f30bdbfc2614",
   "metadata": {},
   "source": [
    "### Boxplot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0e2153-3c42-4e0f-91fe-000df1c535bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the figure object\n",
    "boxplot = plt.figure()\n",
    "axes1 = boxplot.add_subplot(1, 1, 1)\n",
    "\n",
    "# make the actual box plot\n",
    "axes1.boxplot(\n",
    "  # first argument of box plot is the data\n",
    "  # since we are plotting multiple pieces of data\n",
    "  # we have to put each piece of data into a list\n",
    "  x=[\n",
    "      tips.loc[tips[\"sex\"] == \"Female\", \"tip\"],\n",
    "      tips.loc[tips[\"sex\"] == \"Male\", \"tip\"],\n",
    "  ],\n",
    "  # we can then pass in an optional labels parameter\n",
    "  # to label the data we passed\n",
    "  labels=[\"Female\", \"Male\"],\n",
    ")\n",
    "\n",
    "\n",
    "# add labels\n",
    "axes1.set_xlabel('Sex')\n",
    "axes1.set_ylabel('Tip')\n",
    "axes1.set_title('Boxplot of Tips by Gender')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f5ed99-ff74-4cd0-8956-51e5ae47ee15",
   "metadata": {},
   "source": [
    "### Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08a847c-7108-47df-951d-a3d30ab2aabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign color values\n",
    "colors = {\n",
    "    \"Female\": \"#f1a340\",  # orange\n",
    "    \"Male\": \"#998ec3\",    # purple\n",
    "}\n",
    "\n",
    "scatter_plot = plt.figure()\n",
    "axes1 = scatter_plot.add_subplot(1, 1, 1)\n",
    "\n",
    "axes1.scatter(\n",
    "  data=tips,\n",
    "  x='total_bill',\n",
    "  y='tip',\n",
    "\n",
    "  # set the size of the dots based on party size\n",
    "  # we multiply the values by 10 to make the points bigger\n",
    "  # and also to emphasize the difference\n",
    "  s=tips[\"size\"] ** 2 * 10,\n",
    "\n",
    "\n",
    "  # set the color for the sex using our color values above\n",
    "  c=tips['sex'].map(colors),\n",
    "\n",
    "  # set the alpha so points are more transparent\n",
    "  # this helps with overlapping points\n",
    "  alpha=0.5\n",
    ")\n",
    "\n",
    "# label the axes\n",
    "axes1.set_title('Colored by Sex and Sized by Size')\n",
    "axes1.set_xlabel('Total Bill')\n",
    "axes1.set_ylabel('Tip')\n",
    "\n",
    "# figure title on top\n",
    "scatter_plot.suptitle(\"Total Bill vs Tip\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab92ed4-c6ae-4182-a869-992ecf35c9c9",
   "metadata": {},
   "source": [
    "## Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0663734-521f-4f95-8f85-75b867129d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the default seaborn context optimized for paper print\n",
    "# the default is \"notebook\"\n",
    "sns.set_context(\"paper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173e08b2-fc42-4381-ad56-143b6cc72fef",
   "metadata": {},
   "source": [
    "### Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4535be-3630-495c-9a53-1c963dc5ee1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the subplots function is a shortcut for\n",
    "# creating separate figure objects and\n",
    "# adding individual subplots (axes) to the figure\n",
    "hist, ax = plt.subplots()\n",
    "\n",
    "# use seaborn to draw a histogram into the axes\n",
    "sns.histplot(data=tips, x=\"total_bill\", ax=ax)\n",
    "\n",
    "# use matplotlib notation to set a title\n",
    "ax.set_title('Total Bill Histogram')\n",
    "\n",
    "# use matplotlib to show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31944f6b-af48-4367-895a-fcc0b35882db",
   "metadata": {},
   "source": [
    "### Density Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d15b0a6-f48d-4c77-8ec6-f539dac6f6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "den, ax = plt.subplots()\n",
    "\n",
    "sns.kdeplot(data=tips, x=\"total_bill\", ax=ax)\n",
    "\n",
    "ax.set_title('Total Bill Density')\n",
    "ax.set_xlabel('Total Bill')\n",
    "ax.set_ylabel('Unit Probability')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefe15dc-724a-4958-9965-d4ffdee2d3d9",
   "metadata": {},
   "source": [
    "### Rug Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7508b669-06d9-484f-bfb2-7eab1faf73b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rug, ax = plt.subplots()\n",
    "\n",
    "# plot 2 things into the axes we created\n",
    "sns.rugplot(data=tips, x=\"total_bill\", ax=ax)\n",
    "sns.histplot(data=tips, x=\"total_bill\", ax=ax)\n",
    "\n",
    "ax.set_title(\"Rug Plot and Histogram of Total Bill\")\n",
    "ax.set_title(\"Total Bill\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f757f76-f3b4-46ad-8731-d2d8009b2f7b",
   "metadata": {},
   "source": [
    "### Distribution Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8fec0c-2c51-4b2c-b041-5b03392a3b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the FacetGrid object creates the figure and axes for us\n",
    "fig = sns.displot(data=tips, x=\"total_bill\", kde=True, rug=True)\n",
    "\n",
    "fig.set_axis_labels(x_var=\"Total Bill\", y_var=\"Count\")\n",
    "fig.figure.suptitle('Distribution of Total Bill')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "count, ax = plt.subplots()\n",
    "\n",
    "# we can use the viridis palette to help distinguish the colors\n",
    "sns.countplot(data=tips, x='day', palette=\"viridis\", ax=ax)\n",
    "\n",
    "ax.set_title('Count of days')\n",
    "ax.set_xlabel('Day of the Week')\n",
    "ax.set_ylabel('Frequency')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b77350-65c1-41b5-9bba-a9a9730f9489",
   "metadata": {},
   "source": [
    "### Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf113633-8e6d-47f6-acce-b352ae2647c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter, ax = plt.subplots()\n",
    "\n",
    "# use fit_reg=False if you do not want the regression line\n",
    "sns.scatterplot(data=tips, x='total_bill', y='tip', ax=ax)\n",
    "\n",
    "ax.set_title('Scatter Plot of Total Bill and Tip')\n",
    "ax.set_xlabel('Total Bill')\n",
    "ax.set_ylabel('Tip')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b5ec0c-7536-4678-bc12-23190f07a20e",
   "metadata": {},
   "source": [
    "### Regression Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa5975d-8d23-4a36-b9cc-61054a0fa8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg, ax = plt.subplots()\n",
    "\n",
    "# use fit_reg=False if you do not want the regression line\n",
    "sns.regplot(data=tips, x='total_bill', y='tip', ax=ax)\n",
    "\n",
    "ax.set_title('Regression Plot of Total Bill and Tip')\n",
    "ax.set_xlabel('Total Bill')\n",
    "ax.set_ylabel('Tip')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8729c7d9-d5af-4376-b326-d627e1d0bd43",
   "metadata": {},
   "source": [
    "### Joint Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e77da7-6b1f-4c76-9852-cf1598afc2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jointplot creates the figure and axes for us\n",
    "joint = sns.jointplot(data=tips, x='total_bill', y='tip')\n",
    "\n",
    "joint.set_axis_labels(xlabel='Total Bill', ylabel='Tip')\n",
    "\n",
    "# add a title and move the text up so it doesn't clash with histogram\n",
    "joint.figure.suptitle('Joint Plot of Total Bill and Tip', y=1.03)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf36b60-3457-42e3-aef4-c3f02f07b6b3",
   "metadata": {},
   "source": [
    "### Hexbin Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1a1efb-f0e7-4f0e-b887-5e7745b0565b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use jointplot with kind=\"hex\" for a hexbin plot\n",
    "hexbin = sns.jointplot(\n",
    "  data=tips, x=\"total_bill\", y=\"tip\", kind=\"hex\"\n",
    ")\n",
    "\n",
    "hexbin.set_axis_labels(xlabel='Total Bill', ylabel='Tip')\n",
    "hexbin.figure.suptitle('Hexbin Plot of Total Bill and Tip', y=1.03)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c440f3-f068-465e-a51a-bb3dfb6dbdeb",
   "metadata": {},
   "source": [
    "### 2D Density Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63559d6a-ca9c-4517-b63e-687baab448af",
   "metadata": {},
   "outputs": [],
   "source": [
    "kde, ax = plt.subplots()\n",
    "\n",
    "# shade will fill in the contours\n",
    "sns.kdeplot(data=tips, x=\"total_bill\", y=\"tip\", shade=True, ax=ax)\n",
    "\n",
    "ax.set_title('Kernel Density Plot of Total Bill and Tip')\n",
    "ax.set_xlabel('Total Bill')\n",
    "ax.set_ylabel('Tip')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bcc57a-b48e-4bd7-a670-e0d3c4ecd14e",
   "metadata": {},
   "source": [
    "### 2D KDE Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6fff7e-d5d3-4705-9432-419418de9c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "kde2d = sns.jointplot(data=tips, x=\"total_bill\", y=\"tip\", kind=\"kde\")\n",
    "\n",
    "kde2d.set_axis_labels(xlabel='Total Bill', ylabel='Tip')\n",
    "kde2d.fig.suptitle('2D KDE Plot of Total Bill and Tip', y=1.03)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6aac1c-47f0-4c11-b7b5-601aa37330ac",
   "metadata": {},
   "source": [
    "### Bar Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164b2beb-0751-4216-9511-88a3581a68dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "bar, ax = plt.subplots()\n",
    "\n",
    "# plot the average total bill for each value of time\n",
    "# mean is calculated using numpy\n",
    "sns.barplot(\n",
    "  data=tips, x=\"time\", y=\"total_bill\", estimator=np.mean, ax=ax\n",
    ")\n",
    "\n",
    "ax.set_title('Bar Plot of Average Total Bill for Time of Day')\n",
    "ax.set_xlabel('Time of Day')\n",
    "ax.set_ylabel('Average Total Bill')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ce1d15-96f0-4577-8299-9ac22ddbf5cd",
   "metadata": {},
   "source": [
    "### Box Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8feffdd-1dcd-4bca-8aec-3c80e0da626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "box, ax = plt.subplots()\n",
    "\n",
    "# the y is optional, but x would have to be a numeric variable\n",
    "sns.boxplot(data=tips, x='time', y='total_bill', ax=ax)\n",
    "\n",
    "ax.set_title('Box Plot of Total Bill by Time of Day')\n",
    "ax.set_xlabel('Time of Day')\n",
    "ax.set_ylabel('Total Bill')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e06d4f-adb0-405b-b51d-44e287bf749e",
   "metadata": {},
   "source": [
    "### Violin Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07bcefe-ce9f-4d1b-9b30-94c698da0bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "violin, ax = plt.subplots()\n",
    "\n",
    "sns.violinplot(data=tips, x='time', y='total_bill', ax=ax)\n",
    "\n",
    "ax.set_title('Violin plot of total bill by time of day')\n",
    "ax.set_xlabel('Time of day')\n",
    "ax.set_ylabel('Total Bill')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd00e2f-517c-485d-b10c-70f5cf5101f5",
   "metadata": {},
   "source": [
    "### Multivariate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36db01b-9c5d-445d-a60f-095b0bf7d62e",
   "metadata": {},
   "source": [
    "### Colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045918be-f6de-476b-b717-a1b5f0845a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "violin, ax = plt.subplots()\n",
    "\n",
    "sns.violinplot(\n",
    "  data=tips,\n",
    "  x=\"time\",\n",
    "  y=\"total_bill\",\n",
    "  hue=\"smoker\", # set color based on smoker variable\n",
    "  split=True,\n",
    "  palette=\"viridis\", # palette specifies the colors for hue\n",
    "\n",
    "  ax=ax,\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# note the use of lmplot instead of regplot to return a figure\n",
    "scatter = sns.lmplot(\n",
    "  data=tips,\n",
    "  x=\"total_bill\",\n",
    "  y=\"tip\",\n",
    "  hue=\"smoker\",\n",
    "  fit_reg=False,\n",
    "  palette=\"viridis\",\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09621aba-ed2d-4c8a-96d7-2eb4a3318e72",
   "metadata": {},
   "source": [
    "### Size and Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da25e31-a7f5-44a1-a7dd-939f16d69dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.scatterplot(\n",
    "  data=tips,\n",
    "  x=\"total_bill\",\n",
    "  y=\"tip\",\n",
    "  hue=\"time\",\n",
    "  size=\"size\",\n",
    "  palette=\"viridis\",\n",
    "  ax=ax,\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfdc25f-e9d9-4d11-a9f1-b257f4c65ad2",
   "metadata": {},
   "source": [
    "## Facets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f583a379-7118-43c2-885f-46bf37a48f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "anscombe_plot = sns.relplot(\n",
    "  data=anscombe,\n",
    "  x=\"x\",\n",
    "  y=\"y\",\n",
    "  kind=\"scatter\",\n",
    "  col=\"dataset\",\n",
    "\n",
    "  col_wrap=2,\n",
    "  height=2,\n",
    "  aspect=1.6, # aspect ratio of each facet\n",
    ")\n",
    "\n",
    "anscombe_plot.figure.set_tight_layout(True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "colors = {\n",
    "  \"Yes\": \"#f1a340\", # orange\n",
    "  \"No\" : \"#998ec3\", # purple\n",
    "}\n",
    "# make the faceted scatter plot\n",
    "# this is the only part that is needed to draw the figure\n",
    "facet2 = sns.relplot(\n",
    "  data=tips,\n",
    "  x=\"total_bill\",\n",
    "  y=\"tip\",\n",
    "  hue=\"smoker\",\n",
    "  style=\"sex\",\n",
    "\n",
    "  kind=\"scatter\",\n",
    "  col=\"day\",\n",
    "  row=\"time\",\n",
    "  palette=colors,\n",
    "  height=1.7, # adjusted to fit figure on page\n",
    ")\n",
    "\n",
    "# below is to make the plot pretty\n",
    "# adjust facet titles\n",
    "facet2.set_titles(\n",
    "  row_template=\"{row_name}\",\n",
    "  col_template=\"{col_name}\"\n",
    ")\n",
    "\n",
    "# adjust the legend to not have it overlap the figure\n",
    "sns.move_legend(\n",
    "  facet2,\n",
    "  loc=\"lower center\",\n",
    "  bbox_to_anchor=(0.5, 1),\n",
    "  ncol=2,   #number legend columns\n",
    "  title=None,   #legend title\n",
    "  frameon=False, #remove frame (i.e., border box) around legend\n",
    ")\n",
    "\n",
    "facet2.figure.set_tight_layout(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e62c1e0-859f-4b6b-ba9c-0b9158d64c71",
   "metadata": {},
   "source": [
    "## FacetGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60934dac-aee1-4b5c-be55-485fed832d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the FacetGrid\n",
    "facet = sns.FacetGrid(tips, col='time')\n",
    "\n",
    "# for each value in time, plot a histogram of total bill\n",
    "# you pass in parameters as if you were passing them directly\n",
    "# into sns.histplot()\n",
    "facet.map(sns.histplot, 'total_bill')\n",
    "plt.show()\n",
    "\n",
    "facet = sns.FacetGrid(\n",
    "  tips, col='day', hue='sex', palette=\"viridis\"\n",
    ")\n",
    "facet.map(plt.scatter, 'total_bill', 'tip')\n",
    "facet.add_legend()\n",
    "plt.show()\n",
    "\n",
    "facet = sns.FacetGrid(\n",
    "  tips, col='time', row='smoker', hue='sex', palette=\"viridis\"\n",
    ")\n",
    "facet.map(plt.scatter, 'total_bill', 'tip')\n",
    "plt.show()\n",
    "\n",
    "facet = sns.catplot(\n",
    "    x=\"day\",\n",
    "    y=\"total_bill\",\n",
    "    hue=\"sex\",\n",
    "    data=tips,\n",
    "    row=\"smoker\",\n",
    "    col=\"time\",\n",
    "    kind=\"violin\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411286cf-5402-49a6-8f51-a222bc14481d",
   "metadata": {},
   "source": [
    "## Finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f97ad61-343e-49af-a25f-b15ebc22b010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "df = yf.download('AAPL',\n",
    "                 start='2011-01-01',\n",
    "                 end='2021-12-31',\n",
    "                 progress=False)\n",
    "\n",
    "print(f\"Downloaded {len(df)} rows of data.\")\n",
    "\n",
    "df.head()\n",
    "\n",
    "aapl_data = yf.Ticker('AAPL')\n",
    "\n",
    "aapl_data.history().info()\n",
    "\n",
    "aapl_data.major_holders\n",
    "\n",
    "aapl_data.institutional_holders\n",
    "\n",
    "aapl_data.financials\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc4075a-5052-48ad-b5f4-376340beefe1",
   "metadata": {},
   "source": [
    "## Looping for iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc2d951-58f2-4af8-9663-2f11cad4548a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# old way\n",
    "for index, row in df.iterrows():\n",
    "    # do sth with row['column']\n",
    "\n",
    "# New way\n",
    "df['column'] = df['column'] + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0edf810-f98c-4ab3-a53c-6f5350235030",
   "metadata": {},
   "source": [
    "## agg & groupby function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825f3684-395a-4c52-ab39-1b940389c906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b0cd4b4-3920-4b34-89a7-c617aa0bfebd",
   "metadata": {},
   "source": [
    "## Pivot Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52c8f1c-7c13-4fd7-a17d-fcf1d08785b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32f656d-a8dc-4e8a-856d-01e4163af48c",
   "metadata": {},
   "source": [
    "## Multi-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48fac82-d8d3-4505-8632-39f445f5e19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24aea80-879e-4c54-9c20-60fa666b64df",
   "metadata": {},
   "source": [
    "## 클래스화, 모듈화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c04606f-23ab-4b06-815e-d318f8215836",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bdb80f-3b52-4c3e-8ef7-1d10f75e11a3",
   "metadata": {},
   "source": [
    "## Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f913506c-8bd6-4f18-8b65-af85ae9f9feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path = Path('My Drive/Data Science/pyexcel-master/data')\n",
    "\n",
    "path\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218d207b-0435-487b-8fd6-52af4473626d",
   "metadata": {},
   "source": [
    "## xlsxWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6794331-8859-49b1-87b7-83e9a5f42bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "\n",
    "workbook = xlsxwriter.Workbook('excelfile')\n",
    "\n",
    "{'shrink': True,\n",
    " 'align': 'center',\n",
    " 'valign': 'vcenter',\n",
    " 'border': 1,\n",
    " 'bottom': 6,\n",
    " 'bg_color': 'green',\n",
    " 'font_name': '바탕체',\n",
    " 'font_size': 10,\n",
    " 'bold': True,\n",
    " 'center_across': True,}\n",
    "\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "input_folder = '/Users/jihunkang/My Drive/Data Science/pyexcel-master/data/ch08'\n",
    "\n",
    "raw_data_dir = Path(input_folder)\n",
    "excel_files = raw_data_dir.glob('영업팀별_*')\n",
    "\n",
    "for ex in excel_files:\n",
    "    print(ex)\n",
    "\n",
    "for excel_file in excel_files:\n",
    "    df = pd.read_excel(excel_file)\n",
    "    total_df = total_df.append(df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87997f95",
   "metadata": {},
   "source": [
    "# Data Processing and Manipulation\n",
    "<a id='Data-Processing-and-Manipulation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b0776f",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "<a id='Exploratory-Data-Analysis-(EDA)'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa86cb4",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "<a id='Machine-Learning'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e87cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed909180",
   "metadata": {},
   "source": [
    "/Users/jihun/Documents/data_science/notebooks/ds_school/image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5d5eb6",
   "metadata": {},
   "source": [
    "## Norm\n",
    "벡터 $a$의 길이는 **norm** $||a||$라고 한다.  \n",
    "\n",
    "$ ||a|| = \\sqrt {a^Ta} = \\sqrt {a^2_1 + \\dots + a^2_N} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3449de5b",
   "metadata": {},
   "source": [
    "## Unit vector\n",
    "길이가 1인 벡터를 **단위벡터**라고 한다.  \n",
    "$$\n",
    "\\begin{align}\n",
    "a = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} ,\\;\\;\n",
    "b = \\begin{bmatrix}0 \\\\ 1\\end{bmatrix} ,\\;\\;\n",
    "c = \\begin{bmatrix} \\dfrac{1}{\\sqrt{2}} \\\\ \\dfrac{1}{\\sqrt{2}} \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$  \n",
    "  \n",
    "영벡터가 아닌 임의의 벡터 $x$를 벡터의 길이 $||x||$로 나눠주면 벡터 $x$와 같은 방향을 가리키는 단위벡터가 된다.  \n",
    "  \n",
    "$$ \\dfrac {x} {||x||} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d8a706",
   "metadata": {},
   "source": [
    "## Linear combination\n",
    "**벡터의 선형조합(linear combination)** 이란 여러 개의 벡터를 스칼라곱을 한 후 더한 것이다.  \n",
    "   \n",
    "$$ c_1x_1 + c_2x_2 + \\dots + c_Nx_N $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e47713",
   "metadata": {},
   "source": [
    "## Euclidean distance\n",
    "두 벡터가 가리키는 점 사이의 거리를 **유클리드 거리(Euclidean distance)** 라고 한다.  \n",
    "두 벡터의 유클리드 거리는 '벡터의 차의 길이'로 구한다.  \n",
    "<br>\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\| a - b \\|\n",
    "&= \\sqrt{\\sum_{i=1} (a_i - b_i)^2} \\\\\n",
    "&= \\sqrt{\\sum_{i=1} ( a_i^2 - 2 a_i b_i + b_i^2 )} \\\\\n",
    "&= \\sqrt{\\sum_{i=1} a_i^2 + \\sum_{i=1} b_i^2 - 2 \\sum_{i=1} a_i b_i} \\\\\n",
    "&= \\sqrt{\\| a \\|^2 + \\| b \\|^2  - 2 a^Tb }\n",
    "\\end{align}\n",
    "$$  \n",
    "\n",
    "즉,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\| a - b \\|^2 = \\| a \\|^2 + \\| b \\|^2 - 2 a^T b\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e3a63f",
   "metadata": {},
   "source": [
    "## Dot product\n",
    "두 벡터의 **내적** 은 두 벡터의 길이와 두 벡터 사이의 각도 $\\theta$의 코사인 함수값으로 계산할 수도 있다.  \n",
    "  \n",
    "$$ a^Tb = \\| a \\| \\| b \\| cos \\theta $$  \n",
    "  \n",
    "두 벡터가 직교하면 내적은 0이 된다.  \n",
    "두 벡터가 같은 방향일 때 내적값이 제일 커진다.  \n",
    "$$\\cos 0^{\\circ} = 0$$\n",
    "$$\\cos 90^{\\circ} = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d77e3c",
   "metadata": {},
   "source": [
    "## Cosine  similarity\n",
    "두 벡터의 방향이 비슷할수록 벡터가 비슷하다고 간주하여 두 벡터 사이의 각의 코사인값을 **코사인 유사도**라고 한다.  \n",
    "두 벡터가 같은 방향을 가리키고 있으면 코사인 유사도가 최댓값 1을 가진다.  \n",
    "  \n",
    "$$ cosine\\;similarity = \\cos \\theta  = \\dfrac{x^Ty}{\\| x \\| \\| y \\|} $$  \n",
    "  \n",
    "**코사인 거리(cosine distance)** 는 $ cosine\\;distance = 1 - cosine\\;similarity$ 로 정의된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5827752",
   "metadata": {},
   "source": [
    "## Vector decomposition\n",
    "\n",
    "어떤 두 벡터 $a,\\;b$의 합이 다른 벡터 $c$가 될 때 $c$가 두 벡터 **성분(component)** $a,\\;b$로 **분해(decomposition)** 된다고 말한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559c11e7",
   "metadata": {},
   "source": [
    "## 투영성분과 직교성분\n",
    "\n",
    "벡터 $a$를 다른 벡터 $b$에 직교하는 성분과 벡터 $b$에 평행한 성분으로로 분해할 수 있는데, 평행한 성분을 벡터 $b$에 대한 **투영성분(projection)**, 벡터 $b$에 직교하는 성분을 벡터 $b$에 대한 **직교성분(rejection)**이라고 하며 각각 다음과 같이 표기한다.\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "a^{\\Vert b} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "a^{\\perp b} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "투영성분의 길이는 다음처럼 구할 수 있다.\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\| a^{\\Vert b} \\| \n",
    "= \\|a\\|\\cos\\theta \n",
    "= \\dfrac{\\|a\\|\\|b\\|\\cos\\theta}{\\|b\\|}  \n",
    "= \\dfrac{a^Tb}{\\|b\\|} \n",
    "= \\dfrac{b^Ta}{\\|b\\|}\n",
    "= a^T\\dfrac{b}{\\|b\\|} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "만약 벡터 $b$ 자체가 이미 단위벡터이면 **단위벡터에 대한 투영길이는 내적**이 된다.\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\| a^{\\Vert b} \\| = a^Tb \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "투영성분 성분 벡터는 투영성분 길이와 벡터 $b$ 방향의 단위벡터의 곱이다.\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "a^{\\Vert b} = \\dfrac{a^Tb}{\\|b\\|} \\dfrac{b}{\\|b\\|}= \\dfrac{a^Tb}{\\|b\\|^2}b\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "직교성분 벡터는 원래의 벡터에서 투영성분 성분 벡터를 뺀 나머지다.\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "a^{\\perp b} = a - a^{\\Vert b}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2d2056",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='/Users/jihun/Documents/data_science/notebooks/ds_school/image/linalg4.png', width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6281d65f",
   "metadata": {},
   "source": [
    "## 직선의 방정식\n",
    "어떤 벡터 $w$가 있을 때\n",
    "- 원점에서 출발한 벡터 $w$가 가리키는 점을 지나면서\n",
    "- 벡터 $w$에 수직인 **직선의 방정식** 은 뭘까?<br>\n",
    "\n",
    "위 두 조건을 만족하는 직선 상의 임의의 점을 가리키는 벡터를 $x$라고 하면, 벡터 $x$가 가리키는 점과 벡터 $w$가 가리키는 점을 이은 벡터 $x - w$는 조건에 따라 벡터 $w$와 직교해야 한다. **이거 바로 위에 그림에서 벡터 $a$랑 벡터 $a^{\\Vert b}$의 뺄셈을 한 벡터를 보면 이해된다!!**  \n",
    "  \n",
    "따라서 다음 식이 성립한다.  \n",
    "$$ w^T(x - w) = 0 $$  \n",
    "정리하면 다음과 같다.  \n",
    "$$ w^T(x - w) = w^Tx - w^Tw = w^Tx - \\| w \\|^2 $$  \n",
    "$$ w^Tx - \\| w \\|^2 = 0 $$  \n",
    "\n",
    "이 직선과 원점 사이의 거리는 벡터 $w$의 norm $\\| w \\|$이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98617a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='/Users/jihun/Documents/data_science/notebooks/ds_school/image/linalg3.png', width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcb166e",
   "metadata": {},
   "source": [
    "예를 들어 \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "w = \\begin{bmatrix}1 \\\\ 2\\end{bmatrix} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "일 때\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\| w \\|^2 = 5 \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\begin{bmatrix}1 & 2\\end{bmatrix} \\begin{bmatrix}x_1 \\\\ x_2 \\end{bmatrix} - 5 = x_1 + 2x_2 - 5 = 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "x_1 + 2x_2 = 5 \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "이 방정식은 벡터 $w$가 가리키는 점 $(1, 2)$를 지나면서 벡터 $w$에 수직인 직선을 뜻한다. 이 직선과 원점 사이의 거리는 $\\|w\\|=\\sqrt{5}$이다.\n",
    "\n",
    "이번에는 벡터 $w$가 가리키는 점을 지나야 한다는 조건을 없애고 단순히\n",
    "\n",
    "* 벡터 $w$에 수직인\n",
    "\n",
    "직선 $x$의 방정식을 구해보자.\n",
    "\n",
    "이때는 직선이 $w$가 아니라 $w$와 방향이 같고 길이가 다른 벡터 $w'=cw$을 지날 것이다. $c$는 양의 실수이다. \n",
    "\n",
    "위에서 했던 방법으로 다시 직선의 방정식을 구하면 다음과 같다.\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "w'^Tx - \\| w' \\|^2 =  cw^Tx - c^2 \\| w \\|^2 = 0  \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "w^Tx - c \\| w \\|^2 = 0 \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "여기에서 $c \\| w \\|^2$는 임의의 수가 될 수 있으므로 단순히 벡터 $w$에 수직인 직선의 방정식은 다음과 같이 나타낼 수 있다.\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "w^Tx - w_0 = 0 \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "이 직선과 원점 사이의 거리는 다음과 같다.\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "c \\| w \\| = \\dfrac{w_0}{\\|w\\|} \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4afdca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='/Users/jihun/Documents/data_science/notebooks/ds_school/image/linalg2.png', width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b60e73",
   "metadata": {},
   "source": [
    "예를 들어 $c=0.5$이면 벡터 $w=[1, 2]^T$에 수직이고 원점으로부터의 거리가 $\\frac{\\sqrt{5}}{2}$인 직선이 된다.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "x_1 + 2x_2 - 2.5 = 0\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0411b083",
   "metadata": {},
   "source": [
    "## 직선과 점의 거리\n",
    "\n",
    "이번에는 직선 $w^Tx - \\|w\\|^2 = 0$ 과 이 직선 위에 있지 않은 점 $x'$ 사이의 거리를 구해보자.\n",
    "\n",
    "벡터 $w$에 대한 벡터 $x'$의 투영성분 $x'^{\\Vert w}$의 길이는 다음과 같다.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\|x'^{\\Vert w}\\| = \\dfrac{w^Tx'}{\\|w\\|} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "직선과 점 $x'$ 사이의 거리는 이 길이에서 원점에서 직선까지의 거리 $\\|w\\|$를 뺀 값의 절댓값이다. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\left|  \\|x'^{\\Vert w}\\| - \\|w\\| \\right| = \n",
    "\\left| \\dfrac{w^Tx'}{\\|w\\|} - \\|w\\| \\right| =\n",
    "\\dfrac{\\left|w^Tx' - \\|w\\|^2 \\right|}{\\|w\\|}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "직선의 방정식이 $w^Tx - w_0 = 0$이면 직선과 점의 거리는 다음과 같다.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\dfrac{\\left|w^Tx' - w_0 \\right|}{\\|w\\|} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "이 공식은 나중에 분류 방법의 하나인 서포트 벡터 머신(SVM: Support Vector Machine)에서 사용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7dbd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='/Users/jihun/Documents/data_science/notebooks/ds_school/image/linalg_.png', width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb4e728",
   "metadata": {},
   "source": [
    "## 선형종속과 선형독립\n",
    "\n",
    "벡터 집합 $x_1, x_2, \\ldots, x_N$을 이루는 벡터의 선형조합이 영벡터가 되도록 하는 스칼라 계수 $c_1, c_2, \\ldots, c_N$이 존재하면 이 벡터들이 **선형종속(linearly dependent)**이라고 한다. 단 $c_1 = c_2 = \\cdots = c_N = 0$ 으로 계수가 모두 0인 경우는 제외한다.\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "c_1 x_1 + c_2 x_2 + \\cdots + c_N x_N = 0  \n",
    "\\end{align}\n",
    "$$  \n",
    "\n",
    "예를 들어 아래의 경우 어떤 스칼라 계수를 사용해도 두 벡터의 선형조합을 영벡터가 되게 할 수 없다.\n",
    "$$ \n",
    "\\begin{align}\n",
    "x_1 = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}, \\;\\;\n",
    "x_2 = \\begin{bmatrix} 3 \\\\ 3 \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$  \n",
    "  \n",
    "  \n",
    "위와 같은 경우를 **선형독립(linearly independent)** 이라고 한다. \n",
    "선형독립을 논리 기호로 나타내면 다음과 같다.\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "c_1 x_1 + \\cdots + c_N x_N = 0  \\;\\; \\rightarrow \\;\\; c_1 = \\cdots = c_N = 0  \n",
    "\\end{align}\n",
    "$$ \n",
    "\n",
    "왼쪽에서 오른쪽 방향 화살표의 의미는 벡터들의 선형조합이 0이면 반드시 계수들이 모두 0이라는 뜻이다.  \n",
    "**계수들이 모두 0이 아니고서는 선형조합이 영벡터가 될 수 없다는 말이다.**  \n",
    "\n",
    "또는 선형독립을 다음처럼 표현하기도 한다.\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "c_1 x_1 + \\cdots + c_N x_N = 0  \\;\\; \\leftrightarrow \\;\\; c_1 = \\cdots = c_N = 0  \n",
    "\\end{align}\n",
    "$$ \n",
    "\n",
    "오른쪽에서 왼쪽 방향 화살표의 의미는 모든 계수가 0일 때 선형조합이 0이 된다는 뜻이다. 이는 꼭 선형독립이 아니더라도 당연하게 성립한다.<br><br>\n",
    "\n",
    "\n",
    "아래의 벡터 $x_1, x_2, x_3$는 선형종속이다.\n",
    "$$ \n",
    "\\begin{align}\n",
    "x_1 = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}, \\;\\;\n",
    "x_2 = \\begin{bmatrix} 3 \\\\ 3 \\end{bmatrix}, \\;\\;\n",
    "x_3 = \\begin{bmatrix} 5 \\\\ 7 \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "이는 다음과 같은 식으로 증명할 수 있다.\n",
    "$$ \n",
    "\\begin{align}\n",
    "2x_1  + x_2 - x_3 = 0\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545cdafd",
   "metadata": {},
   "source": [
    "## 선형독립과 선형 연립방정식\n",
    "\n",
    "\n",
    "위의 내용을 다음 식으로 표현할 수도 있다.\n",
    "$$ \n",
    "\\begin{align}\n",
    "c_1 x_1 + \\cdots + c_N x_N =\n",
    "\\begin{bmatrix} x_1 & x_2 & \\cdots & x_N \\end{bmatrix} \n",
    "\\begin{bmatrix} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_N \\end{bmatrix}\n",
    "= X c\n",
    "\\end{align}\n",
    "$$<br>\n",
    "\n",
    "- $c_i$는 $x_i$에 대한 가중치 계수이고, $c$는 $c_i$를 원소로 가지는 가중치 벡터이다.\n",
    "- $\\boldsymbol X$는 열벡터 $x_1, x_2, \\dots, x_N$을 열로 가지는 행렬이다.<br><br>\n",
    "\n",
    "\"따라서 어떤 벡터들이 선형독립인지 아닌지를 알아내는 문제는 선형연립방정식을 푸는 문제와 같다.\"  \n",
    "방정식 $\\boldsymbol Xc = 0$의 해가 영벡터밖에 없으면 선형독립이다. 만약 영벡터가 아닌 해가 존재하면 선형종속이다. (해가 무한히 많은 경우도 선형종속이다)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa7e17a",
   "metadata": {},
   "source": [
    "## 선형종속인 경우\n",
    "특징행렬 $\\boldsymbol X$의 열벡터들이 선형종속이거나 선형종속에 가까운 현상을 **다중공선성(multicollinearity)** 라고 부른다.  \n",
    "\"벡터가 선형종속이 되는 대표적인 세 가지 경우가 있다.\"  \n",
    "\n",
    "#### 1. 벡터의 개수가 벡터의 차원보다 크면 선형종속이다\n",
    "벡터의 차원보다 벡터의 수가 많으면 그 벡터를 행으로 가지는 행렬 $\\boldsymbol X$의 행의 개수보다 열의 개수가 많다.  \n",
    "따라서 이 행렬이 표현하는 연립방정식을 고려하면 \"미지수의 수가 방정식의 수보다 많은 것\"이며, 해가 무한히 많게 된다.  \n",
    "반대로 행의 개수가 열의 개수와 같거나 크면 대부분 선형독립이다.\n",
    "\n",
    "#### 2. 값이 같은 벡터가 있으면 반드시 선형종속이다.\n",
    "만약 $i$번째 벡터 $x_i$와 $j$번째 벡터 $x_j$가 같으면, 가중치 $c_j = -c_i$로 놓고 나머지 c값은 다 0으로 놓으면 선형조합은 영벡터가 된다.  \n",
    "혹은 벡터 $x_j$가 벡터 $x_i$의 실수배인 경우에도 반드시 선형종속이다.\n",
    "\n",
    "#### 3. 어떤 벡터가 다른 벡터의 선형조합이면 반드시 선형종속이다.\n",
    "예를 들어 벡터 $x_1$과 다른 벡터 $x_2, x_3$ 사이에 다음 관계가 성립한다고 하자.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "x_1 = 2 x_2 - 3 x_3\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "그러면 $c_1=-1, c_2=2 c_3=-3$일 때\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "-1\\cdot x_1 + 2 x_2 - 3 x_3 = 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "이므로 선형종속이다.\n",
    "\n",
    "이 경우도 데이터 분석에서 흔히 하는 실수이다. 예를 들어 국어, 영어, 수학 점수를 각각 별도의 데이터로 포함하면서 이 세 점수에 의존하는  총점수나 평균을 다시 데이터로 포함하면 선형종속이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29672f33",
   "metadata": {},
   "source": [
    "## 랭크\n",
    "행렬의 열벡터 중 서로 독립인 열벡터의 최대 개수를 **열랭크(column rank)** 라고 하고,  \n",
    "행벡터 중 서로 독립인 행벡터의 최대 개수를 **행랭크(row rank)** 라고 한다.  \n",
    "> 행랭크와 열랭크에 대해서는 다음 정리가 성립한다: **\"행랭크와 열랭크는 항상 같다.\"**<br><br>\n",
    "\n",
    "따라서 행랭크나 열랭크를 그냥 '랭크(rank)'라고 하기도 한다. 행렬 $\\boldsymbol A$의 랭크는 기호로 $rank\\boldsymbol A$로 쓴다.<br><br>\n",
    "\n",
    "행랭크는 행의 개수보다 클 수 없고 열랭크는 열의 개수보다 클 수 없기 때문에 NxM의 행렬에서 다음이 성립한다:\n",
    "$$ rankA \\leq min(M,N) $$<br><br>\n",
    "\n",
    "다음 행렬 $X_1$의 두 열벡터는 선형독립이기 때문에 열랭크는 2다.\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "X_1 = \\begin{bmatrix} 1 & 3 \\\\ 2 & 3 \\end{bmatrix} \n",
    "\\end{align}\n",
    "$$<br><br><br>\n",
    "\n",
    "\n",
    "다음 행렬 $X_2$의 세 열벡터는 선형종속이므로 열랭크는 3보다는 작다. 그런데 이 열벡터 중 앞의 두 개는 서로 독립이므로 $X_2$의 랭크는 2다.\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "X_2 = \\begin{bmatrix} 1 & 3 & 5 \\\\ 2 & 3 & 7 \\end{bmatrix} \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e3a4fb",
   "metadata": {},
   "source": [
    "## 풀랭크\n",
    "위의 예시로 든 행렬 $\\boldsymbol X_1$이나 $\\boldsymbol X_2$ 처럼 랭크가 행의 개수와 열의 개수 중 작은 값과 같으면 **풀랭크(Full Rank)** 라고 한다.<br>\n",
    "\n",
    "\"선형독립인 벡터들을 행 또는 열로 가지는 행렬을 만들면 항상 풀랭크다.\"<br>\n",
    "\n",
    "> 예를 들어.. $X_2$의 경우 열벡터의 최대 rank는 3이지만 두 개만 선형독립이라서 rank가 2였다. 근데 행벡터의 기준에서 봤을 때 rank가 2라는 것은 모든 행벡터가 선형독립이라는 것이다 => 열 혹은 행의 모든 벡터가 선형독립일 때 full-rank라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4f8309",
   "metadata": {},
   "source": [
    "## 로우-랭크 행렬\n",
    "$N$차원 벡터 $x$ 하나를 이용하여 만들어지는 다음과 같은 행렬을 **랭크-1 행렬(rank-1 matrix)**이라고 한다.\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "xx^T \\in \\mathbf{R}^{N \\times N}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "이 행렬의 열벡터들은 $x$라고 하는 하나의 벡터를 $x_1$배, $x_2$배, ... $x_n$배한 벡터이므로 독립적인 열벡터는 1개다. 따라서 **랭크-1 행렬의 랭크는 1**이다.\n",
    "> **추가: 예를 들어... x라는 열벡터가 3x1 이라면, $xx^T$는 3x3 matrix가 된다. 매트릭스의 열벡터는 서로 선형조합 관계이기 때문에 선형독립인 열벡터는 1개다.**\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\begin{aligned}\n",
    "xx^T \n",
    "&= x \\begin{bmatrix} x_1 & x_2 & \\cdots & x_n \\end{bmatrix} \\\\\n",
    "&= \\begin{bmatrix} x_1x & x_2x & \\cdots & x_nx \\end{bmatrix} \n",
    "\\end{aligned}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "선형독립인 두 개의 $N$차원 벡터 $x_1, x_2$를 이용하여 만든 다음과 같은 행렬은 **랭크-2 행렬(rank-2 matrix)**이라고 한다.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\begin{bmatrix} x_1 & x_2 \\end{bmatrix}\n",
    "\\begin{bmatrix} x_1^T \\\\ x_2^T \\end{bmatrix}\n",
    "= x_1x_1^T + x_2x_2^T\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "앞서와 비슷한 방법으로 **랭크-2 행렬의 랭크는 2**임을 보일 수 있다.\n",
    "\n",
    "만약 $M$개의 $N$차원 벡터 $x_1, x_2, \\cdots, x_M$을 이용하면 **랭크-M 행렬(rank-M matrix)**이 된다.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\begin{bmatrix} x_1 & x_2 & \\cdots & x_M \\end{bmatrix}\n",
    "\\begin{bmatrix} x_1^T \\\\ x_2^T \\\\ \\vdots \\\\ x_M^T \\end{bmatrix}\n",
    "= x_1x_1^T + x_2x_2^T + \\cdots + x_Mx_M^T\n",
    "= \\sum_{i=1}^M x_ix_i^T\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "이러한 행렬들을 가리켜 **로우-랭크 행렬(low-rank matrix)**이라고 한다. 로우-랭크 행렬은 나중에 특이분해(singular value decomposition)와 PCA(principal component analysis)에서 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c721a98b",
   "metadata": {},
   "source": [
    "## 벡터공간과 기저벡터\n",
    "벡터 N개가 서로 선형독립이면 이 벡터들을 선형조합하여 만들어지는 모든 벡터의 집합을 **벡터공간(vector space)** $\\boldsymbol V$ 라고 하고, 이 벡터공간의 차원을 N이라고 한다.<br>\n",
    "그리고 그 벡터들을 벡터공간의 **기저벡터(basis vector)** 라고 한다.<br>\n",
    "\n",
    "> 벡터공간의 차원은 벡터의 차원(길이)가 아니라 *기저벡터의 개수*로 정의된다!<br>\n",
    "\n",
    "$N$차원 벡터 $N$개 $x_1, x_2, \\cdots, x_N$이 선형독립인 경우에는 다음 정리가 성립한다.\n",
    "\n",
    "> **[정리] $N$개의 $N$차원 벡터 $x_1, x_2, \\cdots, x_N$이 선형독립이면 이를 선형조합하여 모든 $N$차원 벡터를 만들 수 있다.**\n",
    "\n",
    "다음과 같이 증명한다. 임의의 벡터 $x$가 있다고 하자. 기저벡터 $x_1, x_2, \\cdots, x_N$와 이 벡터 $x$를 열벡터로 사용하여 만든 행렬\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "X = \\left[ x_1, x_2, \\cdots, x_N, x \\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "는 크기가 $N \\times (N+1)$이므로 랭크값은 $N$보다 커질 수는 없다. 그런데 $N$개의 선형독립인 열벡터가 있기 때문에 랭크값은 $N$이고 풀랭크다. 따라서 어떠한 $N$차원 벡터를 생각하더라도 기저벡터의 조합으로 표현할 수 있다.\n",
    "\n",
    "#### 예제\n",
    "\n",
    "다음 벡터의 집합은 선형독립이므로 벡터공간의 기저벡터이다. \n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "x_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 0 \\end{bmatrix}, \\;\\;\n",
    "x_2 = \\begin{bmatrix} 2 \\\\ 1 \\\\ 0 \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "하지만 이 벡터공간은 3차원 벡터공간이 아니라 2차원 벡터공간이라고 한다. 예를 들어 이 벡터 $x_1, x_2$를 어떻게 선형조합해도 다음 벡터는 만들 수 없다.\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "c_1x_1 + c_2x_2 = \n",
    "\\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "벡터공간의 차원을 기저벡터의 차원과 다르게 정의하는 이유는 선형독립인 기저벡터를 선형조합했을 때 이렇게 만들어낼 수 없는 벡터들이 존재하기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b88eba",
   "metadata": {},
   "source": [
    "## 랭크와 역행렬\n",
    "정방행렬의 랭크와 역행렬 사이에는 다음과 같은 정리가 성립한다.\n",
    "\n",
    "> **[정리] 정방행렬이 풀랭크면 역행렬이 존재한다. 역도 성립한다. 즉, 정방행렬의 역행렬이 존재하면 풀랭크**다.\n",
    "\n",
    "따라서 다음 두 문장은 같은 뜻이다.\n",
    "\n",
    "> **정방행렬이 풀랭크다** $\\leftrightarrow$ **역행렬이 존재한다**\n",
    "\n",
    "다음과 같이 증명한다.\n",
    "\n",
    "(1) \n",
    "우선 왼쪽에서 오른쪽 방향 즉, 정방행렬이 풀랭크이면 역행렬이 존재한다는 것을 증명하자. **정방행렬이 풀랭크이면 선형독립이고 기저벡터가 되므로 어떠한 벡터에 대해서도 그 벡터를 만들 수 있는 선형조합을 생각할 수 있다.** 예를 들어 다음과 같은 벡터 $e_1, \\cdots, e_N$을 만들기 위한 조합 $c_1, \\cdots, c_N$도 있을 수 있다. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Xc_1 = e_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Xc_2 = e_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ \\vdots \\\\ 0 \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "이 식들을 모으면 다음과 같아진다.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "X \\begin{bmatrix} c_1 & c_2 & \\cdots & c_N \\end{bmatrix} = XC = I\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "정방행렬의 경우 $XC=I$이면 $CX=I$가 성립한다.(연습문제 2.4.4) 따라서\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "XC = CX = I\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "인 행렬 $C$가 존재한다. 이 행렬이 역행렬이다.\n",
    "\n",
    "(2) \n",
    "다음으로 오른쪽에서 왼쪽 방향 즉, 역행렬이 존재하면 풀랭크라는 것을 증명하자. \n",
    "역행렬이 존재하는 경우에 다음 식이 성립한다는 것을 증명하면 된다. \n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "Xc = 0 \\;\\; \\leftrightarrow \\;\\; c=0 \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "(i) 우선 역행렬이 존재하든 말든 $c = 0$이면 $Xc=0$는 당연하다. 따라서 오른쪽에서 왼쪽 방향은 증명된다. \n",
    "\n",
    "(ii) 다음으로 역행렬이 존재할 때 $Xc=0$이면 \n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "X^{-1}Xc = c = 0 \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "이므로 왼쪽에서 오른쪽 방향도 증명된다. 따라서 역행렬이 존재하면 풀랭크다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0dc5bb",
   "metadata": {},
   "source": [
    "## 벡터공간 투영\n",
    "$M$개의 $N$차원 기저벡터 $v_1, v_2, \\cdots, v_M$ 가 존재한다고 하자. $M$은 $N$보다 작다.\n",
    "이 때 모든 $N$차원 벡터 $x$에 대해 기저벡터 $v_1, v_2, \\cdots, v_M$를 선형조합하여 만든 벡터 $x^{\\Vert v}$와 원래 벡터 $x$의 차 $x - x^{\\Vert v}$가 모든 기저벡터에 직교하면 그 벡터 $x^{\\Vert v}$를 $v_1, v_2, \\cdots, v_M$ **벡터공간에 대한 투영벡터**라 하고 차이 벡터 $x - x^{\\Vert v} = x^{\\perp v}$를 **벡터공간에 대한 직교벡터**라 한다.\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "(x - x^{\\Vert V}) \\perp \\{ v_1, v_2, \\cdots, v_M \\} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "다음 그림은 $N=3,M=2$ 즉 3차원 벡터를 2차원 벡터공간에 투영하는 예를 보인 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6355b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='/Users/jihun/Documents/data_science/notebooks/ds_school/image/linalg5.png', width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c35a01",
   "metadata": {},
   "source": [
    "## 정규직교기저란?\n",
    "\n",
    "- 정규 직교 기저에서 우리는 이미 ‘기저’라는 단어를 접했습니다. 기저는 어떤 벡터 공간을 생성하는 벡터들이죠.\n",
    "- 정규 직교 기저에서 쓰이는 ‘정규’라는 단어는 벡터의 크기가 1임을 의미합니다.\n",
    "- 직교란 두 직선 또는 두 평면이 직각을 이루며 만나는 것입니다.\n",
    "\n",
    "정규직교기저란 ‘벡터의 크기가 1이고 서로 수직인 기저 벡터’\n",
    "\n",
    "벡터가 직교하면 그 내적값은 0이다.\n",
    "행렬 $\\boldsymbol A$가 직교행렬이면 $\\boldsymbol A \\boldsymbol A^T = \\boldsymbol I$ 라고 한다.<br>\n",
    "즉 $\\boldsymbol A^{-1} = \\boldsymbol A^T$인 행렬을 직교행렬이라고 한다.<br><br>\n",
    "\n",
    "직교행렬의 성질\n",
    "1. 직교 행렬의 전치 행렬은 직교 행렬입니다. \n",
    "2. 직교 행렬의 역행렬은 직교 행렬입니다. \n",
    "3. 직교 행렬의 곱은 직교 행렬입니다. \n",
    "4. 만약 A가 직교 행렬이라면 A의 행렬식은 1 또는 -1입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79817bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_context(\"notebook\", font_scale=1.5)\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.model_selection import cross_val_score , cross_validate\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8889c2",
   "metadata": {},
   "source": [
    "# <font color='blue'>[K-Means]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2921290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73290c2",
   "metadata": {},
   "source": [
    "# <font color='blue'>[Clustering]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2977d710",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaec705",
   "metadata": {},
   "source": [
    "# <font color='blue'>[DBSCAN]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a99105e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7dd233",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#[Ensemble-Classifier-직접-구현]\" data-toc-modified-id=\"[Ensemble-Classifier-직접-구현]-1\"><font color=\"blue\">[Ensemble Classifier 직접 구현]</font></a></span></li><li><span><a href=\"#[Bagging]\" data-toc-modified-id=\"[Bagging]-2\"><font color=\"blue\">[Bagging]</font></a></span></li><li><span><a href=\"#[RandomForest]\" data-toc-modified-id=\"[RandomForest]-3\"><font color=\"blue\">[RandomForest]</font></a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#RandomForestRegressor\" data-toc-modified-id=\"RandomForestRegressor-3.0.1\">RandomForestRegressor</a></span></li></ul></li></ul></li><li><span><a href=\"#[Boosting]\" data-toc-modified-id=\"[Boosting]-4\"><font color=\"blue\">[Boosting]</font></a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Boosting의-작동-원리\" data-toc-modified-id=\"Boosting의-작동-원리-4.0.0.1\">Boosting의 작동 원리</a></span></li><li><span><a href=\"#AdaBoost\" data-toc-modified-id=\"AdaBoost-4.0.0.2\">AdaBoost</a></span></li><li><span><a href=\"#GradientBoosting\" data-toc-modified-id=\"GradientBoosting-4.0.0.3\">GradientBoosting</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#[Stacking]\" data-toc-modified-id=\"[Stacking]-5\"><font color=\"blue\">[Stacking]</font></a></span></li><li><span><a href=\"#[SHAP-Value]\" data-toc-modified-id=\"[SHAP-Value]-6\"><font color=\"blue\">[SHAP Value]</font></a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a3d5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_context(\"notebook\", font_scale=1.5)\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.model_selection import cross_val_score , cross_validate\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbe1acc",
   "metadata": {},
   "source": [
    "# <font color='blue'>[Ensemble Classifier 직접 구현]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6189430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import _name_estimators\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "\n",
    "class MajorityVoteClassifier(BaseEstimator, ClassifierMixin):\n",
    "    # BaseEstimator와 ClassifierMixin이 포함하는 기능은\n",
    "        # get_params, set_params, score 메서드 등이다.\n",
    "    \"\"\"\n",
    "    매개변수\n",
    "    ------\n",
    "    classifiers: 배열 타입, 크기 = [n_classifiers]\n",
    "      앙상블에 사용할 분류기\n",
    "    \n",
    "    vote: str, {'classlabel', 'probability'}\n",
    "      'classlabel' 이면 다수인 클래스 레이블로 예측 반환\n",
    "      'probability' 이면 예측을 확률값으로 반환\n",
    "    \n",
    "    weights: array-like, 크기 = [n_classifiers]\n",
    "      'int' 또는 'float' 값의 리스트가 주어지면 분류기가 이 중요도로 가중치된다.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, classifiers, vote='classlabel', weights=None):\n",
    "        self.classifiers = classifiers\n",
    "        self.named_classifiers = {key: value for\n",
    "                                  key, value in\n",
    "                                  _name_estimators(classifiers)}\n",
    "        self.vote = vote\n",
    "        self.weights = weights\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        if self.vote not in ('probability', 'classlabel'):\n",
    "            raise ValueError(\"vote는 'probability' 또는 'classlabel'이어야 합니다.\"\\\n",
    "                             \"; (vote=%r)이 입력되었습니다.\"\n",
    "                             % self.vote)\n",
    "            \n",
    "        if self.weights and len(self.weights) != len(self.classifiers):\n",
    "            raise ValueError(\"분류기와 가중치의 개수는 같아야 합니다.\"\n",
    "                             \"; 가중치 %d개, 분류기 %d 개\"\n",
    "                             % (len(self.weights), len(self.classifiers)))\n",
    "            \n",
    "        # self.predict 메서드에서 np.argmax를 호출할 때\n",
    "            # 클래스 레이블이 0부터 시작되어야 하므로 LabelEncoder 사용\n",
    "        self.lablenc_ = LabelEncoder()\n",
    "        self.lablenc_.fit(y)\n",
    "        \n",
    "        self.classes_ = self.lablenc_.classes_\n",
    "        self.classifiers_ = []\n",
    "        for clf in self.classifiers:\n",
    "            fitted_clf = clone(clf).fit(X,\n",
    "                                        self.lablenc_.transform(y))\n",
    "            self.classifiers_.append(fitted_clf)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.vote == 'probability':\n",
    "            maj_vote = np.argmax(self.predict_proba(X), axis=1)\n",
    "            \n",
    "        else:\n",
    "            predictions = np.asarray([clf.predict(X)\n",
    "                                      for clf in\n",
    "                                      self.classifiers_]).T\n",
    "            \n",
    "            maj_vote = np.apply_along_axis(\n",
    "                            lambda x:\n",
    "                            np.argmax(np.bincount(x,\n",
    "                                      weights=self.weights)),\n",
    "                            axis=1,\n",
    "                            arr=predictions)\n",
    "        maj_vote = self.lablenc_.inverse_transform(maj_vote)\n",
    "        return maj_vote\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        probas = np.asarray([clf.predict_proba(X)\n",
    "                             for clf in self.classifiers_])\n",
    "        avg_proba = np.average(probas, axis=0, weights=self.weights)\n",
    "        return avg_proba\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        if not deep:\n",
    "            return super(MajorityVoteClassifier,\n",
    "                         self).get_params(deep=False)\n",
    "        else:\n",
    "            out = self.named_classifiers.copy()\n",
    "            for name, step in\\\n",
    "                    six.iteritems(self.named_classifiers):\n",
    "                for key, value in six.iteritems(\n",
    "                        step.get_params(deep=True)):\n",
    "                    out['%s__%s' % (name, key)] = value\n",
    "            return out\n",
    "\n",
    "# iris data 에 대한 개별 분류기(DT, KNN, LR) 성능 비교\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[50:, [1,2]], iris.target[50:]\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test =\\\n",
    "        train_test_split(X, y, test_size=0.5,\n",
    "                         random_state=1, stratify=y)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf1 = LogisticRegression(penalty='l2', C=0.001, random_state=0)\n",
    "clf2 = DecisionTreeClassifier(max_depth=1, criterion='entropy',\n",
    "                              random_state=0)\n",
    "clf3 = KNeighborsClassifier(n_neighbors=1, p=2, metric='minkowski')\n",
    "\n",
    "pipe1 = Pipeline([['sc', StandardScaler()],\n",
    "                  ['clf', clf1]])\n",
    "pipe3 = Pipeline([['sc', StandardScaler()],\n",
    "                  ['clf', clf3]])\n",
    "clf_labels = ['Logistic regression', 'Decision tree', 'KNN']\n",
    "\n",
    "print('10-겹 교차 검증:\\n')\n",
    "for clf, label in zip([pipe1, clf2, pipe3], clf_labels):\n",
    "    scores = cross_val_score(estimator=clf, X=X_train,\n",
    "                             y=y_train, cv=10, scoring='roc_auc')\n",
    "    print(\"ROC AUC: %0.2f (+/- %0.2f) [%s]\"\n",
    "          % (scores.mean(), scores.std(), label))\n",
    "\n",
    "# 직접 만든 분류기 실행\n",
    "mv_clf = MajorityVoteClassifier(\n",
    "                classifiers=[pipe1, clf2, pipe3])\n",
    "clf_labels += ['Majority Voting']\n",
    "all_clf = [pipe1, clf2, pipe3, mv_clf]\n",
    "for clf, label in zip(all_clf, clf_labels):\n",
    "    scores = cross_val_score(estimator=clf, X=X_train,\n",
    "                             y=y_train, cv=10, scoring='roc_auc')\n",
    "    print(\"ROC AUC: %0.2f (+/- %0.2f) [%s]\"\n",
    "          % (scores.mean(), scores.std(), label))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9e9b79",
   "metadata": {},
   "source": [
    "# <font color='blue'>[Bagging]</font>\n",
    "- 배깅은 위에서 구현한 MajorityVoteClassifier와 매우 밀접하게 작동하는 기법인데, 차이점은 개별 분류기를 동일한 훈련 데이터셋으로 학습하는 것이 아니라 원본 훈련 데이터셋에서 부트스트랩 샘플(중복을 허용한 랜덤 샘플)을 뽑아서 사용한다. (=> 그래서 배깅을 'bootstrap aggregating'이라고도 한다)\n",
    "    - 랜덤 포레스트는 개별 결정 트리를 학습할 때 '랜덤하게 특성의 부분 집합을 선택하는' 배깅의 특별한 경우다.\n",
    "    - 각각의 부트스트랩 샘플은 대략 63.2%의 unique values로 구성되게 된다.\n",
    "    - \"Bagging can improve the accuracy of unstable models that tend to overfit.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1ae856",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76321a6e",
   "metadata": {},
   "source": [
    "# <font color='blue'>[RandomForest]</font>\n",
    "\n",
    "- 여러 개의 (깊은) 결정 트리의 평균을 내는 것이다.\n",
    "- 개개의 트리는 분산이 높은 문제가 있지만, 앙상블 기법을 사용하면 일반화 성능을 높이고 과대적합의 위험을 줄여준다.<br><br>\n",
    "\n",
    "1. n개의 랜덤한 부트스트랩 샘플을 뽑는다. (훈련 데이터셋에서 중복을 허용하면서 랜덤하게 n개의 샘플을 선택한다)\n",
    "2. 부트스트랩 샘플에서 결정 트리를 학습한다. 각 노드에서 다음과 같이 한다.\n",
    "    - 중복을 허용하지 않고 랜덤하게 d개의 특성을 선택한다.\n",
    "    - 정보 이득과 같은 목적 함수를 기준으로 최선의 분할을 만드는 특성을 사용해서 노드를 분할한다.\n",
    "3. 단계 1~2를 k번 반복한다.\n",
    "4. 각 트리의 예측을 모아 majority voting으로 클래스 레이블을 할당한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3ed639",
   "metadata": {},
   "source": [
    "> 랜덤 포레스트는 결정 트리만큼 해석이 쉽진 않지만 하이퍼파라미터 튜닝에 많은 노력을 기울이지 않아도 된다는 장점이 있다. (일반적으로 가지치기를 할 필요가 없다.) 신경써야 하는 파라미터는 단계 3번의 랜덤 포레스트가 만들 트리 개수다. 보통 트리 개수가 많을수록 계산 비용이 증가하고 분류기의 성능이 좋아진다.<br>\n",
    "\n",
    "- Random Forest에서 최적화할 만한 다른 파라미터는 부트스트랩 샘플 크기 $n$이랑, 각 분할에서 무작위로 선택할 특성 개수 $d$이다.\n",
    "    - 샘플 크기가 작아지면 => 개별 트리의 다양성 증가하고 => 과대적합 영향이 줄어들지만 랜덤 포레스트의 전체적인 성능도 줄어든다.\n",
    "    - 샘플 크기가 늘어나면 과대적합 가능성이 늘어난다.\n",
    "    - 보통은 부트스트랩 샘플 크기를 원본 훈련 데이터셋의 샘플 개수와 동일하게 한다.\n",
    "    - 분할에 사용할 특성 개수 $d$는 훈련 데이터셋에 있는 전체 개수보다 작게 지정하곤 한다. 훈련 데이터셋 특성 개수의 제곱근을 취하기도 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b611ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 실습\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(criterion='gini',\n",
    "                               n_estimators=25,\n",
    "                               random_state=1,\n",
    "                               n_jobs=2)\n",
    "    # n_estimators는 생성할 결정 트리 개수; n_jobs는 컴퓨터의 멀티 코어를 몇 개나 사용할지 지정\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "plot_decision_regions(X_combined, y_combined,\n",
    "                     classifier=forest, test_idx=range(105,150))\n",
    "\n",
    "plt.xlabel('petal length [cm]')\n",
    "plt.ylabel('petal width [cm]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad40a1c5",
   "metadata": {},
   "source": [
    "### RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c697f69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "df = pd.read_csv('../pos_bigdata/3data/체질검사.csv')\n",
    "\n",
    "df_x = df.drop('FAT', axis=1)\n",
    "df_y = df['FAT']\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(df_x, df_x, test_size=0.3)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1, refit=True)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "scores_df = pd.DataFrame(rf_random.cv_results_)\n",
    "scores_df[['params', 'mean_test_score', 'rank_test_score']].sort_values(by=['rank_test_score'])\n",
    "\n",
    "export_graphviz(rf1.estimator_[0],\n",
    "                out_file='rfr_0.dot',\n",
    "                feature_names=X_train.columns,\n",
    "                impurity=True, filled=True)\n",
    "with open('rfr_0.dot') as f:\n",
    "    dot_graph = f.read()\n",
    "display(graphviz.Source(dot.graph))\n",
    "\n",
    "rf1.get_params\n",
    "\n",
    "rf_random.best_estimator_\n",
    "\n",
    "rf1 = rf_random.best_estimator_\n",
    "\n",
    "df_importance = pd.DataFrame()\n",
    "df_importance['Feature'] = X_train.columns\n",
    "df_importance['Importance'] = rf1.feature_importances_\n",
    "\n",
    "df_importance.sort_values(by=['Importance'], ascending=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aefdfe4",
   "metadata": {},
   "source": [
    "# <font color='blue'>[Boosting]</font>\n",
    "\n",
    "> **Boosting이란.. it means boosting 'weak learners' to 'strong learners.'** => 부스팅은 iterative process이다: *각각의 iteration마다 the training set is reweighted based on errors of weak learners.*\n",
    "> - Adaptive Boosting과 Gradient Boosting의 차이는:\n",
    "    - on how the weights are updated;\n",
    "    - and on how the classifiers are combined.\n",
    "\n",
    "#### Boosting의 작동 원리\n",
    "1. 훈련 데이터셋 $D$에서 중복을 허용하지 않고 랜덤한 부분 집합 $d_1$를 뽑아 약한 학습기 $C_1$(예를 들어 깊이가 1인 결정트리)를 훈련한다.\n",
    "2. 훈련 데이터셋에서 중복을 허용하지 않고 두 번째 랜덤한 훈련 부분 집합 $d_2$를 뽑고 이전에 잘못 분류된 샘플의 50%를 더해서 약한 학습기 $C_2$를 훈련한다.\n",
    "3. 훈련 데이터셋 $D$에서 $C_1$과 $C_2$에서 잘못 분류한 훈련 샘플 $d_3$를 찾아 세 번째 약한 학습기인 $C_3$를 훈련한다.\n",
    "4. 약한 학습기 $C_1$, $C_2$, $C_3$를 다수결 투표로 연결한다.\n",
    "\n",
    "#### AdaBoost\n",
    "training set의 가중치를 weak learner의 에러에 기반해 계속 업데이트하고, 모든 weak learner의 예측을 majority voting으로 합친다.\n",
    "\n",
    "\n",
    "#### GradientBoosting\n",
    "In contrast to AdaBoost, the output of gradient boosting is an additive models of multiple weak learners. (rather than the majority voting of the ensemble of models)\n",
    "- GB를 가동하면, 그 첫 번째 모델은 예를 들어 깊이가 1인 결정트리이고, 마지막 결과물은 깊이가 훨씬 더 깊은 결정트리가 되는 것이다.\n",
    "- GB의 작동 방식\n",
    "    1. Construct a base tree (just the root node).\n",
    "    2. Build next tree based on errors of the previous tree.\n",
    "    3. Combine tree from step 1 with trees from step 2. Go to step 2.\n",
    "    \n",
    "\n",
    "- AdaBoost와 달리 이전의 약한 학습기가 만든 '잔차오차(residual error)를 학습하는 새로운 학습기를 추가한다.\n",
    "\n",
    "- 장점\n",
    "    - 하이퍼파라미터 조정이 쉽고 성능이 뛰어남\n",
    "    - 데이터 scale 변환이 불필요\n",
    "- 단점\n",
    "    - 개별 트리 분석이 어렵고, 트리 개수가 늘어날수록 과대적합되는 경향이 있음\n",
    "    - 차원이 크고 희소한 데이터에서는 성능이 미흡\n",
    "    - 모델 성능이 하이퍼파라미터, 특히 학습률에 민감함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644e07e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41307064",
   "metadata": {},
   "source": [
    "# <font color='blue'>[Stacking]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be784e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651d3330",
   "metadata": {},
   "source": [
    "# <font color='blue'>[SHAP Value]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d33800",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a46e57",
   "metadata": {
    "id": "19a46e57"
   },
   "source": [
    "# Linear Regression<a id='section1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19895551",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 381,
     "status": "ok",
     "timestamp": 1627482688283,
     "user": {
      "displayName": "Jihun Kang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVHrn8kkf46QnIfPEzPFTJv7X-WGi7V6rMxh-6ug=s64",
      "userId": "17468962336362829758"
     },
     "user_tz": -540
    },
    "id": "19895551",
    "outputId": "2160f027-8954-41d7-dbcb-a4e838212404",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "boston = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data/Fastcampus/Boston_house.csv')\n",
    "boston.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8242f0",
   "metadata": {
    "id": "4e8242f0"
   },
   "source": [
    "**타겟 데이터**<br>\n",
    "1978 보스턴 주택 가격\n",
    "506개 타운의 주택 가격 중앙값 (단위 1,000 달러)\n",
    "\n",
    "**특징 데이터**\n",
    "- CRIM: 범죄율\n",
    "- INDUS: 비소매상업지역 면적 비율\n",
    "- NOX: 일산화질소 농도\n",
    "- RM: 주택당 방 수\n",
    "- LSTAT: 인구 중 하위 계층 비율\n",
    "- B: 인구 중 흑인 비율\n",
    "- PTRATIO: 학생/교사 비율\n",
    "- ZN: 25,000 평방피트를 초과 거주지역 비율\n",
    "- CHAS: 찰스강의 경계에 위치한 경우는 1, 아니면 0\n",
    "- AGE: 1940년 이전에 건축된 주택의 비율\n",
    "- RAD: 방사형 고속도로까지의 거리\n",
    "- DIS: 직업센터의 거리\n",
    "- TAX: 재산세율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02adb1ca",
   "metadata": {
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1627482718891,
     "user": {
      "displayName": "Jihun Kang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVHrn8kkf46QnIfPEzPFTJv7X-WGi7V6rMxh-6ug=s64",
      "userId": "17468962336362829758"
     },
     "user_tz": -540
    },
    "id": "02adb1ca"
   },
   "outputs": [],
   "source": [
    "# target 제외한 데이터만 뽑기\n",
    "boston_data = boston.drop(['Target'],axis=1)\n",
    "\n",
    "## 변수 설정 target/crim/rm/lstat\n",
    "target = boston[['Target']]\n",
    "crim = boston[['CRIM']]\n",
    "rm = boston[['RM']]\n",
    "lstat = boston[['LSTAT']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c291e8",
   "metadata": {
    "id": "86c291e8"
   },
   "source": [
    "### target ~ crim linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1ce952",
   "metadata": {
    "executionInfo": {
     "elapsed": 250,
     "status": "ok",
     "timestamp": 1627482722278,
     "user": {
      "displayName": "Jihun Kang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVHrn8kkf46QnIfPEzPFTJv7X-WGi7V6rMxh-6ug=s64",
      "userId": "17468962336362829758"
     },
     "user_tz": -540
    },
    "id": "ee1ce952"
   },
   "outputs": [],
   "source": [
    "# crim변수에 상수항추가하기 \n",
    "# 상수항을 추가하지 않으면 y절편값이 계산이 안 된다.\n",
    "    # x변수값 + 상수값 = y값이 되어야 하는 거니까\n",
    "    # crim값 + constant = target 형식으로 fitting하는 거다.\n",
    "crim1 = sm.add_constant(crim, has_constant=\"add\")\n",
    "\n",
    "# sm.OLS 적합시키기\n",
    "model1 = sm.OLS(target, crim1)\n",
    "fitted_model1 = model1.fit()\n",
    "\n",
    "# summary함수통해 결과출력\n",
    "fitted_model1.summary()\n",
    "\n",
    "# y의 총 변동성 중에 x(범죄율)이 설명하는 비율은 약 0.151이다 (R^2)\n",
    "# 범죄율의 회귀계수는 -0.4152 (범죄율이 1 증가할 때, y가 약 0.4 감소한다)\n",
    "# p value는 0에 가까우며, 매우 유의하다.\n",
    "\n",
    "## 회귀 계수 출력\n",
    "fitted_model1.params\n",
    "\n",
    "#회귀 계수 x 데이터(X)\n",
    "\n",
    "# np.dot 즉 벡터 내적으로 계산한다.\n",
    "    # crim1 np.dot fitted_model1.params 는...\n",
    "    # (상수1, x변수)의 행벡터 * (y절편, coefficient)의 열벡터 =   y절편 + (x변수 * coefficient)\n",
    "np.dot(crim1,fitted_model1.params)[:5]\n",
    "\n",
    "## predict함수를 통해 yhat구하기\n",
    "pred1 = fitted_model1.predict(crim1)\n",
    "pred1[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb0d088",
   "metadata": {
    "id": "0fb0d088"
   },
   "source": [
    "### Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1177ddc",
   "metadata": {
    "id": "c1177ddc",
    "outputId": "44c84ee3-4adf-4e0c-9a3a-865f32a04646"
   },
   "outputs": [],
   "source": [
    "## bostan data에서 crim, rm, lstat 변수만 뽑아오기 \n",
    "x_data = boston[['CRIM', 'RM', 'LSTAT']]\n",
    "\n",
    "# 상수항 추가\n",
    "x_data1 = sm.add_constant(x_data, has_constant='add')\n",
    "# 회귀모델 적합\n",
    "multi_model = sm.OLS(target, x_data1)\n",
    "fitted_multi_model = multi_model.fit()\n",
    "\n",
    "# 다중선형회귀모델의 회귀 계수\n",
    "print(fitted_multi_model.params, '\\n')\n",
    "\n",
    "# 회귀계수(베타)를 행렬연산으로 구하기.\n",
    "    # SSE를 최소화하는 beta를 구하는 방법은 => SSE의 도함수가 0이 되는 값을 구하는 것!\n",
    "        # (X'X)^-1X'y\n",
    "from numpy import linalg\n",
    "ba = linalg.inv(np.dot(x_data1.T, x_data1))  # (X'X)^-1\n",
    "beta = np.dot(np.dot(ba, x_data1.T), target)\n",
    "print(beta, '\\n')\n",
    "\n",
    "# y_hat 구하는 법: np.dot(x_data1, fitted_multi_model.params) & pred함수\n",
    "pred2 = fitted_multi_model.predict(x_data1)\n",
    "print(pred2[:5])\n",
    "    # fitted_multi_model 은 적합도 값(베타)를 가지고 있는 함수다.\n",
    "    # 여기다가 x_data1를 넣어서, y값을 예측하라는 말이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5925e27b",
   "metadata": {
    "id": "5925e27b"
   },
   "source": [
    "### <font color='blue'>fitted_model.summary() 를 통한 model performance 확인</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72362c8",
   "metadata": {
    "id": "f72362c8",
    "outputId": "00e00d21-8594-48b6-c914-c08c094a7da0"
   },
   "outputs": [],
   "source": [
    "# summary - R^2, AIC, BIC 등 모델의 성능지표 확인 가능\n",
    "    # 변수의 유의수준(p-value)도 확인 가능\n",
    "fitted_multi_model.summary()\n",
    "\n",
    "# 단순선형회귀분석을 할 때의 R^2 값 3개를 더한 것보다\n",
    "# 다중선형회귀분석의 R^2 값이 작다.\n",
    "    # 그 이유는.. \"3개의 x변수가 각각 설명하는 y값 변동의 비율이 겹친다.\"\n",
    "    # 그래서 각각 변수의 coefficient값도 변화한다.\n",
    "    # 이 현상이 \"다중공선성\"이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318a1229",
   "metadata": {
    "id": "318a1229"
   },
   "source": [
    "### <font color='blue'>MSE를 통한 model performance 확인</font>\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = data\n",
    "y = target\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=1)\n",
    "print(train_x.shape, test_x.shape, train_y.shape, test_y.shape)\n",
    "\n",
    "\n",
    ">>> modelling and fitting\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(test_y, fit_1.predict(test_x))\n",
    "mean_squared_error(test_y, fit_2.predict(test_x))\n",
    "mean_squared_error(test_y, fit_3.predict(test_x))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4236b1",
   "metadata": {
    "id": "cf4236b1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36c6609e",
   "metadata": {
    "id": "36c6609e"
   },
   "source": [
    "# Multicollinearity and its Solutions\n",
    "\n",
    "> **VIF를 통해 변수 제거를 하든, <br>\n",
    "> Feature selection을 하든, <br>\n",
    "> Regularization을 하든, <br>\n",
    "> <font color='red'>결국은 \"performance metrics\"가 좋아져야 하고, 모델이 \"adequate\"해야 한다.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd84430d",
   "metadata": {
    "id": "bd84430d",
    "outputId": "a38918f9-b711-4417-ef1a-127cfa4c81b2"
   },
   "outputs": [],
   "source": [
    "x_data2 = boston[['CRIM', 'RM', 'LSTAT', 'B', 'TAX', 'AGE', 'ZN', 'NOX', 'INDUS']]\n",
    "x_data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf125a1",
   "metadata": {
    "id": "bcf125a1"
   },
   "source": [
    "### 다중공선성 확인 - 변수간 corr와 산점도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d0c0e6",
   "metadata": {
    "id": "29d0c0e6",
    "outputId": "ee8e1e45-844e-4e4c-ebbd-969e95dfc613"
   },
   "outputs": [],
   "source": [
    "# 행렬로 베타를 구하는 방법은 (X'X)^-1X'y 인데,\n",
    "    # 다중공선성이 심해지면 이론적으로 (X'X)^-1을 구할 수 없다.\n",
    "x_data2.corr()\n",
    "\n",
    "## 상관행렬 시각화 해서 보기 \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "cmap = sns.light_palette(\"darkgray\", as_cmap=True)\n",
    "sns.heatmap(x_data2.corr(), annot=True, cmap=cmap)\n",
    "plt.show()\n",
    "\n",
    "## 변수별 산점도 시각화\n",
    "sns.pairplot(x_data2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af172e39",
   "metadata": {
    "id": "af172e39"
   },
   "source": [
    "### 다중공선성 확인 및 변수 제거 - VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7115beba",
   "metadata": {
    "id": "7115beba",
    "outputId": "f6d3ad06-9c67-4903-c0e7-6f003eade2f7"
   },
   "outputs": [],
   "source": [
    "# VIF를 통한 다중공선성 확인\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(\n",
    "    x_data2.values, i) for i in range(x_data2.shape[1])]\n",
    "vif[\"features\"] = x_data2.columns\n",
    "vif\n",
    "\n",
    "## nox 변수 제거후(X_data3) VIF 확인 \n",
    "vif = pd.DataFrame()\n",
    "x_data3= x_data2.drop('NOX',axis=1)\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(\n",
    "    x_data3.values, i) for i in range(x_data3.shape[1])]\n",
    "vif[\"features\"] = x_data3.columns\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fe335e",
   "metadata": {
    "id": "09fe335e"
   },
   "source": [
    "> VIF 활용해서 변수 제거했으면, <br>\n",
    "> **1) 이제 MSE 등의 지표를 활용해 모델의 성능이 높아졌는지 확인하고** <br>\n",
    "> **2) 잔차분석(독립성,정규성,등분산성)을 통해 모델이 적합한지 확인해야 한다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3373b1b3",
   "metadata": {
    "id": "3373b1b3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0a88410",
   "metadata": {
    "id": "e0a88410"
   },
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdc345c",
   "metadata": {
    "id": "ecdc345c",
    "outputId": "485d2d62-e7d8-447a-d9ae-0cd0f4c77e71"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# read data\n",
    "corolla = pd.read_csv('/Users/jihun/Desktop/Fastcampus/Data/ToyotaCorolla.csv')\n",
    "\n",
    "# one-hot-encoding 'Fuel_Type'\n",
    "mlr_data = pd.get_dummies(corolla, columns=['Fuel_Type'])\n",
    "# rename column\n",
    "mlr_data.rename(columns = {'Fuel_Type_CNG': 'CNG', 'Fuel_Type_Diesel': 'Diesel', 'Fuel_Type_Petrol': 'Petrol'}, inplace=True)\n",
    "# dropping 'id' and 'model'\n",
    "mlr_data.drop(['Id', 'Model'], axis=1, inplace=True)\n",
    "mlr_data.head()\n",
    "\n",
    "# bias 추가\n",
    "mlr_data = sm.add_constant(mlr_data, has_constant='add')\n",
    "\n",
    "# train test split\n",
    "feature_columns = list(mlr_data.columns.difference(['Price']))\n",
    "\n",
    "X = mlr_data[feature_columns]\n",
    "y = mlr_data.Price   # same as mlr_data['Price']\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, train_size=0.7, test_size=0.3)\n",
    "print(train_x.shape, test_x.shape, train_y.shape, test_y.shape)\n",
    "\n",
    "# modelling and fitting\n",
    "full_model = sm.OLS(train_y, train_x)\n",
    "fitted_full_model = full_model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf75ba3c",
   "metadata": {
    "id": "cf75ba3c"
   },
   "source": [
    "### best AIC를 갖는 feature_combo 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f000426e",
   "metadata": {
    "id": "f000426e",
    "outputId": "a2036f40-2470-458c-a545-447f3e2df1e5"
   },
   "outputs": [],
   "source": [
    "# 1) X, y, feature를 받았을 때 AIC를 return하는 함수 정의\n",
    "def processSubset(X, y, feature_set):\n",
    "    model = sm.OLS(y, X[list(feature_set)]) # modelling\n",
    "    regr = model.fit()                      # fitting\n",
    "    AIC = regr.aic                          # model's aic\n",
    "    return {\"model\": regr, \"AIC\": AIC}\n",
    "    \n",
    "print(processSubset(X=train_x, y=train_y, feature_set=feature_columns[0:5]))\n",
    "print(processSubset(X=train_x, y=train_y, feature_set=feature_columns))\n",
    "\n",
    "\n",
    "# 2) 각각의 feature_columns 조합 중 가장 낮은 AIC를 가지는 모델 선택 및 저장하는 함수 정의\n",
    "\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "def getBest(X, y, k):\n",
    "    tic = time.time() # 시작시간\n",
    "    results = []  # 결과 저장 공간\n",
    "    for combo in itertools.combinations(X.columns.difference(['const']), k):\n",
    "        combo = (list(combo) + ['const'])  # 상수열은 빼고 combo 만든 다음에, 다시 상수항을 콤보에 추가해준다. 왜냐면 상수항은 모든 콤보에 포함되어야 하기 때문\n",
    "        results.append(processSubset(X, y, feature_set=combo)) # 콤보 리스트를 feature set로 갖는 모델의 AIC값을 저장한다.\n",
    "    models = pd.DataFrame(results) # 데이터 프레임으로 변환\n",
    "    best_model = models.loc[models['AIC'].argmin()]\n",
    "        # models['AIC'].argmin() 은 AIC가 가장 낮은 행의 index를 가짐\n",
    "        # models.loc[위의 인덱스] 를 하면 best 행만 return\n",
    "    toc = time.time()   # 종료시간\n",
    "    print(\"Processed \", models.shape[0], \"models on\", k, \"predictors in\", (toc - tic), \"seconds.\")\n",
    "    return best_model\n",
    "\n",
    "print(getBest(X=train_x, y=train_y, k=2))\n",
    "\n",
    "# 3) 변수 선택에 따른 학습시간 출력 & 모델 저장\n",
    "models = pd.DataFrame(columns=['AIC', 'model'])\n",
    "tic = time.time()\n",
    "for i in range(1,5):\n",
    "    models.loc[i] = getBest(X=train_x, y=train_y, k=i)\n",
    "toc = time.time()\n",
    "print(\"Total elapsed time:\", (toc - tic), \"seconds.\")\n",
    "\n",
    "models\n",
    "\n",
    "# summary로 모델 성능지표 확인\n",
    "models.loc[4, 'model'].summary()\n",
    "\n",
    "# example... Plot the result\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.rcParams.update({'font.size': 18, 'lines.markersize': 10})\n",
    "\n",
    "## Mallow Cp\n",
    "plt.subplot(2, 2, 1)\n",
    "Cp= models.apply(lambda row: (row[1].params.shape[0]+(row[1].mse_total-\n",
    "                               fitted_full_model.mse_total)*(train_x.shape[0]-\n",
    "                                row[1].params.shape[0])/fitted_full_model.mse_total\n",
    "                               ), axis=1)\n",
    "plt.plot(Cp)\n",
    "plt.plot(Cp.argmin(), Cp.min(), \"or\")\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('Cp')\n",
    "\n",
    "# adj-rsquared plot\n",
    "# adj-rsquared = Explained variation / Total variation\n",
    "adj_rsquared = models.apply(lambda row: row[1].rsquared_adj, axis=1)\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(adj_rsquared)\n",
    "plt.plot(adj_rsquared.argmax(), adj_rsquared.max(), \"or\")\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('adjusted rsquared')\n",
    "\n",
    "# aic\n",
    "aic = models.apply(lambda row: row[1].aic, axis=1)\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(aic)\n",
    "plt.plot(aic.argmin(), aic.min(), \"or\")\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('AIC')\n",
    "\n",
    "# bic\n",
    "bic = models.apply(lambda row: row[1].bic, axis=1)\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(bic)\n",
    "plt.plot(bic.argmin(), bic.min(), \"or\")\n",
    "plt.xlabel(' # Predictors')\n",
    "plt.ylabel('BIC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318230bc",
   "metadata": {
    "id": "318230bc"
   },
   "source": [
    "### 전진선택법 (forward selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f039be",
   "metadata": {
    "id": "c4f039be"
   },
   "outputs": [],
   "source": [
    "# 1) 전진선택법을 위한 함수 정의\n",
    "\n",
    "def forward(X, y, predictors):\n",
    "    # 데이터 변수들이 미리 정의된 predictors에 있는지 없는지 확인 및 분류\n",
    "    remaining_predictors = [p for p in X.columns.difference(['const']) if p not in predictors]\n",
    "        # 상수항을 뺀 X의 모든 칼럼들 중 'predictors'라는 리스트에\n",
    "            # 없는 칼럼들을 모두 'remainig_predictors'라는 리스트에 넣는다.\n",
    "    tic = time.time()\n",
    "    results = []\n",
    "    for p in remaining_predictors:\n",
    "        results.append(processSubset(X, y, feature_set=predictors+[p]+['const']))\n",
    "        # remaining_predictors에 있는 값들을 하나씩\n",
    "        # 기존 predictors 리스트에 추가하고 상수항도 추가해서 AIC 값을 내고,\n",
    "        # 그 값을 results 리스트에 추가한다.\n",
    "    \n",
    "    # df로 변환\n",
    "    models = pd.DataFrame(results)\n",
    "    \n",
    "    # AIC가 가장 낮은 것을 선택\n",
    "    best_model = models.loc[models['AIC'].argmin()]\n",
    "    toc = time.time()\n",
    "    \n",
    "    print(\"Processed \", models.shape[0], \"models on\", len(predictors)+1, \"predictors in\", (toc-tic), \"seconds.\")\n",
    "    print(\"Selected predictors: \", best_model['model'].model.exog_names, '\\nAIC: ', best_model[0])\n",
    "    return best_model\n",
    "\n",
    "# 2) 전진선택법 모델 정의\n",
    "\n",
    "def forward_model(X, y):\n",
    "    Fmodels = pd.DataFrame(columns=['AIC', 'model'])\n",
    "    tic = time.time()\n",
    "    \n",
    "    # 변수가 10개면 10개가 될때까지 forward하기.\n",
    "        # 하지만 중간에 변수가 추가되면서 AIC가 낮아지면 break\n",
    "    predictors = []\n",
    "    for i in range(1, len(X.columns.difference(['const'])) + 1):\n",
    "        Forward_result = forward(X, y, predictors=predictors)\n",
    "        # Forward_result에 저장된 값은 'predictors'에 대해 best AIC model\n",
    "        if i > 1:\n",
    "            if Forward_result['AIC'] > Fmodel_before:\n",
    "                break  # 이번 AIC값이 이전 값보다 높으면 break\n",
    "                \n",
    "        Fmodels.loc[i] = Forward_result # Fmodels 행마다 'AIC','Model 저장'\n",
    "        predictors = Fmodels.loc[i][\"model\"].model.exog_names # forward_result의 predictors(feature_columns)를 저장.\n",
    "        Fmodel_before = Fmodels.loc[i][\"AIC\"]  # before에는 AIC값 저장\n",
    "        predictors = [k for k in predictors if k != 'const']\n",
    "            # predictors 리스트 내의 'const'가 아닌 모든 feature_columns를 저장\n",
    "    toc = time.time()\n",
    "    \n",
    "    print(\"Total elapsed time:\", (toc-tic), \"seconds.\")\n",
    "    \n",
    "    return(Fmodels['model'][len(Fmodels['model'])])\n",
    "        # Fmodels의 'model' column에서 가장 마지막 인덱스를 return\n",
    "\n",
    "Forward_best_model = forward_model(X=train_x, y= train_y)\n",
    "\n",
    "Forward_best_model.aic\n",
    "\n",
    "Forward_best_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02276be1",
   "metadata": {
    "id": "02276be1"
   },
   "source": [
    "### 후진소거법(Backward Elimination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4c823e",
   "metadata": {
    "id": "de4c823e"
   },
   "outputs": [],
   "source": [
    "# 1) 후진소거법 함수 정의\n",
    "\n",
    "def backward(X, y, predictors):\n",
    "    tic = time.time()\n",
    "    results = []\n",
    "    \n",
    "    for combo in itertools.combinations(predictors, len(predictors) - 1):\n",
    "        results.append(processSubset(X, y, feature_set=list(combo)+['const']))\n",
    "        # processSubset은.. AIC값을 return하는 함수\n",
    "    # df 변환\n",
    "    models = pd.DataFrame(results)\n",
    "    # 가장 낮은 AIC의 모델을 선택\n",
    "    best_model = models.loc[models['AIC'].argmin()]\n",
    "    toc = time.time()\n",
    "    \n",
    "    print(\"Processed \", models.shape[0], \"models on\", len(predictors) - 1, \"predictors in\",\n",
    "          (toc - tic))\n",
    "    print('Selected predictors:',best_model['model'].model.exog_names,' AIC:',best_model[0] )\n",
    "    return best_model\n",
    "\n",
    "# 2) 후진소거법 모델 정의\n",
    "\n",
    "def backward_model(X, y):\n",
    "    Bmodels = pd.DataFrame(columns=['AIC', 'model'], index = range(1, len(X.columns)))\n",
    "    tic = time.time()\n",
    "    predictors = X.columns.difference(['const'])\n",
    "    Bmodel_before = processSubset(X, y, predictors)['AIC']\n",
    "    # 모든 features를 넣었을 때의 초기 AIC값 = Bmodel_before\n",
    "    \n",
    "    while (len(predictors) > 1):\n",
    "        Backward_result = backward(X, y, predictors=predictors)\n",
    "        if Backward_result['AIC'] > Bmodel_before:\n",
    "            break\n",
    "        Bmodels.loc[len(predictors) - 1] = Backward_result\n",
    "        predictors = Bmodels.loc[len(predictors) - 1]['model'].model.exog_names\n",
    "        Bmodel_before = Backward_result['AIC']\n",
    "        predictors = [k for k in predictors if k != 'const']\n",
    "            \n",
    "    toc = time.time()\n",
    "    print(\"Total elapsed time:\", (toc-tic), \"seconds.\")\n",
    "    return (Bmodels['model'].dropna().iloc[0])\n",
    "\n",
    "Backward_best_model = backward_model(train_x, train_y)\n",
    "\n",
    "Backward_best_model.aic\n",
    "\n",
    "Backward_best_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36ca23d",
   "metadata": {
    "id": "a36ca23d"
   },
   "source": [
    "### 단계적 선택법(Stepwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fddeda3",
   "metadata": {
    "id": "4fddeda3"
   },
   "outputs": [],
   "source": [
    "def Stepwise_model(X, y):\n",
    "    Stepmodels = pd.DataFrame(columns=['AIC', 'model'])\n",
    "    tic = time.time()\n",
    "    predictors = []\n",
    "    Smodel_before = processSubset(X, y, predictors + ['const'])['AIC']\n",
    "    \n",
    "    for i in range(1, len(X.columns.difference(['const'])) + 1):\n",
    "        Forward_result = forward(X, y, predictors=predictors)\n",
    "        print('forward')\n",
    "        Stepmodels.loc[i] = Forward_result\n",
    "        predictors = Stepmodels.loc[i][\"model\"].model.exog_names\n",
    "        predictors = [ k for k in predictors if k != 'const']\n",
    "        Backward_result = backward(X=X, y=y, predictors=predictors)\n",
    "        if Backward_result['AIC']< Forward_result['AIC']:\n",
    "            Stepmodels.loc[i] = Backward_result\n",
    "            predictors = Stepmodels.loc[i][\"model\"].model.exog_names\n",
    "            Smodel_before = Stepmodels.loc[i][\"AIC\"]\n",
    "            predictors = [ k for k in predictors if k != 'const']\n",
    "            print('backward')\n",
    "        if Stepmodels.loc[i]['AIC']> Smodel_before:\n",
    "            break\n",
    "        else:\n",
    "            Smodel_before = Stepmodels.loc[i][\"AIC\"]\n",
    "    toc = time.time()\n",
    "    print(\"Total elapsed time:\", (toc - tic), \"seconds.\")\n",
    "    return (Stepmodels['model'][len(Stepmodels['model'])])\n",
    "\n",
    "Stepwise_best_model = Stepwise_model(train_x, train_y)\n",
    "\n",
    "Stepwise_best_model.aic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac9a3cb",
   "metadata": {
    "id": "0ac9a3cb"
   },
   "source": [
    "### 모델 성능평가 및 적합성 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bda8808",
   "metadata": {
    "id": "5bda8808",
    "outputId": "6a0cbedc-f903-4672-830f-2b53d5c6fba1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the number of features used\n",
    "print(fitted_full_model.params.shape,\n",
    "      Forward_best_model.params.shape,\n",
    "      Backward_best_model.params.shape,\n",
    "      Stepwise_best_model.params.shape)\n",
    "    # 비슷한 성능이면 param 수가 적은 게 더 좋은 모델이다.\n",
    "\n",
    "# model's y-hat\n",
    "pred_y_full = fitted_full_model.predict(test_x)\n",
    "pred_y_forward = Forward_best_model.predict(test_x[Forward_best_model.model.exog_names])\n",
    "pred_y_backward = Backward_best_model.predict(test_x[Backward_best_model.model.exog_names])\n",
    "pred_y_stepwise = Stepwise_best_model.predict(test_x[Stepwise_best_model.model.exog_names])\n",
    "\n",
    "# performance matrix\n",
    "perf_mat = pd.DataFrame(columns=['ALL', 'FORWARD', 'BACKWARD', 'STEPWISE'],\n",
    "                       index = ['MSE', 'RMSE', 'MAE', 'MAPE'])\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "from sklearn import metrics\n",
    "perf_mat.loc['MSE']['ALL'] = metrics.mean_squared_error(test_y,pred_y_full)\n",
    "perf_mat.loc['MSE']['FORWARD'] = metrics.mean_squared_error(test_y,pred_y_forward)\n",
    "perf_mat.loc['MSE']['BACKWARD'] = metrics.mean_squared_error(test_y,pred_y_backward)\n",
    "perf_mat.loc['MSE']['STEPWISE'] = metrics.mean_squared_error(test_y,pred_y_stepwise)\n",
    "\n",
    "perf_mat.loc['RMSE']['ALL'] = np.sqrt(metrics.mean_squared_error(test_y, pred_y_full))\n",
    "perf_mat.loc['RMSE']['FORWARD'] = np.sqrt(metrics.mean_squared_error(test_y, pred_y_forward))\n",
    "perf_mat.loc['RMSE']['BACKWARD'] = np.sqrt(metrics.mean_squared_error(test_y, pred_y_backward))\n",
    "perf_mat.loc['RMSE']['STEPWISE'] = np.sqrt(metrics.mean_squared_error(test_y, pred_y_stepwise))\n",
    "\n",
    "perf_mat.loc['MAE']['ALL'] = metrics.mean_absolute_error(test_y, pred_y_full)\n",
    "perf_mat.loc['MAE']['FORWARD'] = metrics.mean_absolute_error(test_y, pred_y_forward)\n",
    "perf_mat.loc['MAE']['BACKWARD'] = metrics.mean_absolute_error(test_y, pred_y_backward)\n",
    "perf_mat.loc['MAE']['STEPWISE'] = metrics.mean_absolute_error(test_y, pred_y_stepwise)\n",
    "\n",
    "perf_mat.loc['MAPE']['ALL'] = mean_absolute_percentage_error(test_y, pred_y_full)\n",
    "perf_mat.loc['MAPE']['FORWARD'] = mean_absolute_percentage_error(test_y, pred_y_forward)\n",
    "perf_mat.loc['MAPE']['BACKWARD'] = mean_absolute_percentage_error(test_y, pred_y_backward)\n",
    "perf_mat.loc['MAPE']['STEPWISE'] = mean_absolute_percentage_error(test_y, pred_y_stepwise)\n",
    "\n",
    "print(perf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4409ecf",
   "metadata": {
    "id": "e4409ecf"
   },
   "source": [
    "**적합성 검증 - 잔차분석**<br>\n",
    "- 정규성: qqplot 잔차의 정규분포 확인\n",
    "- 독립성: t-시점과 t-1시점 잔차의 산점도 분석\n",
    "- 등분산성: 예측값(y-hat)에 따른 잔차의 분산 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cd7f98",
   "metadata": {
    "id": "71cd7f98",
    "outputId": "71aec2ff-effd-4fc9-e0d8-6934c726ba4c"
   },
   "outputs": [],
   "source": [
    "# 정규성 - qq plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "res_full = fitted_full_model.resid\n",
    "res_step = Stepwise_best_model.resid\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(20,10))\n",
    "fig1 = sm.qqplot(res_full, fit=True, line='45', ax=ax1, xlabel='res_full')\n",
    "fig2 = sm.qqplot(res_step, fit=True, line='45', ax=ax2, xlabel='res_step')\n",
    "\n",
    "# 등분산성 - y-hat~resid 산점도\n",
    "    # residual pattern 확인\n",
    "\n",
    "pred_y_full = fitted_full_model.predict(train_x)\n",
    "pred_y_step = Stepwise_best_model.predict(train_x[Stepwise_best_model.model.exog_names])\n",
    "\n",
    "\n",
    "ax1 = plt.subplot()\n",
    "plt.scatter(pred_y_full, res_full, s=4)\n",
    "plt.xlim(4000, 30000)\n",
    "plt.xlabel('Fitted values')\n",
    "plt.ylabel('Residual')\n",
    "plt.title('full model')\n",
    "plt.show()\n",
    "\n",
    "ax2 = plt.subplot()\n",
    "plt.scatter(pred_y_step, res_step, s=4, c='green')\n",
    "plt.xlim(4000, 30000)\n",
    "plt.xlabel('Fitted values')\n",
    "plt.ylabel('Residual')\n",
    "plt.title('step model')\n",
    "plt.show()\n",
    "\n",
    "# 독립성은 to-research\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1674e7",
   "metadata": {
    "id": "cb1674e7"
   },
   "source": [
    "# Logistic Regression<a id='section4'></a>\n",
    "\n",
    "**also.. <font color='red'>Feature Selection</font>**<br>\n",
    "**<font color='red'>Regression Regularization</font>**<br>\n",
    "**in order to achieve better performance**<br><br>\n",
    "\n",
    "**::NOTE**<br>\n",
    "- Logistic regression은 회귀 모델이 아니라 '분류' 모델이다.\n",
    "- 그래서 성능지표도 accuracy, g-mean, f1 measure 등을 사용해야 한다.\n",
    "    - confusion matrix, ROC, AUC 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f664395e",
   "metadata": {
    "id": "f664395e",
    "outputId": "861dced9-e7d7-4a1b-aa1f-e5c37cff28c9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import time\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "\n",
    "ploan = pd.read_csv('/Users/jihun/Desktop/Fastcampus/Data/Personal Loan.csv')\n",
    "\n",
    "# 결측치 있는 행 제거 & 의미 없는 변수 제거 (ID, ZIP code)\n",
    "ploan_processed = ploan.dropna().drop(['ID', 'ZIP Code'], axis=1, inplace=False)\n",
    "\n",
    "# 상수항 추가\n",
    "ploan_processed = sm.add_constant(ploan_processed, has_constant='add')\n",
    "\n",
    "# train, test split\n",
    "feature_columns = list(ploan_processed.columns.difference(['Personal Loan']))\n",
    "X = ploan_processed[feature_columns]\n",
    "y = ploan_processed['Personal Loan']  # 대출여부: 1 or 0\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, stratify=y, train_size=0.7, test_size=0.3, random_state=42)\n",
    "\n",
    "ploan_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaac09c0",
   "metadata": {
    "id": "eaac09c0"
   },
   "source": [
    "**ploan data**<br>\n",
    "- Experience 경력\n",
    "- Income 수입\n",
    "- Famliy 가족단위\n",
    "- CCAvg 월 카드사용량 \n",
    "- Education 교육수준 (1: undergrad; 2, Graduate; 3; Advance)\n",
    "- Mortgage 가계대출\n",
    "- Securities account 유가증권계좌유무\n",
    "- CD account 양도예금증서 계좌 유무\n",
    "- Online 온라인계좌유무\n",
    "- CreidtCard 신용카드유무"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec20a7e",
   "metadata": {
    "id": "2ec20a7e",
    "outputId": "7a2f9728-7a9c-496f-dcc6-50d641b9d318"
   },
   "outputs": [],
   "source": [
    "# logistic modelling and fitting\n",
    "logit = sm.Logit(train_y, train_x)\n",
    "fitted_logit = logit.fit(method='newton')\n",
    "\n",
    "# summary\n",
    "fitted_logit.summary()\n",
    "\n",
    "# params(회귀계수)\n",
    "fitted_logit.params\n",
    "\n",
    "# exp\n",
    "np.exp(fitted_logit.params)\n",
    "\n",
    "## 나이가 한살 많을수록록 대출할 확률이 1.024배 높다.\n",
    "## 수입이 1단위 높을소룩 대출할 확률이 1.05배 높다 \n",
    "## 가족 구성원수가 1많을수록 대출할 확률이 2.13배 높다\n",
    "## 경력이 1단위 높을수록 대출할 확률이 0.99배 높다(귀무가설 채택)\n",
    "# Experience,  Mortgage는 제외할 필요성이 있어보임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82c5a25",
   "metadata": {
    "id": "b82c5a25"
   },
   "source": [
    "### 로지스틱 모델 성능지표 확인\n",
    "- confusion matrix\n",
    "    - accuracy\n",
    "- ROC, AUC 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd54d224",
   "metadata": {
    "id": "dd54d224",
    "outputId": "0cf625e2-e6dc-4e92-d350-b49ae0898d10"
   },
   "outputs": [],
   "source": [
    "# y-hat\n",
    "pred_y = fitted_logit.predict(test_x)\n",
    "\n",
    "# logit의 pred 값이 1혹은 0이 되도록 하는 함수 정의\n",
    "def cut_off(y, threshold):\n",
    "    Y = y.copy()  # copy함수를 사용하여 이전의 y값이 변하지 않게 함\n",
    "    # Y: 0 혹은 1의 예측값,    y: 0~1 사이 실수 예측값\n",
    "    Y[Y > threshold] = 1\n",
    "    Y[Y <= threshold] = 0\n",
    "    return(Y.astype(int))\n",
    "\n",
    "pred_Y = cut_off(pred_y, 0.5)\n",
    "pred_Y\n",
    "\n",
    "# confusion matrix\n",
    "cfmat = confusion_matrix(test_y, pred_Y)\n",
    "print(cfmat)\n",
    "\n",
    "# confusion matrix accuracy\n",
    "print('accuracy:', (cfmat[0,0]+cfmat[1,1]) / len(pred_Y))\n",
    "\n",
    "# accuracy 구하는 함수 정의\n",
    "def acc(cfmat):\n",
    "    acc = (cfmat[0,0] + cfmat[1,1]) / (cfmat[0,0] + cfmat[1,1] + cfmat[0,1] + cfmat[1,0])\n",
    "    return acc\n",
    "\n",
    "# cut_off의 threshold에 따른 성능지표 비교\n",
    "threshold = np.arange(0, 1, 0.1)\n",
    "table = pd.DataFrame(columns=['ACC'])\n",
    "for i in threshold:\n",
    "    pred_Y = cut_off(pred_y, i)\n",
    "    cfmat = confusion_matrix(test_y, pred_Y)\n",
    "    table.loc[i] = acc(cfmat)\n",
    "table.index.name = 'threshold'\n",
    "table.columns.name = 'performance'\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677c8b77",
   "metadata": {
    "id": "677c8b77"
   },
   "source": [
    "**!!! threshold에 따른 성능지표 비교 후 가장 높은 ACC 반납하는 함수 만들기**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5384a021",
   "metadata": {
    "id": "5384a021"
   },
   "source": [
    "best_cut_off\n",
    "best_threshold_acc\n",
    "pred_y_full을 받으면\n",
    "threshold 0.1~0.9까지로 cut_off를 돌리고\n",
    "cut_off로 나온 pred_Y_full과 test_y의 cfmat를 만들고\n",
    "cfmat의 acc를 저장한다.\n",
    "0.9까지 다 돌리면 \"가장 높은 acc 값\"과 \"사용한 threshold\"를 반환한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7eecde",
   "metadata": {
    "id": "ff7eecde",
    "outputId": "6a8d8b18-340d-4a17-9340-69a74e88e6d3"
   },
   "outputs": [],
   "source": [
    "\n",
    "def cut_off(y, threshold):\n",
    "    Y = y.copy()  # copy함수를 사용하여 이전의 y값이 변하지 않게 함\n",
    "    # Y: 0 혹은 1의 예측값,    y: 0~1 사이 실수 예측값\n",
    "    Y[Y > threshold] = 1\n",
    "    Y[Y <= threshold] = 0\n",
    "    return(Y.astype(int))\n",
    "\n",
    "\n",
    "def acc(cfmat):\n",
    "    acc = (cfmat[0,0] + cfmat[1,1]) / (cfmat[0,0] + cfmat[1,1] + cfmat[0,1] + cfmat[1,0])\n",
    "    return acc\n",
    "\n",
    "\n",
    "\n",
    "threshold = np.arange(0, 1, 0.1)\n",
    "table = pd.DataFrame(columns=['ACC'])\n",
    "result = []\n",
    "for i in threshold:\n",
    "    pred_Y = cut_off(pred_y, i)\n",
    "    cfmat = confusion_matrix(test_y, pred_Y)\n",
    "    table.loc[i] = acc(cfmat)\n",
    "table.index.name = 'threshold'\n",
    "table.columns.name = 'performance'\n",
    "table\n",
    "\n",
    "\n",
    "def forward(X, y, predictors):\n",
    "    # 데이터 변수들이 미리정의된 predictors에 있는지 없는지 확인 및 분류\n",
    "    remaining_predictors = [p for p in X.columns.difference(['const']) if p not in predictors]\n",
    "    tic = time.time()\n",
    "    results = []\n",
    "    for p in remaining_predictors:\n",
    "        results.append(processSubset(X=X, y= y, feature_set=predictors+[p]+['const']))\n",
    "    # 데이터프레임으로 변환\n",
    "    models = pd.DataFrame(results)\n",
    "\n",
    "    # AIC가 가장 낮은 것을 선택\n",
    "    best_model = models.loc[models['AIC'].argmin()] # index\n",
    "    toc = time.time()\n",
    "    print(\"Processed \", models.shape[0], \"models on\", len(predictors)+1, \"predictors in\", (toc-tic))\n",
    "    print('Selected predictors:',best_model['model'].model.exog_names,' AIC:',best_model[0] )\n",
    "    return best_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# sklearn ROC 패키지\n",
    "fpr, tpr, thresholds = metrics.roc_curve(test_y, pred_y, pos_label=1)\n",
    "\n",
    "# print ROC curve\n",
    "plt.plot(fpr, tpr)\n",
    "\n",
    "# print AUC\n",
    "auc = np.trapz(tpr,fpr)\n",
    "print('AUC:', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9ca7b5",
   "metadata": {
    "id": "7b9ca7b5"
   },
   "source": [
    "### Feature Selection again & 성능 측정\n",
    "- 특이하게.. 로지스틱 회귀모형은 cut_off 함수를 지나기 전엔 '회귀' 값을 갖는다.\n",
    "- 그러니까 변수선택법을 통해 regression 값의 AIC를 최대한 높이고,\n",
    "- 그 후에 cut_off 함수를 지나게 해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b5f7ad",
   "metadata": {
    "id": "c9b5f7ad"
   },
   "outputs": [],
   "source": [
    "# 위 섹션에 정의된 함수 그대로 사용\n",
    "    # !!processSubset에서 model만 logit으로 변경!!\n",
    "\n",
    "def processSubset(X,y, feature_set):\n",
    "            model = sm.Logit(y,X[list(feature_set)])\n",
    "            regr = model.fit()\n",
    "            AIC = regr.aic\n",
    "            return {\"model\":regr, \"AIC\":AIC}\n",
    "        \n",
    "'''\n",
    "전진선택법\n",
    "'''\n",
    "def forward(X, y, predictors):\n",
    "    # 데이터 변수들이 미리정의된 predictors에 있는지 없는지 확인 및 분류\n",
    "    remaining_predictors = [p for p in X.columns.difference(['const']) if p not in predictors]\n",
    "    tic = time.time()\n",
    "    results = []\n",
    "    for p in remaining_predictors:\n",
    "        results.append(processSubset(X=X, y= y, feature_set=predictors+[p]+['const']))\n",
    "    # 데이터프레임으로 변환\n",
    "    models = pd.DataFrame(results)\n",
    "\n",
    "    # AIC가 가장 낮은 것을 선택\n",
    "    best_model = models.loc[models['AIC'].argmin()] # index\n",
    "    toc = time.time()\n",
    "    print(\"Processed \", models.shape[0], \"models on\", len(predictors)+1, \"predictors in\", (toc-tic))\n",
    "    print('Selected predictors:',best_model['model'].model.exog_names,' AIC:',best_model[0] )\n",
    "    return best_model\n",
    "\n",
    "def forward_model(X,y):\n",
    "    Fmodels = pd.DataFrame(columns=[\"AIC\", \"model\"])\n",
    "    tic = time.time()\n",
    "    # 미리 정의된 데이터 변수\n",
    "    predictors = []\n",
    "    # 변수 1~10개 : 0~9 -> 1~10\n",
    "    for i in range(1, len(X.columns.difference(['const'])) + 1):\n",
    "        Forward_result = forward(X=X,y=y,predictors=predictors)\n",
    "        if i > 1:\n",
    "            if Forward_result['AIC'] > Fmodel_before:\n",
    "                break\n",
    "        Fmodels.loc[i] = Forward_result\n",
    "        predictors = Fmodels.loc[i][\"model\"].model.exog_names\n",
    "        Fmodel_before = Fmodels.loc[i][\"AIC\"]\n",
    "        predictors = [ k for k in predictors if k != 'const']\n",
    "    toc = time.time()\n",
    "    print(\"Total elapsed time:\", (toc - tic), \"seconds.\")\n",
    "\n",
    "    return(Fmodels['model'][len(Fmodels['model'])])\n",
    "\n",
    "\n",
    "'''\n",
    "후진소거법\n",
    "'''\n",
    "def backward(X,y,predictors):\n",
    "    tic = time.time()\n",
    "    results = []\n",
    "    \n",
    "    # 데이터 변수들이 미리정의된 predictors 조합 확인\n",
    "    for combo in itertools.combinations(predictors, len(predictors) - 1):\n",
    "        results.append(processSubset(X=X, y= y,feature_set=list(combo)+['const']))\n",
    "    models = pd.DataFrame(results)\n",
    "    \n",
    "    # 가장 낮은 AIC를 가진 모델을 선택\n",
    "    best_model = models.loc[models['AIC'].argmin()]\n",
    "    toc = time.time()\n",
    "    print(\"Processed \", models.shape[0], \"models on\", len(predictors) - 1, \"predictors in\",\n",
    "          (toc - tic))\n",
    "    print('Selected predictors:',best_model['model'].model.exog_names,' AIC:',best_model[0] )\n",
    "    return best_model\n",
    "\n",
    "\n",
    "def backward_model(X, y):\n",
    "    Bmodels = pd.DataFrame(columns=[\"AIC\", \"model\"], index = range(1,len(X.columns)))\n",
    "    tic = time.time()\n",
    "    predictors = X.columns.difference(['const'])\n",
    "    Bmodel_before = processSubset(X,y,predictors)['AIC']\n",
    "    while (len(predictors) > 1):\n",
    "        Backward_result = backward(X=train_x, y= train_y, predictors = predictors)\n",
    "        if Backward_result['AIC'] > Bmodel_before:\n",
    "            break\n",
    "        Bmodels.loc[len(predictors) - 1] = Backward_result\n",
    "        predictors = Bmodels.loc[len(predictors) - 1][\"model\"].model.exog_names\n",
    "        Bmodel_before = Backward_result['AIC']\n",
    "        predictors = [ k for k in predictors if k != 'const']\n",
    "\n",
    "    toc = time.time()\n",
    "    print(\"Total elapsed time:\", (toc - tic), \"seconds.\")\n",
    "    return (Bmodels['model'].dropna().iloc[0])\n",
    "\n",
    "\n",
    "'''\n",
    "단계적 선택법\n",
    "'''\n",
    "def stepwise_model(X,y):\n",
    "    Stepmodels = pd.DataFrame(columns=[\"AIC\", \"model\"])\n",
    "    tic = time.time()\n",
    "    predictors = []\n",
    "    Smodel_before = processSubset(X,y,predictors+['const'])['AIC']\n",
    "    # 변수 1~10개 : 0~9 -> 1~10\n",
    "    for i in range(1, len(X.columns.difference(['const'])) + 1):\n",
    "        Forward_result = forward(X=X, y=y, predictors=predictors) # constant added\n",
    "        print('forward')\n",
    "        Stepmodels.loc[i] = Forward_result\n",
    "        predictors = Stepmodels.loc[i][\"model\"].model.exog_names\n",
    "        predictors = [ k for k in predictors if k != 'const']\n",
    "        Backward_result = backward(X=X, y=y, predictors=predictors)\n",
    "        if Backward_result['AIC']< Forward_result['AIC']:\n",
    "            Stepmodels.loc[i] = Backward_result\n",
    "            predictors = Stepmodels.loc[i][\"model\"].model.exog_names\n",
    "            Smodel_before = Stepmodels.loc[i][\"AIC\"]\n",
    "            predictors = [ k for k in predictors if k != 'const']\n",
    "            print('backward')\n",
    "        if Stepmodels.loc[i]['AIC']> Smodel_before:\n",
    "            break\n",
    "        else:\n",
    "            Smodel_before = Stepmodels.loc[i][\"AIC\"]\n",
    "    toc = time.time()\n",
    "    print(\"Total elapsed time:\", (toc - tic), \"seconds.\")\n",
    "    return (Stepmodels['model'][len(Stepmodels['model'])])\n",
    "\n",
    "Forward_best_model = forward_model(train_x, train_y)\n",
    "Backward_best_model = backward_model(train_x, train_y)\n",
    "stepwise_best_model = stepwise_model(train_x, train_y)\n",
    "\n",
    "# y-hat\n",
    "pred_y_full = fitted_logit.predict(test_x)\n",
    "pred_y_forward = Forward_best_model.predict(test_x[Forward_best_model.model.exog_names])\n",
    "pred_y_backward = Backward_best_model.predict(test_x[Backward_best_model.model.exog_names])\n",
    "pred_y_stepwise = stepwise_best_model.predict(test_x[stepwise_best_model.model.exog_names])\n",
    "\n",
    "# cut_off 함수 적용\n",
    "pred_Y_full = cut_off(pred_y_full, 0.5)\n",
    "pred_Y_forward = cut_off(pred_y_forward, 0.5)\n",
    "pred_Y_backward = cut_off(pred_y_backward, 0.5)\n",
    "pred_Y_stepwise = cut_off(pred_y_stepwise, 0.5)\n",
    "\n",
    "# cfmat 만들기\n",
    "cfmat_full = confusion_matrix(test_y, pred_Y_full)\n",
    "cfmat_forward = confusion_matrix(test_y, pred_Y_forward)\n",
    "cfmat_backward = confusion_matrix(test_y, pred_Y_backward)\n",
    "cfmat_stepwise = confusion_matrix(test_y, pred_Y_stepwise)\n",
    "\n",
    "# acc 함수 사용하기\n",
    "print(acc(cfmat_full))\n",
    "print(acc(cfmat_forward))\n",
    "print(acc(cfmat_backward))\n",
    "print(acc(cfmat_stepwise))\n",
    "\n",
    "## 추가로 ROC curve와 AUC도 구하면 됌.\n",
    "    ## 결론: 이 데이터에서 변수선택법을 사용해도 logit regression의 성능이 큰 차이가 없음.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210085a9",
   "metadata": {
    "id": "210085a9"
   },
   "source": [
    "# Regression Regularization<a id='section5'></a>\n",
    "\n",
    "#### Lasso & Ridge\n",
    "- 회귀계수축소법으로 위의 logit regression 성능이 증가할까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8c347c",
   "metadata": {
    "id": "ca8c347c",
    "outputId": "92507faa-fb34-4bb0-9cd9-b4c389b28e34"
   },
   "outputs": [],
   "source": [
    "# Lasso modelling and fitting\n",
    "ll = Lasso(alpha=0.01)\n",
    "ll.fit(train_x, train_y)\n",
    "\n",
    "# 회귀계수 출력\n",
    "ll.coef_\n",
    "    # 실제로는 쓸모 없는 변수가 살아남기도 하고,\n",
    "    # p value가 낮았던 변수의 회귀계수가 0에 가까워지기도 한다.\n",
    "\n",
    "# 기존 모델의 summary와 비교\n",
    "fitted_logit.summary()\n",
    "\n",
    "# y-hat, confusion_matrix, acc 계산\n",
    "pred_y_lasso = ll.predict(test_x)\n",
    "pred_Y_lasso = cut_off(pred_y_lasso, 0.5)\n",
    "\n",
    "cfmat = confusion_matrix(test_y, pred_Y_lasso)\n",
    "print('Lasso accuracy:', acc(cfmat))\n",
    "\n",
    "# ROC curve와 AUC 구하기\n",
    "fpr, tpr, thresholds = metrics.roc_curve(test_y, pred_y_lasso, pos_label=1)\n",
    "plt.plot(fpr,tpr)\n",
    "\n",
    "auc = np.trapz(tpr,fpr)\n",
    "print('Lasso AUC:', auc)\n",
    "\n",
    "# Ridge modelling and fitting\n",
    "rr = Ridge(alpha=0.01)\n",
    "rr.fit(train_x, train_y)\n",
    "\n",
    "# 회귀계수 출력\n",
    "print('rr coef:', rr.coef_)\n",
    "\n",
    "# lasso 회귀계수와 비교\n",
    "print('\\nll coef:', ll.coef_)\n",
    "\n",
    "# y-hat, confusion_matrix, acc 계산\n",
    "pred_y_ridge = rr.predict(test_x)\n",
    "pred_Y_ridge = cut_off(pred_y_ridge, 0.5)\n",
    "\n",
    "cfmat = confusion_matrix(test_y, pred_Y_ridge)\n",
    "print('Ridge accuracy:', acc(cfmat))\n",
    "\n",
    "# ROC curve와 AUC 구하기\n",
    "fpr, tpr, thresholds = metrics.roc_curve(test_y, pred_y_ridge, pos_label=1)\n",
    "plt.plot(fpr,tpr)\n",
    "\n",
    "auc = np.trapz(tpr,fpr)\n",
    "print('Ridge AUC:', auc)\n",
    "\n",
    "# lambda 값에 따른 회귀계수 / accuracy 계산\n",
    "    # lambda는 0.001 ~ 10 까지 범위 설정\n",
    "alpha = np.logspace(-3, 1, 5)\n",
    "\n",
    "data = []\n",
    "acc_table = []\n",
    "for i, a in enumerate(alpha):   # enumerate는 (0, 0.001), (1, 0.01) ... 이런 식으로 (index, value) 처럼 인덱스를 넣어주는 역할\n",
    "            # !!! 근데 여기서 enumerate 안해도 함수 작동되는데 왜 했지?\n",
    "    lasso = Lasso(alpha=a).fit(train_x, train_y)\n",
    "    data.append(pd.Series(np.hstack([lasso.intercept_, lasso.coef_])))\n",
    "        # np.hstack를 하면 nparray(intercept, weight1, weight2, ... weight 12)  가 되고 pd.Series 하면 인덱스가 추가된 series가 된다.\n",
    "        # 이걸 data에 넣으면 0번 index에 intercept, 1~12번 index에는 각 feature마다의 weight가 입력된다.\n",
    "    pred_y = lasso.predict(test_x)\n",
    "    pred_y = cut_off(pred_y, 0.5)\n",
    "    cfmat = confusion_matrix(test_y, pred_y)\n",
    "    acc_table.append((acc(cfmat)))\n",
    "\n",
    "df_lasso = pd.DataFrame(data, index=alpha)  \n",
    "df_lasso\n",
    "acc_table_lasso = pd.DataFrame(ac_table, index=alpha).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ff7d70",
   "metadata": {
    "id": "d9ff7d70"
   },
   "source": [
    "### >>> 위의 함수 이해하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a303ee",
   "metadata": {
    "id": "97a303ee",
    "outputId": "5e96ebe7-beb1-42bd-c585-447d30b08b7b"
   },
   "outputs": [],
   "source": [
    "np.hstack([Lasso(alpha=0.001).fit(train_x, train_y).intercept_, Lasso(alpha=0.001).fit(train_x, train_y).coef_])\n",
    "    # 1번째가 intercept, 나머지는 coef\n",
    "    # np.hstack은 두 array를 하나의 array로 합쳐줌\n",
    "\n",
    "# series로 만들기\n",
    "pd.Series(np.hstack([Lasso(alpha=0.001).fit(train_x, train_y).intercept_, Lasso(alpha=0.001).fit(train_x, train_y).coef_]))\n",
    "\n",
    "# series 두 개를 data1=[]에 합치기\n",
    "data1 = []\n",
    "s1 = pd.Series(np.hstack([Lasso(alpha=0.001).fit(train_x, train_y).intercept_, Lasso(alpha=0.001).fit(train_x, train_y).coef_]))\n",
    "s2 = pd.Series(np.hstack([Lasso(alpha=0.01).fit(train_x, train_y).coef_, Lasso(alpha=0.01).fit(train_x, train_y).intercept_]))\n",
    "\n",
    "data1.append(s1)\n",
    "\n",
    "data1.append(s2)\n",
    "\n",
    "data1\n",
    "\n",
    "# 위에는 intercept랑 coef 저장하는 'data1'을 만들어놓은 거고.. 이건 type(data1) = list.. 즉, series를 원소로 갖는 list다.\n",
    "\n",
    "# 다음으로 acc값을 저장하는 acc_table을 만들면..\n",
    "acc_table = []\n",
    "pred_y1 = Lasso(alpha=0.001).fit(train_x, train_y).predict(test_x)\n",
    "pred_y1 = cut_off(pred_y1, 0.5)\n",
    "cfmat = confusion_matrix(test_y, pred_y1)\n",
    "acc_table.append((acc(cfmat)))\n",
    "\n",
    "pred_y2 = Lasso(alpha=0.01).fit(train_x, train_y).predict(test_x)\n",
    "pred_y2 = cut_off(pred_y2, 0.5)\n",
    "cfmat = confusion_matrix(test_y, pred_y2)\n",
    "acc_table.append((acc(cfmat)))\n",
    "\n",
    "acc_table\n",
    "# pred_y1 의 accuracy와 pred_y2의 accuracy가 저장된 list\n",
    "\n",
    "# 이제 data1과 acc_table이라는 두 리스트를 dataframe으로 만들자.\n",
    "\n",
    "df_lasso1 = pd.DataFrame(data1, index=['0.001', '0.01'])\n",
    "df_lasso1\n",
    "# 인덱스를 0.001, 0.01 같은 alpha값으로 주면 기존 series의 인덱스는 각각 값의 칼럼이 된다.\n",
    "\n",
    "\n",
    "# 근데 보기 더 편하게 transpose해준다.\n",
    "df_lasso1.T\n",
    "\n",
    "# acc_table도 똑같다.\n",
    "acc_table_lasso1 = pd.DataFrame(acc_table, index=['0.001', '0.01']).T\n",
    "acc_table_lasso1\n",
    "\n",
    "## +⍺ Ridge lambda값 변화에 따른 coef, intercept, acc 변화\n",
    "data = []\n",
    "acc_table = []\n",
    "for i, a in enumerate(alpha):\n",
    "    ridge = Ridge(alpha=a).fit(train_x, train_y)\n",
    "    data.append(pd.Series(np.hstack([ridge.intercept_, ridge.coef_])))\n",
    "    pred_y = ridge.predict(test_x)\n",
    "    pred_y = cut_off(pred_y, 0.5)\n",
    "    cfmat = confusion_matrix(test_y, pred_y)\n",
    "    acc_table.append((acc(cfmat)))\n",
    "    \n",
    "df_ridge = pd.DataFrame(data, index=alpha).T\n",
    "acc_table_ridge = pd.DataFrame(acc_table, index=alpha).T\n",
    "\n",
    "df_ridge\n",
    "\n",
    "acc_table_ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d81247",
   "metadata": {
    "id": "d3d81247"
   },
   "source": [
    "### lambda 값의 변화에 따른 회귀계수 축소 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996341fe",
   "metadata": {
    "id": "996341fe",
    "outputId": "d69c2964-a6d9-4d05-ad76-a67d52bec6e3"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ax1 = plt.subplot(121)\n",
    "plt.semilogx(df_ridge.T)\n",
    "plt.xticks(alpha)\n",
    "plt.title(\"Ridge\")\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "plt.semilogx(df_lasso.T)\n",
    "plt.xticks(alpha)\n",
    "plt.title(\"Lasso\")\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bfa1db",
   "metadata": {
    "id": "15bfa1db"
   },
   "source": [
    "# PCA<a id='section6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5ea14c",
   "metadata": {
    "id": "7f5ea14c"
   },
   "source": [
    "### 데이터 전처리 및 EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7661cc0",
   "metadata": {
    "id": "c7661cc0",
    "outputId": "622e712f-0ba8-47e0-a803-26577fc7c04f"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "dir(iris)\n",
    "\n",
    "keys = iris.keys()\n",
    "print('붓꽃 데이터 세트의 키들:', keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6e84fc",
   "metadata": {
    "id": "ab6e84fc"
   },
   "source": [
    "```python\n",
    "# dataset 확인 방법 예시\n",
    "print('\\n feature_names 의 type:',type(iris_data.feature_names))\n",
    "print(' feature_names 의 shape:',len(iris_data.feature_names))\n",
    "print(iris_data.feature_names)\n",
    "\n",
    "print('\\n target_names 의 type:',type(iris_data.target_names))\n",
    "print(' feature_names 의 shape:',len(iris_data.target_names))\n",
    "print(iris_data.target_names)\n",
    "\n",
    "print('\\n data 의 type:',type(iris_data.data))\n",
    "print(' data 의 shape:',iris_data.data.shape)\n",
    "print(iris_data['data'])\n",
    "\n",
    "print('\\n target 의 type:',type(iris_data.target))\n",
    "print(' target 의 shape:',iris_data.target.shape)\n",
    "print(iris_data.target)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce81ca1",
   "metadata": {
    "id": "dce81ca1"
   },
   "outputs": [],
   "source": [
    "# 독립변수 중 0열과 2열의 두 개만 사용한다면..\n",
    "X = iris.data[:, [0,2]]\n",
    "y = iris.target\n",
    "\n",
    "print(X.shape)\n",
    "feature_names = [iris.feature_names[0], iris.feature_names[2]]\n",
    "df_X = pd.DataFrame(X)\n",
    "df_X.head()\n",
    "\n",
    "print(y.shape)\n",
    "df_y = pd.DataFrame(y)\n",
    "df_y.head()\n",
    "\n",
    "# 결측치 여부 파악\n",
    "print(df_X.isnull().sum())\n",
    "print(df_y.isnull().sum())\n",
    "\n",
    "# 종속변수 파악\n",
    "print(set(y))\n",
    "print(iris.target_names)\n",
    "\n",
    "# 종속변수 분포\n",
    "df_y[0].value_counts().plot(kind='bar')\n",
    "plt.show()\n",
    "\n",
    "# 독립변수 분포\n",
    "for i in range(df_X.shape[1]):\n",
    "    sns.distplot(df_X[i])\n",
    "    plt.title(feature_names[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c7acd2",
   "metadata": {
    "id": "76c7acd2"
   },
   "source": [
    "### PCA 함수 사용\n",
    "- PCA는 차원축소 기법.\n",
    "- 비지도학습  ->\n",
    "    - e.g.) 2개의 x 변수를 성분2개로 표현하거나, 성분1개로 표현한다.\n",
    "    - 첫 번째 성분은 가장 분산이 큰 방향의 축, 두 번째는 두 번째로 분산이 큰 방향의 축\n",
    "    \n",
    ">대부분 실무에서 분석하는 데이터는 매우 많은 특성(feature)들을 가지고 있다. 이러한 데이터를 가지고 머신러닝 알고리즘을 적용해 문제를 해결하려고 한다면, 데이터의 차원이 크기 때문에 학습 속도가 느릴 뿐만아니라 성능 또한 좋지 않을 가능성이 크다.\n",
    "\n",
    "<img src='https://t1.daumcdn.net/cfile/tistory/990B83345A75B1ED31'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61664ea6",
   "metadata": {
    "id": "61664ea6",
    "outputId": "e090c32d-c5a5-41d6-e494-6bcb2e5ec496",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# x 변수간 산점도\n",
    "plt.scatter(X[:,0], X[:,1])\n",
    "\n",
    "# pca 함수를 이용해 주성분 2개 만들기\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "\n",
    "PCscore = pca.transform(X)\n",
    "PCscore    # PCscore의 1열은 component1로 변환된 값이고, 2열은 component2로 변환된 값이다.\n",
    "\n",
    "eigen_v=pca.components_\n",
    "print('eigen_v:', eigen_v)\n",
    "\n",
    "mX = np.matrix(X)\n",
    "for i in range(X.shape[1]):\n",
    "    mX[:, i] = mX[:, i] - np.mean(X[:, i])\n",
    "\n",
    "(mX * eigen_v)[0:5]\n",
    "\n",
    "plt.scatter(PCscore[:,0],PCscore[:,1])\n",
    "# 아래의 그래프에서 가로축이 component1(가장 분산이 큰 방향의 성분1), 세로축이 component2이다.\n",
    "\n",
    "# quiver to-research\n",
    "dfmX = pd.DataFrame(mX)\n",
    "plt.scatter(dfmX[0], dfmX[1])\n",
    "origin = [0], [0]\n",
    "plt.quiver(eigen_v[0], eigen_v[1], scale_units=\"xy\", color=['r','b'], scale=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3680d8c3",
   "metadata": {
    "id": "3680d8c3"
   },
   "source": [
    "### another example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1ed29b",
   "metadata": {
    "id": "5f1ed29b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import datasets\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "X, y = datasets.load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "X.head()\n",
    "\n",
    "print(X.shape)\n",
    "print(set(y))\n",
    "\n",
    "targetDf = pd.DataFrame(y)\n",
    "\n",
    "# StandardScaler로 데이터를 단위 분산으로 조정 (평균 0, 분산 1)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "principalComponents = pca.fit_transform(X)\n",
    "principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])\n",
    "\n",
    "finalDf = pd.concat([principalDf, targetDf], axis=1)\n",
    "finalDf.head()\n",
    "\n",
    "labels = [1, 0]\n",
    "\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 Component PCA', fontsize = 20)\n",
    "\n",
    "colors = [\"#7fc97f\",\"#beaed4\"]\n",
    "for label, color in zip(labels, colors):\n",
    "    indiciesToKeep = finalDf['target'] == label\n",
    "    ax.scatter(finalDf.loc[indiciesToKeep, 'principal component 1']\n",
    "                , finalDf.loc[indiciesToKeep, 'principal component 2']\n",
    "                , c = color\n",
    "                , s = 30)\n",
    "                \n",
    "ax.legend(labels)\n",
    "ax.grid()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94814f3c",
   "metadata": {
    "id": "94814f3c"
   },
   "source": [
    "# Gaussian Naive Bayes<a id='section7'></a>\n",
    "- 연속형: Gaussian Naive Bayes classifier\n",
    "- 범주형: Multinomial Naive Bayes\n",
    "    - 이분형: Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94422710",
   "metadata": {
    "id": "94422710"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# loading the dataset\n",
    "X, y = datasets.load_iris(return_X_y=True, as_frame=True)\n",
    "\n",
    "X.head()\n",
    "\n",
    "y.head()\n",
    "\n",
    "# modelling and fitting\n",
    "gnb = GaussianNB()\n",
    "fitted_gnb = gnb.fit(X, y)\n",
    "y_pred = fitted_gnb.predict(X)\n",
    "\n",
    "fitted_gnb.predict_proba(X)[[1, 48, 51, 100]]\n",
    "# 1, 48, 51, 100 인덱스의 데이터에 대해서..\n",
    "    # 1번 인덱스가 타겟 0일 확률은 1, 타겟 1일 확률은 1.51e-017, ...\n",
    "\n",
    "fitted_gnb.predict(X)[[1, 48, 51, 100]]\n",
    "\n",
    "# confusion matrix\n",
    "confusion_matrix(iris.target, y_pred)\n",
    "\n",
    "# 사전확률 설정 및 confusion matrix\n",
    "gnb2=GaussianNB(priors=[1/100,1/100,98/100])\n",
    "fitted2=gnb2.fit(iris.data,iris.target)\n",
    "y_pred2=fitted2.predict(iris.data)\n",
    "confusion_matrix(iris.target,y_pred2)\n",
    "\n",
    "gnb2=GaussianNB(priors=[1/100,98/100,1/100])\n",
    "fitted2=gnb2.fit(iris.data,iris.target)\n",
    "y_pred2=fitted2.predict(iris.data)\n",
    "confusion_matrix(iris.target,y_pred2)\n",
    "\n",
    "# 2번째 범주의 사전확률을 98%로 주니까\n",
    "    # 2번째 범주 예측은 다 맞추는데\n",
    "    # 3번째 범주를 2번째로 예측하는 경우가 많아진다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc39e9d",
   "metadata": {
    "id": "5bc39e9d"
   },
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f35980f",
   "metadata": {
    "id": "2f35980f"
   },
   "outputs": [],
   "source": [
    "X = np.random.randint(5, size=(6, 100))\n",
    "y = np.array([1, 2, 3, 4, 5, 6,])\n",
    "\n",
    "# modelling and fitting\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(clf.predict(X[2:3]))\n",
    "print(clf.predict_proba(X[2:3]))\n",
    "\n",
    "# 사전확률(prior) 설정해보기\n",
    "clf2 = MultinomialNB(class_prior=[0.1,0.5,0.1,0.1,0.1,0.1])\n",
    "clf2.fit(X,y)\n",
    "clf2.predict_proba(X[2:3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c71f39a",
   "metadata": {
    "id": "4c71f39a"
   },
   "source": [
    "# KNN<a id='section8'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18956fa",
   "metadata": {
    "id": "c18956fa"
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors, datasets\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "\n",
    "# modelling and fitting\n",
    "clf = neighbors.KNeighborsClassifier(5)\n",
    "clf.fit(X,y)\n",
    "\n",
    "y_pred = clf.predict(X)\n",
    "confusion_matrix(y,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81488af8",
   "metadata": {
    "id": "81488af8"
   },
   "source": [
    "### cross-validation으로 최적의 k 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9604b971",
   "metadata": {
    "id": "9604b971"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "k_range = range(1,100)\n",
    "k_scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    knn = neighbors.KNeighborsClassifier(k)\n",
    "    scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n",
    "    k_scores.append(scores.mean())\n",
    "\n",
    "plt.plot(k_range, k_scores)\n",
    "plt.xlabel('value of K for KNN')\n",
    "plt.ylabel('Cross-validated accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f0b048",
   "metadata": {
    "id": "bb7fdeb1"
   },
   "source": [
    "### Weight를 준 KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5c11a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 40\n",
    "h = .02 # step size in the mesh\n",
    "\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "for weights in ['uniform', 'distance']:\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n",
    "    clf.fit(X, y)\n",
    "    \n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n",
    "                edgecolor='k', s=20)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.title(\"3-Class classification (k = %i, weights = '%s')\"\n",
    "              % (n_neighbors, weights))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "np.random.seed(0)\n",
    "X = np.sort(5 * np.random.rand(40, 1), axis=0)\n",
    "T = np.linspace(0, 5, 500)[:, np.newaxis]\n",
    "y = np.sin(X).ravel()\n",
    "y[::5] += 1 * (0.5 - np.random.rand(8))\n",
    "\n",
    "knn = neighbors.KNeighborsRegressor(n_neighbors)\n",
    "y_ = knn.fit(X, y).predict(T)\n",
    "\n",
    "n_neighbors = 5\n",
    "\n",
    "for i, weights in enumerate(['uniform', 'distance']):\n",
    "    knn = neighbors.KNeighborsRegressor(n_neighbors, weights=weights)\n",
    "    y_ = knn.fit(X, y).predict(T)\n",
    "\n",
    "    plt.subplot(2, 1, i + 1)\n",
    "    plt.scatter(X, y, c='k', label='data')\n",
    "    plt.plot(T, y_, c='g', label='prediction')\n",
    "    plt.axis('tight')\n",
    "    plt.legend()\n",
    "    plt.title(\"KNeighborsRegressor (k = %i, weights = '%s')\" % (n_neighbors,\n",
    "                                                                weights))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75de34bf",
   "metadata": {
    "id": "75de34bf"
   },
   "source": [
    "# LDA<a id='section9'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675b0e38",
   "metadata": {
    "id": "675b0e38"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aa7c15",
   "metadata": {
    "id": "69aa7c15"
   },
   "source": [
    "# Decision Tree<a id='section10'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9363acfc",
   "metadata": {
    "id": "9363acfc"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ee8c5b",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "<a id='Deep-Learning'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb09963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_context(\"notebook\", font_scale=1.5)\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.model_selection import cross_val_score , cross_validate\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb5b235",
   "metadata": {},
   "source": [
    "### 2.1.1 인공 뉴런의 수학적 정의\n",
    "\n",
    "$z = w_1x_1+\\cdots+w_mx_m=\\sum_{j=1}^mx_jw_j=\\boldsymbol{w}^T\\boldsymbol{x}$\n",
    "\n",
    "퍼셉트론의 결정 함수 : $\\phi(z)=\\begin{cases}1&z\\ge\\theta\\mbox{ 일 때} \\\\ -1&\\mbox{그 외}\\end{cases}$\n",
    "$\\;\\;\\;\\;\\;$\n",
    "\n",
    "음수 임계값, 또는 가중치 $w_0 = -\\theta$ 를 *절편* 이라고 한다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cc265b",
   "metadata": {},
   "source": [
    "### 2.1.2 퍼셉트론 학습 규칙\n",
    "\n",
    "Rosenblatt의 임계 퍼셉트론 모델은 \"출력을 내거나 내지 않는 두 가지 경우만 있다.\"<br><br>\n",
    "퍼셉트론 알고리즘을 요약하면..\n",
    "1. 가중치를 0 또는 랜덤한 작은 값으로 초기화한다.\n",
    "2. 각 훈련 샘플 $x^{(i)}$에서 다음 작업을 한다.\n",
    "    1. 출력 값 $\\hat{y}$를 계산한다.\n",
    "    2. 가중치를 업데이트한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e15ef6",
   "metadata": {},
   "source": [
    "> $x_1^{(2)}$ 는 x의 1번째 feature의 (2)번째 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2ff38e",
   "metadata": {},
   "source": [
    "$\\Delta w_j = \\eta(y^{(i)}-\\hat{y}^{(i)})x_j^{(i)}$ <br>\n",
    "$y^{(i)}=1,\\;\\hat{y}^{(i)}=-1, \\qquad \\Delta w_j=\\eta(1-(-1))x_j^{(i)}=\\eta(2)x_j^{(i)}$ <br>\n",
    "$w_j := w_j + \\Delta w_j$\n",
    "> => $w_j$ j번째 feature의 가중치가 업데이트되는 방식은..<br>\n",
    "> i번째 행의 $\\hat{y}$ 과 $y$가 같다면 업데이트 하지 않고,<br>\n",
    "> 다르다면 이를 x의 j번째 feature의 i번째 데이터 & Learning Rate($\\eta$)와 곱한 만큼을 $\\Delta w_j$ 의 가중치 변화량으로 삼는다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24a665e",
   "metadata": {},
   "source": [
    "> 2차원 데이터셋, 즉 feature가 2개고 당연히 가중치도 2개인 데이터셋에서 업데이트되는 방식은 다음과 같다.\n",
    "$$\\Delta w_0 = \\eta (y^{(i)} - output^{(i)})$$\n",
    "$$\\Delta w_1 = \\eta (y^{(i)} - output^{(i)})x_1^{(i)}$$\n",
    "$$\\Delta w_2 = \\eta (y^{(i)} - output^{(i)})x_2^{(i)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa15d15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url='https://git.io/JtIbq', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90492aa",
   "metadata": {},
   "source": [
    "- 제일 왼쪽의 예처럼 선형으로 완전히 나눌 수 있으면 퍼셉트론이 특정 가중치에 수렴한다.\n",
    "- 만약 선형 결정 경계로 구분이 안 된다면, 훈련 데이터셋을 반복할 최대 횟수(epoch)와 분류 허용 오차를 지정해야 한다. Otherwise, 퍼셉트론은 가중치 업데이트를 멈추지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b57207",
   "metadata": {},
   "source": [
    "## 2.2 파이썬으로 퍼셉트론 학습 알고리즘 구현\n",
    "\n",
    "### 2.2.1 객체 지향 퍼셉트론 API\n",
    "> 객체 지향 방식을 사용하여 퍼셉트론 인터페이스를 가진 파이썬 클래스를 정의한다. Perceptron 객체를 초기화한 후 fit 메서드로 데이터에서 학습하고, 별도의 predict 메서드로 예측을 만든다.\n",
    "\n",
    "> 관례에 따라 객체의 초기화 과정에서 생성하지 않고 다른 메서드를 호출하여 만든 속성은 밑줄(\\_)을 추가한다. 예를 들어 self.w_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60010cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron(object):\n",
    "    \"\"\"퍼셉트론 분류기\n",
    "    \n",
    "    매개변수\n",
    "    --------------\n",
    "    eta : float\n",
    "        학습률 (0.0과 1.0 사이)\n",
    "    n_iter : int\n",
    "        훈련 데이터셋 반복 횟수 (epoch)\n",
    "    random_state : int\n",
    "        가중치 무작위 초기화를 위한 난수 생성기 시드\n",
    "    \n",
    "    속성\n",
    "    --------------\n",
    "    w_ : 1d-array\n",
    "        학습된 가중치\n",
    "    errors_ : list\n",
    "        epoch마다 누적된 분류 오류\n",
    "    \"\"\"\n",
    "    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"훈련데이터 학습\n",
    "        \n",
    "        매개변수\n",
    "        -----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "            n_samples개의 샘플과 n_features개의 특성으로 이루어진 훈련 데이터\n",
    "        y: array_like, shape = [n_samples]\n",
    "            타깃값\n",
    "        \n",
    "        반환값\n",
    "        -----------\n",
    "        self : object\n",
    "        \n",
    "        \"\"\"\n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1]) # X.shape[1] 은 X의 feature 갯수 => 즉 feature 개수+1 차원의 행벡터이자 평균 0에 표준편차 0.01의 random normal distribution 값을 원소로 갖고 있다.\n",
    "            # 짧게 말하면 weight 초기화다.\n",
    "            # feature 개수 + 1인 이유는 w_0 (절편값)이 필요하기 때문\n",
    "        self.errors_ = []\n",
    "        \n",
    "        for _ in range(self.n_iter):\n",
    "            errors = 0\n",
    "            for xi, target in zip(X, y):\n",
    "                update = self.eta * (target - self.predict(xi))\n",
    "                self.w_[1:] += update * xi\n",
    "                self.w_[0] += update\n",
    "                errors += int(update != 0.0)  # xi,target이 한번 돌 때마다 각각의 error값이 errors에 더해지고, errors 총합 값이 errors_ 리스트에 들어간다.\n",
    "            self.errors_.append(errors)\n",
    "        return self\n",
    "    \n",
    "    def net_input(self, X):\n",
    "        # 입력 계산\n",
    "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Unit step function을 이용해 클래스 레이블을 반환\n",
    "        return np.where(self.net_input(X) >= 0.0, 1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c9f03e",
   "metadata": {},
   "source": [
    "### 2.2.2 붓꽃 데이터셋에서 퍼셉트론 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dceb4c56",
   "metadata": {},
   "source": [
    "#### 붓꽃 데이터셋 읽기 / 그래프 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0001e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv('ch02/iris.data', header=None, encoding='utf-8')\n",
    "\n",
    "y = df.iloc[0:100, 4].values  # .values 안넣으면 pd.Series고, 넣으면 ndarray다.\n",
    "y = np.where(y == 'Iris-setosa', -1, 1)  #y = np.where(y == 'Iris-setosa', -1, 1)  # Setosa이면 -1, 아니면 1\n",
    "X = df.iloc[0:100, [0, 2]].values # 여긴 .values 안넣으면 pd.DataFrame이다.\n",
    "\n",
    "# scatter plot\n",
    "plt.scatter(X[:50, 0], X[:50, 1], color='red', marker='o', label='setosa')\n",
    "plt.scatter(X[50:100, 0], X[50:100, 1], color='blue', marker='x', label='versicolor')\n",
    "\n",
    "plt.xlabel('sepal length [cm]')\n",
    "plt.ylabel('petal length [cm]')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beb7c34",
   "metadata": {},
   "source": [
    "- 분포를 보니 선형 결정 경계로 구분할 수 있겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee4704e",
   "metadata": {},
   "source": [
    "#### 퍼셉트론 모델 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b7d5bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ppn = Perceptron(eta=0.1, n_iter=10)\n",
    "\n",
    "ppn.fit(X, y)\n",
    "\n",
    "plt.plot(range(1, len(ppn.errors_) + 1), ppn.errors_, marker='o')\n",
    "    # 1부터 errors_ 리스트의 갯수까지가 X label인데,\n",
    "        # errors_ 리스트는 epoch가 한 번 돌 때마다 추가되는 거니까, x축은 epoch다.\n",
    "        # ppn.errors_ 각 epoch 당 error의 총합\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Number of updates')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbc8b0d",
   "metadata": {},
   "source": [
    "- epoch 6 이후로 퍼셉트론은 수렴한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7521e0b7",
   "metadata": {},
   "source": [
    "#### 결정 경계 그래프 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a33014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_decision_regions(X, y, classifier, resolution=0.02):\n",
    "    \n",
    "    # 마커와 컬러맵을 설정한다\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "        # np.unique(y)는 y의 unique value를 리스트로 반환 => len은 unique value의 개수\n",
    "    \n",
    "    # 결정 경계를 그린다\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "        # 꽃받침 길이 최소/최대\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "        # 꽃잎 길이 최소/최대\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                          np.arange(x2_min, x2_max, resolution))\n",
    "        # meshgrid함수는 축에 해당하는 1차원 배열을 전달받아 벡터 공간의 모든 좌표를 담은 행렬을 반환한다.\n",
    "    \n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "            # ravel로 입력된 배열을 1차원으로 펼치고 => np.array로 두 배열을 묶어서 하나의 행렬로 만들고 => 이를 전치(T)하여 두 개의 열이 되도록 바꾼다.\n",
    "            # ! 이제 이 두 열이 xy평면의 좌표값이다.\n",
    "            # !! 그 각각의 xy좌표에 대해 predict를 수행하면 각각의 좌표마다 클래스예측값을 반환하는 거다.\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "            # !!! 그 각각의 예측값을 지닌 1차원 배열을 meshgrid shape으로 다시 바꿔준다.\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n",
    "    \"\"\"contourf\n",
    "    parameters\n",
    "    ------------\n",
    "    X, Y: array-like\n",
    "      the coordinates of the values in Z.\n",
    "    Z: array-like\n",
    "      the height values over which the contour is drawn.\n",
    "    \"\"\"\n",
    "    # Z의 shape을 xx1과 맞춰준 이유는..\n",
    "        # xx1의 각 원소가 x좌표, xx2의 각 원소가 y좌표일 때,\n",
    "        # Z의 각 원소는 각 x,y좌표의 'height value', 즉 class value가 되어야 하기 때문이다.\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "    \n",
    "    # 샘플의 산점도를 그린다\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0],\n",
    "                   y=X[y == cl, 1],\n",
    "                   alpha=0.8,\n",
    "                   c=colors[idx],\n",
    "                   marker=markers[idx],\n",
    "                   label=cl,\n",
    "                   edgecolor='black')\n",
    "        \n",
    "        # 위에서 X[y == cl, 0] 설명..\n",
    "            # X에서 y == cl인 것들 중 0열의 데이터를 x로 두고\n",
    "            # X에서 y == cl인 것들 중 1열의 데이터를 y로 둔다\n",
    "            # 이렇게 되면 결국 좌표를 찍는 거다.\n",
    "                # 클래스가 1인 feature 2개의 샘플데이터들을\n",
    "                # feature별로 x,y좌표를 주고 산점도 위에 찍는다.\n",
    "            # 똑같이 클래스가 -1인 feature 2개의 샘플데이터들을\n",
    "            # feature별로 x,y 좌표를 주고 산점도 위에 찍는다.\n",
    "\n",
    "X[y == 1, 0].shape\n",
    "\n",
    "Image(filename='ch02/meshgrid.png', width=500)\n",
    "\n",
    "plot_decision_regions(X, y, classifier=ppn)\n",
    "plt.xlabel('sepal length [cm]')\n",
    "plt.ylabel('petal length [cm]')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "\n",
    "resolution = 0.02\n",
    "xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                          np.arange(x2_min, x2_max, resolution))\n",
    "\n",
    "X[:, 1].max()\n",
    "\n",
    "np.arange(x1_min, x1_max, 0.02).shape\n",
    "\n",
    "xx1, xx2\n",
    "\n",
    "xx1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5045b9",
   "metadata": {},
   "source": [
    "## 2.3 적응형 선형 뉴런과 학습의 수렴\n",
    "- 적응형 선형 뉴런(ADAptive LInear NEuron, ADALINE)\n",
    "- Rosenblatt가 퍼셉트론을 발표한지 채 몇 년이 지나지 않아 Bernard Widrow와 Tedd Hoff가 아달린(Adaline)을 발표했다.\n",
    "    - 아달린은 퍼셉트론의 향상된 버전이다.\n",
    "    - 아달린은 연속 함수(continuous function)로 비용 함수를 정의하고 최소화하는 핵심 개념을 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637c4f89",
   "metadata": {},
   "source": [
    "> 아달린 규칙(aka 위드로우-호프 규칙)과 로젠블라트 퍼셉트론의 가장 큰 차이점은 가중치를 업데이트하는 데 퍼셉트론처럼 unit step function 대신 linear activation function을 쓴다는 것이다.\n",
    "\n",
    "아달린의 선형 활성화 함수는 단순한 항등 함수(identity function)다.<br><br>\n",
    "$$\\phi(z) = z$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59fa1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url='https://git.io/JtIbn', width=500) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8ff82f",
   "metadata": {},
   "source": [
    "> 아달린 알고리즘은 **진짜 클래스 레이블**과 **선형 활성화 함수의 실수 출력 값**을 비교하여 모델의 오차를 계산하고 가중치를 업데이트한다.\n",
    "\n",
    "> 퍼셉트론은 **진짜 클래스 레이블**과 **예측 클래스 레이블**을 비교한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb1eda4",
   "metadata": {},
   "source": [
    "### 2.3.1 경사 하강법으로 비용 함수 최소화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acd3c2c",
   "metadata": {},
   "source": [
    "목적 함수 == 비용 함수 == 손실 함수<br>\n",
    "\n",
    "> 아달린은 \"activation 출력\"과 \"진짜 클래스 레이블\" 사이의 **제곱 오차합(SSE)** 으로 가중치를 학습하기 위한 비용 함수 J를 정의한다. <br>\n",
    "\n",
    "$J(\\boldsymbol{w})=\\dfrac{1}{2}\\sum_i\\left(y^{(i)}-\\phi(z^{(i)})\\right)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed2f305",
   "metadata": {},
   "source": [
    "- 위에서 1/2 항은 가중치 파라미터에 대한 비용 함수 또는 손실 함수의 그라디언트를 간소하게 만들려고 편의상 추가한 것임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e712116",
   "metadata": {},
   "source": [
    "> unit step function 대신 continuous linear activation function을 사용하는 장점은 비용 함수가 미분 가능해진다는 것이다.\n",
    "\n",
    "> 또 다른 장점은 이 함수가 볼록 함수라는 것이다. => 경사 하강법을 사용할 수 있다는 말이다.\n",
    ">> \"비용 함수 $J(w)$의 그라디언트 $\\nabla J(w)$ 반대 방향으로 조금씩 가중치를 업데이트할 수 있다.\"<br>\n",
    "\n",
    "- 가중치 변화량 $\\Delta w$는 음수의 그라디언트에 학습률 $\\eta$를 곱한 것으로 정의한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86ac996",
   "metadata": {},
   "source": [
    "- 아래는 비용함수의 그라디언트를 계산하기 위해 각 가중치 $w_j$에 대한 편도 함수를 계산하는 과정이다.<br><br>\n",
    "$\\\n",
    "\\begin{align*}\n",
    "\\dfrac{\\partial J}{\\partial w_j}\\\n",
    "&=\\dfrac{\\partial}{\\partial w_j}\\dfrac{1}{2}\\sum_i\\left(y^{(i)}-\\phi(z^{(i)})\\right)^2 \\\\\n",
    "&=\\dfrac{1}{2}\\dfrac{\\partial}{\\partial w_j}\\sum_i\\left(y^{(i)}-\\phi(z^{(i)})\\right)^2 \\\\\n",
    "&=\\dfrac{1}{2}\\sum_i2\\left(y^{(i)}-\\phi(z^{(i)})\\right)\\dfrac{\\partial}{\\partial w_j}\\left(y^{(i)}-\\phi(z^{(i)})\\right) \\\\\n",
    "&=\\sum_i\\left(y^{(i)}-\\phi(z^{(i)})\\right)\\dfrac{\\partial}{\\partial w_j}\\left(y^{(i)}-\\sum(w_kx_k^{(i)})\\right) \\\\\n",
    "&=\\sum_i\\left(y^{(i)}-\\phi(z^{(i)})\\right)\\left(-x_j^{(i)}\\right) \\\\\n",
    "&=-\\sum_i\\left(y^{(i)}-\\phi(z^{(i)})\\right)x_j^{(i)} \\\\\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "$\\mathit\\Delta w_j=-\\eta\\dfrac{\\partial J}{\\partial w_j}=\\eta\\sum_i\\left(y^{(i)}-\\phi(z^{(i)})\\right)x_j^{(i)}$\n",
    "\n",
    "$\\boldsymbol{w} := \\boldsymbol{w} + \\mathit\\Delta \\boldsymbol{w}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b0ae72",
   "metadata": {},
   "source": [
    "### 2.3.2 파이썬으로 아달린 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b310c23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdalineGD(object):\n",
    "    \"\"\"적응형 선형 뉴런 분류기\n",
    "    \n",
    "    매개변수\n",
    "    -----------\n",
    "    eta : float\n",
    "      학습률 (0.0과 1.0 사이)\n",
    "    n_iter : int\n",
    "      훈련 데이터셋 반복 횟수\n",
    "    random_state : int\n",
    "      가중치 무작위 초기화를 위한 난수 생성기 시드\n",
    "      \n",
    "    속성\n",
    "    -----------\n",
    "    w_ : 1d-array\n",
    "      학습된 가중치\n",
    "    cost_ : list\n",
    "      에포크마다 누적된 비용 함수의 제곱합    \n",
    "    \"\"\"\n",
    "    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"훈련 데이터 학습\n",
    "        \n",
    "        매개변수\n",
    "        ---------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "        y : array-like, shape = [n_samples]\n",
    "        \n",
    "        \n",
    "        반환값\n",
    "        ---------\n",
    "        self : object\n",
    "        \"\"\"\n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])\n",
    "        self.cost_ = []\n",
    "        \n",
    "        for i in range(self.n_iter):\n",
    "            net_input = self.net_input(X)\n",
    "            # 이 코드의 활성화 함수는 항등함수이기 때문에 아무런 효과가 없다.\n",
    "            # 이 대신 'output = self.net_input(X)'로 바로 쓸 수 있다.\n",
    "            # 아래 활성화 함수는 개념적인 목적을 위해 만들었다.\n",
    "            output = self.activation(net_input)\n",
    "            errors = (y - output)\n",
    "            self.w_[1:] += self.eta * X.T.dot(errors)\n",
    "                # 이 식은 위에 있던 편도함수의 식과 같다.\n",
    "            self.w_[0] += self.eta * errors.sum()\n",
    "            cost = (errors**2).sum() / 2.0\n",
    "            self.cost_.append(cost)\n",
    "        return self\n",
    "    \n",
    "    def net_input(self, X):\n",
    "        \"\"\"최종 입력 계산\"\"\"\n",
    "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
    "    \n",
    "    def activation(self, X):\n",
    "        \"\"\"선형 활성화 계산\"\"\"\n",
    "        return X\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"단위 계단 함수를 사용하여 클래스 레이블을 반환한다\"\"\"\n",
    "        return np.where(self.activation(self.net_input(X)) >= 0.0, 1, -1)\n",
    "\n",
    "# 두 학습률, eta=0.01과 eta=0.0001에서 \"에포크 횟수 대비 비용 그래프\"\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "\n",
    "ada1 = AdalineGD(n_iter=10, eta=0.01).fit(X, y)\n",
    "ax[0].plot(range(1, len(ada1.cost_) + 1),\n",
    "          np.log10(ada1.cost_), marker='o')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('log(Sum-squared-error)')\n",
    "ax[0].set_title('Adaline - Learning rate 0.01')\n",
    "\n",
    "ada2 = AdalineGD(n_iter=10, eta=0.0001).fit(X, y)\n",
    "ax[1].plot(range(1, len(ada2.cost_) + 1), ada2.cost_, marker='o')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Sum-squared-error')\n",
    "ax[1].set_title('Adaline - Learning rate 0.0001')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c44930f",
   "metadata": {},
   "source": [
    "- 왼쪽의 경우 학습률이 너무 큰 것이다.\n",
    "    - 에포크가 지날수록 비용함수는 최소화되긴 커녕 오차가 점점 커진다. = 전역 최솟값을 지나친 것이다.\n",
    "- 오른쪽의 경우 비용은 꾸준히 감소하지만 학습률이 너무 작기 때문에 알고리즘이 전역 최솟값에 수렴하려면 아주 많은 에포크가 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a6653c",
   "metadata": {},
   "source": [
    "### 2.3.3 특성 스케일을 조정하여 경사 하강법 결과 향상"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf974427",
   "metadata": {},
   "source": [
    "> 경사 하강법은 특성 스케일을 조정하여 혜택을 볼 수 있는 많은 알고리즘 중 하나다.<br>\n",
    "\n",
    "- 표준화(standardization)은 데이터가 평균이 0이고 단위 분산을 갖는 표준정규분포가 되도록 만들어준다. => 경사 하강법 학습이 좀 더 빠르게 수렴된다.\n",
    "- j번째 특성을 표준화하는 것은 => j번째 특성의 모든 샘플에서 평균 $\\mu_j$를 빼고 표준편차 $\\sigma_j$로 나누는 것이다.<br>\n",
    "$$\\boldsymbol{x}'_j=\\dfrac{\\boldsymbol{x}_j-\\mu_j}{\\sigma_j}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec49d01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url='https://git.io/JtIbB', width=700) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e6f36c",
   "metadata": {},
   "source": [
    "- 왼쪽은 표준화하지 않은 데이터셋, 오른쪽은 표준화한 데이터셋이다.\n",
    "- global cost minimum에 도달하는 거리가 더 짧아진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f0ad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np를 활용해 표준화한다\n",
    "X_std = np.copy(X)\n",
    "X_std[:, 0] = (X[:, 0] - X[:, 0].mean()) / X[:, 0].std()\n",
    "X_std[:, 1] = (X[:, 1] - X[:, 1].mean()) / X[:, 1].std()\n",
    "\n",
    "# 다시 adalin 모델을 사용해본다\n",
    "ada_gd = AdalineGD(n_iter=15, eta=0.01)\n",
    "ada_gd.fit(X_std, y)\n",
    "\n",
    "plot_decision_regions(X_std, y, classifier=ada_gd)\n",
    "plt.title('Adaline - Gradient Descent')\n",
    "plt.xlabel('sepal length [standardized]')\n",
    "plt.ylabel('petal length [standardized]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(1, len(ada_gd.cost_) + 1), ada_gd.cost_, marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Sum-squared-error')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d66b075",
   "metadata": {},
   "source": [
    "### 2.3.4 대규모 머신 러닝과 확률적 경사 하강법 (Stochastic Gradient Descent)\n",
    "- 위에서 지금까지 한 것은 배치 경사 하강법(Batch Gradient Descent)다.\n",
    "    - 배치 방식이라는 건, \"훈련 데이터셋에 있는 **모든 샘플을 기반으로** 가중치 업데이트를 계산하는 것이다.\"\n",
    "    - 배치 방식의 경우 데이터셋의 규모가 아주 커지면 계산 비용이 매우 많이 든다. 전역 최솟값을 찾는 단계마다 매번 전체 훈련 데이터셋을 다시 평가해야 하기 때문이다.\n",
    "- 확률적 경사 하강법(aka 반복/온라인 경사 하강법)은 배치 처럼 모든 샘플 $x^{(i)}$에 대해 누적된 오차합을 기반으로 가중치를 업데이트하는 것이 아니라, **각 훈련 샘플에 대해 조금씩 가중치를 업데이트한다.**<br><br>\n",
    "    - 온라인 학습으로도 사용할 수 있다는 장점: 모델이 새로운 훈련 데이터가 도착하는 대로 실시간으로 훈련해야 할 때 사용하기 좋다.<br><br>\n",
    "    \n",
    "Batch Gradient: $\\mathit\\Delta \\boldsymbol{w}=\\eta\\sum_i\\left(y^{(i)}-\\phi(z^{(i)})\\right)\\boldsymbol{x}^{(i)}$<br>\n",
    "\n",
    "Stochastic Gradient: $\\mathit\\Delta \\boldsymbol{w}=\\eta\\left(y^{(i)}-\\phi(z^{(i)})\\right)\\boldsymbol{x}^{(i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2eef58",
   "metadata": {},
   "source": [
    "- 전체 훈련 샘플의 누적된 오차합이 아니라 **하나의 훈련 샘플을 기반으로 가중치가 계산된다.**\n",
    "- 가중치가 훨씬 더 자주 업데이트된다는 얘기고, 그렇기에 수렴 속도가 빠르다.\n",
    "- 중요한 점은..\n",
    "    - 훈련 샘플 순서를 무작위로 주입해야 하고,\n",
    "    - 순환되지 않도록 에포크마다 훈련 데이터셋을 섞어줘야 한다.<br>\n",
    "    \n",
    "> Stochastic에서는 종종 고정된 학습률 eta를 시간이 지남에 따라 적응적 학습률로 대체한다. (학습률을 n_iter에 따라 줄여나감으로써, 전역 최솟값에 더욱 근접할 수 있도록 해준다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4b2429",
   "metadata": {},
   "source": [
    "- Batch와 Stochastic의 절충안으로 **mini-batch learning**도 있다.\n",
    "    - 훈련 데이터의 작은 일부분으로 배치 경사 하강법을 적용하는 것이다. e.g. 한 번에 32개의 샘플을 사용하는 식이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00428cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdalineSGD(object):\n",
    "    \"\"\"ADAptive LInear NEuron 분류기\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    eta : float\n",
    "    n_iter : int\n",
    "    shuffle : bool (default: True)\n",
    "      True로 설정하면 같은 반복이 되지 않도록 에포크마다 훈련 데이터를 섞는다.\n",
    "    random_state : int\n",
    "    \n",
    "    Attributes\n",
    "    -------------\n",
    "    w_ : 1d-array\n",
    "      학습된 가중치\n",
    "    cost_ : list\n",
    "      모든 훈련 샘플에 대해 에포크마다 누적된 평균 비용 함수의 제곱합\n",
    "    \"\"\"\n",
    "    # 주제 벗어난 이야기지만, parameter로 받지 않았는데 self.에 입력시키는 attribute에는 underscore(_)를 붙인다.\n",
    "    def __init__(self, eta=0.01, n_iter=10, shuffle=True, random_state=None):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.w_initialized = False\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # epoch를 1번 돌때, 가중치 학습이 X.shape[0]번 만큼 일어나는 것이다.\n",
    "        self._initialize_weights(X.shape[1])\n",
    "        self.cost_ = []\n",
    "        for i in range(self.n_iter):\n",
    "            if self.shuffle: # class parameter의 default가 True다.\n",
    "                X, y = self._shuffle(X, y)\n",
    "            cost = []\n",
    "            for xi, target in zip(X, y):\n",
    "                cost.append(self._update_weights(xi, target))\n",
    "                # self._update_weights할 때 이미 weight update는 일어났다.\n",
    "                # 근데 _update_weights의 return값은 cost니까, 이걸\n",
    "                # cost에 append 한다. 이걸 샘플데이터 갯수만큼 반복하고\n",
    "                # 샘플데이터 갯수로 나눠주면 avg_cost, 즉 이번 epoch의\n",
    "                # avg_cost가 되는 것이다.\n",
    "            avg_cost = sum(cost) / len(y)\n",
    "            self.cost_.append(avg_cost)\n",
    "        return self\n",
    "    \n",
    "    def partial_fit(self, X, y):\n",
    "        \"\"\"가중치를 다시 초기화하지 않고 훈련 데이터를 학습한다.\n",
    "        (온라인 학습 활용)\"\"\"\n",
    "        if not self.w_initialized:\n",
    "            self._initialize_weights(X.shape[1])\n",
    "        if y.ravel().shape[0] > 1:\n",
    "            for xi, target in zip(X, y):\n",
    "                self._update_weights(xi, target)\n",
    "                # 2개 이상이면 zip으로 하나씩 빼고\n",
    "        else:\n",
    "            self._update_weights(X, y)\n",
    "                # 1개면 zip할 필요 없으니까 1개의 샘플을 가지고 그대로 _update_weights 한다.\n",
    "        return self\n",
    "    \n",
    "    def _shuffle(self, X, y):\n",
    "        \"\"\"훈련 데이터를 섞는다\"\"\"\n",
    "        r = self.rgen.permutation(len(y))  # self.rgen은 fit메서드의 앞 단계에서 _initialize_weights 하면서 생성되어 있다.\n",
    "        return X[r], y[r]\n",
    "    \n",
    "    def _initialize_weights(self, m): # m = X.shape[1] (feature 개수)\n",
    "        \"\"\"랜덤한 작은 수로 가중치를 초기화한다\"\"\"\n",
    "        self.rgen = np.random.RandomState(self.random_state)\n",
    "        self.w_ = self.rgen.normal(loc=0.0, scale=0.01, size=1 + m)\n",
    "        self.w_initialized = True\n",
    "    \n",
    "    def _update_weights(self, xi, target):\n",
    "        \"\"\"아달린 학습 규칙을 적용하여 가중치를 업데이트한다\"\"\"\n",
    "        output = self.activation(self.net_input(xi))\n",
    "        error = (target - output)\n",
    "        self.w_[1:] += self.eta * xi.dot(error)\n",
    "        self.w_[0] += self.eta * error\n",
    "        cost = 0.5 * error**2\n",
    "        return cost\n",
    "    \n",
    "    def net_input(self, X):\n",
    "        \"\"\"입력 계산\"\"\"\n",
    "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
    "    \n",
    "    def activation(self, X):\n",
    "        \"\"\"선형 활성화 계산\"\"\"\n",
    "        return X\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"단위 계단 함수를 사용하여 클래스 레이블을 반환\"\"\"\n",
    "        return np.where(self.activation(self.net_input(X)) >= 0.0, 1, -1)\n",
    "\n",
    "# visualize the result\n",
    "ada_sgd = AdalineSGD(n_iter=15, eta=0.01, random_state=1)\n",
    "ada_sgd.fit(X_std, y)\n",
    "\n",
    "plot_decision_regions(X_std, y, classifier=ada_sgd)\n",
    "plt.title('Adaline - Stochastic Gradient Descent')\n",
    "plt.xlabel('sepal length [standardized]')\n",
    "plt.ylabel('petal length [standardized]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(1, len(ada_sgd.cost_) + 1), ada_sgd.cost_, marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Average Cost')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 온라인 학습 방식으로 모델을 훈련하려면 개개의 샘플마다\n",
    "    # partial_fit 메서드를 호출하면 된다.\n",
    "ada_sgd.partial_fit(X_std[0, :], y[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6a6ab0",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Perceptron\" data-toc-modified-id=\"Perceptron-1\">Perceptron</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#퍼셉트론의-기본-수식\" data-toc-modified-id=\"퍼셉트론의-기본-수식-1.0.0.1\">퍼셉트론의 기본 수식</a></span></li><li><span><a href=\"#퍼셉트론의-가중치-업데이트-수식\" data-toc-modified-id=\"퍼셉트론의-가중치-업데이트-수식-1.0.0.2\">퍼셉트론의 가중치 업데이트 수식</a></span></li></ul></li></ul></li><li><span><a href=\"#활성함수의-종류-(Activation-Function)\" data-toc-modified-id=\"활성함수의-종류-(Activation-Function)-1.1\">활성함수의 종류 (Activation Function)</a></span></li><li><span><a href=\"#손실-함수-(Loss-Function)\" data-toc-modified-id=\"손실-함수-(Loss-Function)-1.2\">손실 함수 (Loss Function)</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Mean-Squared-Error\" data-toc-modified-id=\"Mean-Squared-Error-1.2.0.1\">Mean Squared Error</a></span></li><li><span><a href=\"#Mean-Absolute-Error\" data-toc-modified-id=\"Mean-Absolute-Error-1.2.0.2\">Mean Absolute Error</a></span></li><li><span><a href=\"#Cross-Entropy-Error\" data-toc-modified-id=\"Cross-Entropy-Error-1.2.0.3\">Cross Entropy Error</a></span></li></ul></li></ul></li><li><span><a href=\"#Perceptron-구현-실습\" data-toc-modified-id=\"Perceptron-구현-실습-1.3\">Perceptron 구현 실습</a></span></li></ul></li><li><span><a href=\"#최적화-이론\" data-toc-modified-id=\"최적화-이론-2\">최적화 이론</a></span><ul class=\"toc-item\"><li><span><a href=\"#Gradient-Descent-최적화-구현\" data-toc-modified-id=\"Gradient-Descent-최적화-구현-2.1\">Gradient Descent 최적화 구현</a></span></li><li><span><a href=\"#Gradient-Descent-Visualization\" data-toc-modified-id=\"Gradient-Descent-Visualization-2.2\">Gradient Descent Visualization</a></span></li><li><span><a href=\"#역전파-알고리즘\" data-toc-modified-id=\"역전파-알고리즘-2.3\">역전파 알고리즘</a></span></li><li><span><a href=\"#from-scratch-수치-미분을-이용한-심층-신경망-학습\" data-toc-modified-id=\"from-scratch-수치-미분을-이용한-심층-신경망-학습-2.4\">from scratch 수치 미분을 이용한 심층 신경망 학습</a></span></li><li><span><a href=\"#오류역전파-이용한-심층신경망-from-scratch\" data-toc-modified-id=\"오류역전파-이용한-심층신경망-from-scratch-2.5\">오류역전파 이용한 심층신경망 from scratch</a></span><ul class=\"toc-item\"><li><span><a href=\"#하이퍼파라미터-최적화-기법\" data-toc-modified-id=\"하이퍼파라미터-최적화-기법-2.5.1\">하이퍼파라미터 최적화 기법</a></span></li></ul></li><li><span><a href=\"#뉴럴-네트워크-학습-알고리즘-구현\" data-toc-modified-id=\"뉴럴-네트워크-학습-알고리즘-구현-2.6\">뉴럴 네트워크 학습 알고리즘 구현</a></span></li><li><span><a href=\"#Overfitting-방지-실습\" data-toc-modified-id=\"Overfitting-방지-실습-2.7\">Overfitting 방지 실습</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4525419",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T16:15:18.144007Z",
     "start_time": "2021-09-18T16:15:18.133749Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b7fc11",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6132ede9",
   "metadata": {},
   "source": [
    "#### 퍼셉트론의 기본 수식\n",
    "$$ y = sign \\bigg(\\sum^{N-1}_{i=0} x_i \\cdot w_i \\bigg) $$\n",
    " \n",
    "$$ sign(t) =\n",
    "\\begin{cases}\n",
    "    +1, & \\text{if t > 0} \\\\\n",
    "    -1, & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "#### 퍼셉트론의 가중치 업데이트 수식\n",
    "\n",
    "$$ w_{t+1} = w_t + \\eta(y-\\tilde y)x $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fdd1e3",
   "metadata": {},
   "source": [
    "## 활성함수의 종류 (Activation Function)\n",
    "\n",
    "$$ tanh(x) = \\dfrac{e^x - e^{-x}}{e^x + e^{-x}} $$\n",
    "- -1 과 1 사이 실수값을 가짐 (이를 soft decision이라고 함. Hard decision은 예를 들어 0아니면 1을 출력하는 sign function)\n",
    "- 미분 값이 0에서 1 사이임\n",
    "- 입력값이 0에 가까울수록 출력이 빠르게 변함\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$$ sigmoid(x) = \\dfrac{1}{1 + e^{-x}} $$\n",
    "- 0에서 1사이 실수값을 가짐 => 그래서 '확률'로서 표현이 가능\n",
    "- 입력값이 0에 가까울수록 출력이 빠르게 변함\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$$ softmax(x)_i = \\dfrac{e^{x_i}}{\\sum_j e^{x_j}} $$\n",
    "- tanh과 sigmoid가 binary classification인 것과 다르게, softmax는 multi-class classification에 쓰임\n",
    "- 각 입력의 지수함수를 정규화한 것. (모든 출력의 합이 1이 되게 하는 것)\n",
    "- 각 출력은 0~1 사이 값을 가짐 => 여러 경우의 수 중 한 가지에 속할 '확률'을 표현\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$$ softmax([x,0])_0 = \\dfrac{e^x}{e^x + e^0} = \\dfrac{1}{1 + e^{-x}} $$\n",
    "- Sigmoid는 하나의 입력을 0으로 강제한 2-class softmax 함수와 같다\n",
    "    - 2가지 클래스를 구분하기 위해 1개의 입력을 받는 형태\n",
    "- 두번째 식에서 분모분자를 $e^x$로 나눠준 형태다.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$$ ReLU(x) = max(0, x) $$\n",
    "- 0보다 작은 값을 0으로 강제하는 함수\n",
    "- 미분 값이 일정(0 또는 1)해서 학습이 잘 되는 특성\n",
    "- 단순한 구현으로 매우 빠른 연산이 가능\n",
    "- 딥러닝에서 가장 많이 사용됌"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04264880",
   "metadata": {},
   "source": [
    "## 손실 함수 (Loss Function)\n",
    "> aka. cost function, 목적 함수, 에너지 함수\n",
    "\n",
    "- 보통 미분 가능한 함수 사용\n",
    "- 최적화 이론에서 '최소화 하고자 하는 함수'\n",
    "- 성능 척도(performance measure/metrics)와 다른 점은.. 성능 척도는 학습 과정에서 사용되는 게 아니라 \"정량적으로 알고리즘을 비교/평가\"하기 위해 사용.\n",
    "    - 또한.. 미분가능한 함수인지는 상관하지 않고, business priority/issue에 따라 성능척도를 정함\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$$ \\tilde \\theta = \\arg\\min_{\\theta} L(x, y; \\theta) $$\n",
    "- $L$: 손실함수\n",
    "- $x$: 학습 데이터 입력\n",
    "- $y$: 학습 데이터 정답\n",
    "- $\\theta$: 학습될 모든 파라미터를 모은 벡터\n",
    "- $\\tilde \\theta$: 추정된 최적의 파라미터\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "#### Mean Squared Error\n",
    "$$ MSE = \\dfrac{1}{2}\\sum_i (y_i - \\tilde y_i)^2 $$\n",
    "- 1\\2를 앞에 써주는 거는 관습적인 건데, 미분했을 때 앞의 1/2이 1이 되도록 하기 위해 넣어주는 거다.\n",
    "\n",
    "#### Mean Absolute Error\n",
    "$$ MAE = \\sum_i |y_i - \\tilde y_i | $$\n",
    "- outlier에 robust한 특징이 있다.\n",
    "    - MSE는 오차 제곱이기 때문에 오차가 커질수록 손실함수가 빠르게 증가하는데, MAE는 오차가 커져도 손실함수가 일정하게 증가한다.\n",
    "    - 통계적으로 '중간값'과 연관이 있다.\n",
    "    \n",
    "#### Cross Entropy Error\n",
    "$$ CEE = - \\sum_i y_i \\log \\tilde y_i $$\n",
    "- multi-class classification, 예를 들어 원핫인코딩 된 레이블을 예측할 때 사용한다.\n",
    "- 원핫인코딩 된 레이블 $y_i$가 곱해져 있기 때문에, 정답이 아닌 클래스(0)의 경우는 무시한다.\n",
    "    - 정답인 클래스(1)에 대해서만 $ - \\log \\tilde y_i $ 를 반환\n",
    "- 정확히 맞추면, 즉 $\\tilde y$가 1이면 오차가 0, 틀릴수록 오차가 무한히 증가하는 특징이 있다.\n",
    "- 오차를 내는 과정에서는 정답 클래스만 비교하지만, 파라미터를 업데이트하면 다른 클래스에 대한 확률에도 영향을 준다. 왜냐면! softmax function은 각 클래스에 대한 확률 총합이 1이니까!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5abcb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T16:15:22.192613Z",
     "start_time": "2021-09-18T16:15:22.173201Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Image(filename='img/deep_xor.png', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2c1838",
   "metadata": {},
   "source": [
    "- NN으로 XOR문제를 푼다는 게 이런 거다. NAND와 OR을 구해놓고, 그걸 AND로 다시 연산하면 XOR이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b626e57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6cd6dc6",
   "metadata": {},
   "source": [
    "## Perceptron 구현 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80e0061",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T16:15:27.891149Z",
     "start_time": "2021-09-18T16:15:25.560635Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# perceptron class\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, w, b):\n",
    "        # 외부에서 입력 받는 numpy값을 tensor로 변환\n",
    "        self.w = tf.Variable(w, dtype=tf.float32)\n",
    "        self.b = tf.Variable(b, dtype=tf.float32)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # 내적 값을 구해서 scalar로 만들어 주고, sign function을 통과시킨다.\n",
    "        return tf.sign(tf.reduce_sum(self.w * x) + self.b)\n",
    "\n",
    "# utility 함수 구현\n",
    "def v(*args):\n",
    "    return np.array(args)\n",
    "\n",
    "# perceptron 정의\n",
    "w = v(1, 1)\n",
    "b = 0.5\n",
    "\n",
    "perceptron = Perceptron(w, b)\n",
    "\n",
    "# OR 구현\n",
    "p1 = perceptron(v(1, 1))  # T, T\n",
    "p2 = perceptron(v(-1, 1))  # F, T\n",
    "p3 = perceptron(v(-1, -1))  # F, F\n",
    "p4 = perceptron(v(1, -1))  # T, F\n",
    "\n",
    "print(p2.numpy(), p1.numpy())\n",
    "print(p3.numpy(), p4.numpy())\n",
    "\n",
    "# XOR 구현\n",
    "\n",
    "p_nand = Perceptron(w=v(-1, -1),\n",
    "                    b=0.5)\n",
    "\n",
    "p_or = Perceptron(w=v(1, 1),\n",
    "                    b=0.5)\n",
    "\n",
    "p_and = Perceptron(w=v(1, 1),\n",
    "                    b=-0.5)\n",
    "\n",
    "def xor(x):\n",
    "    h1 = p_nand(x)\n",
    "    h2 = p_or(x)\n",
    "    return p_and(v(h1, h2))\n",
    "\n",
    "p1 = xor(v(1, 1)) # T, T\n",
    "p2 = xor(v(-1, 1)) # F, T\n",
    "p3 = xor(v(-1, -1)) # F, F\n",
    "p4 = xor(v(1, -1)) # T, F\n",
    "\n",
    "print(p2.numpy(), p1.numpy())\n",
    "print(p3.numpy(), p4.numpy())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ca2b55",
   "metadata": {},
   "source": [
    "# 최적화 이론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee83f11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T16:15:30.053942Z",
     "start_time": "2021-09-18T16:15:30.044014Z"
    }
   },
   "outputs": [],
   "source": [
    "Image(filename='img/deep_opt.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004cfb81",
   "metadata": {},
   "source": [
    "- $f(x)$ 를 최소가 되게 하는 x를 구하는 문제.\n",
    "    - 다만 어떠어떠한 제약 조건 하에서.. (딥러닝에서는 보통 제약조건이 없기 때문에 신경 안써도 됌)\n",
    "- 최대화 문제이면 $-f(x)$로 설정하면 됌."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2659c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T16:15:30.429870Z",
     "start_time": "2021-09-18T16:15:30.418686Z"
    }
   },
   "outputs": [],
   "source": [
    "Image(filename='img/deep_opt2.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13f5d5f",
   "metadata": {},
   "source": [
    "- Numerical method 중 가장 많이 쓰이는 게 \"Gradient Descent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6789b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c6b753",
   "metadata": {},
   "source": [
    "## Gradient Descent 최적화 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8b6039",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T16:15:31.401762Z",
     "start_time": "2021-09-18T16:15:31.396449Z"
    }
   },
   "outputs": [],
   "source": [
    "# 손실함수 정의 (analytic)\n",
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    return 0.1*x**4 - 1.5*x**3 + 0.6*x**2 + 1.0*x + 20.0\n",
    "\n",
    "# 손실함수 미분 정의\n",
    "def df_dx(x):\n",
    "    return 0.4*x**3 - 4.5*x**2 + 1.2*x + 1.0\n",
    "\n",
    "# 하이퍼파라미터 정의\n",
    "x = 5\n",
    "eps = 1e-5\n",
    "lr = 0.01\n",
    "max_epoch = 1000\n",
    "\n",
    "# gradient descent algorithm 구현\n",
    "\n",
    "def gradient_descent(initial_x, max_epoch=1000, learning_rate=0.01, eps=1e-5):\n",
    "    x = initial_x\n",
    "    lr = learning_rate\n",
    "    \n",
    "    x_log = [x] # x의 발자취 확인\n",
    "    \n",
    "    min_x = x  # 손실함수를 가장 작게 만드는 x 값을 구해야 한다\n",
    "    min_y = f(min_x)\n",
    "\n",
    "    for _ in range(max_epoch):\n",
    "        grad = df_dx(x)\n",
    "        new_x = x - lr * grad\n",
    "        y = f(new_x)\n",
    "        \n",
    "        x_log.append(new_x) # x의 발자취 확인\n",
    "\n",
    "        if min_y > y:\n",
    "            min_x = new_x\n",
    "            min_y = y\n",
    "\n",
    "        if np.abs(x - new_x) < eps:\n",
    "            break\n",
    "\n",
    "        x = new_x\n",
    "    return min_x, min_y, x_log\n",
    "\n",
    "gradient_descent(5)[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81258300",
   "metadata": {},
   "source": [
    "- 위에서 초기 x값을 -5로 하게 되면 값이 완전히 다르게 나온다.\n",
    "    - **vanilla Gradient descent는 구한 minimum이 local인지 global인지 알 수가 없다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dbd26b",
   "metadata": {},
   "source": [
    "## Gradient Descent Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd27d8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T16:15:32.845435Z",
     "start_time": "2021-09-18T16:15:32.700565Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "min_x1, min_y1, x_log1 = gradient_descent(5)\n",
    "min_x2, min_y2, x_log2 = gradient_descent(-5)\n",
    "\n",
    "y_log1 = f(np.array(x_log1))\n",
    "y_log2 = f(np.array(x_log2))\n",
    "\n",
    "plt.plot(x_log1, y_log1, '.-', label='init +5')\n",
    "plt.plot(x_log2, y_log2, '.-', label='init -5')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de30c13",
   "metadata": {},
   "source": [
    "## 역전파 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47646bfc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T16:15:33.442859Z",
     "start_time": "2021-09-18T16:15:33.426777Z"
    }
   },
   "outputs": [],
   "source": [
    "Image(filename='img/backpro.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3512898e",
   "metadata": {},
   "source": [
    "- output 바로 전 layer의 파라미터를 수정할 때, 필요한 미분을 구해서 저장해놓고... 한 단계 더 앞으로 나아갈 때는 저장해 둔 미분을 그대로 사용하고, 그렇게 하는 거다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f979fb9e",
   "metadata": {},
   "source": [
    "## from scratch 수치 미분을 이용한 심층 신경망 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3366e988",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T16:15:33.993740Z",
     "start_time": "2021-09-18T16:15:33.986910Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# utillity\n",
    "def _t(x):\n",
    "    return np.transpose(x)\n",
    "\n",
    "def _m(A, B):\n",
    "    return np.matmul(A, B)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def mean_squared_error(h, y):\n",
    "    return 1 / 2 * np.mean(np.square(h - y))\n",
    "\n",
    "epsilon = 0.0001\n",
    "\n",
    "# Dense Layer 구현\n",
    "class Dense:\n",
    "    def __init__(self, W, b, a):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.a = a\n",
    "        \n",
    "        self.dW = np.zeros_like(self.W)  # 각각의 w에 대한 미분값을 저장할 ndarray\n",
    "        self.db = np.zeros_like(self.b)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.a(_m(_t(self.W), x) + self.b)\n",
    "        # weight matrix를 \"input by output\"으로 해주면 transpose해줘야 된다.\n",
    "\n",
    "# DNN 구현\n",
    "class DNN:\n",
    "    def __init__(self, hidden_depth, num_neuron, num_input, num_output, activation=sigmoid):\n",
    "        def init_var(i, o):\n",
    "            return np.random.normal(0.0, 0.01, (i, o)), np.zeros((o,))\n",
    "        \n",
    "        self.sequence = list()\n",
    "        \n",
    "        # first hidden layer\n",
    "        # weight initialization = 처음 입력 개수 by 첫번째 레이어 뉴런 개수\n",
    "        W, b = init_var(num_input, num_neuron)\n",
    "        self.sequence.append(Dense(W, b, activation))\n",
    "        # dense layer를 sequence에 append\n",
    "        \n",
    "        # hidden layers\n",
    "        for _ in range(hidden_depth - 1):\n",
    "            W, b = init_var(num_neuron, num_neuron)\n",
    "            self.sequence.append(Dense(W, b, activation))\n",
    "        \n",
    "        # output layer\n",
    "        W, b = init_var(num_neuron, num_output)\n",
    "        self.sequence.append(Dense(W, b, activation))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for layer in self.sequence:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "    def calc_gradient(self, x, y, loss_func):\n",
    "        # 기존 시퀀스에서 원하는 layer 하나만 새로운 layer로 변경!\n",
    "        def get_new_sequence(layer_index, new_layer):\n",
    "            new_sequence = list()\n",
    "            for i, layer in enumerate(self.sequence):\n",
    "                if i == layer_index:\n",
    "                    new_sequence.append(new_layer)\n",
    "                else:\n",
    "                    new_sequence.append(layer)\n",
    "            \n",
    "            return new_sequence\n",
    "        \n",
    "        # call을 쓰면 self.sequence를 eval할 수밖에 없기 때문에, eval_sequence 함수를 추가로 정의\n",
    "        def eval_sequence(x, sequence):\n",
    "            for layer in sequence:\n",
    "                x = layer(x)\n",
    "            return x\n",
    "        \n",
    "        \n",
    "        \n",
    "        loss = loss_func(self(x), y)\n",
    "        # self(x)는 위의 call을 불러와서 x 입력받은 걸 출력으로 내주고 정답과 비교해서 loss를 반환한다\n",
    "        \n",
    "        # 각 스칼라들을 바꿔가면서 위의 loss와 비교를 해줘야 한다.\n",
    "            # ww와 bb는 스칼라\n",
    "        for layer_id, layer in enumerate(self.sequence):\n",
    "            # 행을 먼저 돌고\n",
    "            for w_i, w in enumerate(layer.W):\n",
    "                # 열을 돌면서 인덱스 반환\n",
    "                for w_j, ww in enumerate(w):\n",
    "                    W = np.copy(layer.W)\n",
    "                    # i행의 j열의 weight에 epsilon 더해준다\n",
    "                    W[w_i][w_j] = ww + epsilon\n",
    "                    \n",
    "                    # 기존 bias와 activation 사용\n",
    "                    new_layer = Dense(W, layer.b, layer.a)\n",
    "                    # 기존 sequence에 위의 layer로 replace\n",
    "                    new_seq = get_new_sequence(layer_id, new_layer)\n",
    "                    h = eval_sequence(x, new_seq)\n",
    "                    \n",
    "                    # (f(x+eps) - f(x)) / eps\n",
    "                    num_grad = (loss_func(h, y) - loss) / epsilon\n",
    "                    layer.dW[w_i][w_j] = num_grad\n",
    "                    \n",
    "            for b_i, bb in enumerate(layer.b):\n",
    "                b = np.copy(layer.b)\n",
    "                b[b_i] = bb + epsilon\n",
    "                \n",
    "                new_layer = Dense(layer.W, b, layer.a)\n",
    "                new_seq = get_new_sequence(layer_id, new_layer)\n",
    "                h = eval_sequence(x, new_seq)\n",
    "                \n",
    "                num_grad = (loss_func(h, y) - loss) / epsilon\n",
    "                layer.db[b_i] = num_grad\n",
    "                \n",
    "        return loss\n",
    "\n",
    "def gradient_descent(network, x, y, loss_obj, alpha=0.01):\n",
    "    loss = network.calc_gradient(x, y, loss_obj)\n",
    "    for layer in network.sequence:\n",
    "        layer.W += -alpha * layer.dW\n",
    "        layer.b += -alpha * layer.db\n",
    "    return loss\n",
    "\n",
    "x = np.random.normal(0.0, 1.0, (10,))\n",
    "y = np.random.normal(0.0, 1.0, (2,))\n",
    "\n",
    "dnn = DNN(hidden_depth=10, num_neuron=32, num_input=10, num_output=2, activation=sigmoid)\n",
    "\n",
    "t = time.time()\n",
    "for epoch in range(100):\n",
    "    loss = gradient_descent(dnn, x, y, mean_squared_error, 0.01)\n",
    "    print('Epoch {}: Test loss {}'.format(epoch, loss))\n",
    "print('{} seconds elapsed.'.format(time.time() - t))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830fc571",
   "metadata": {},
   "source": [
    "## 오류역전파 이용한 심층신경망 from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdde252c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T16:16:25.303595Z",
     "start_time": "2021-09-18T16:16:25.301514Z"
    }
   },
   "outputs": [],
   "source": [
    "# sigmoid class\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.last_o = 1\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.last_o = 1.0 / (1.0 + np.exp(-x))\n",
    "        return self.last_o\n",
    "\n",
    "    def grad(self): # sigmoid(x)(1 - sigmoid(x))\n",
    "        return self.last_o * (1.0 - self.last_o)\n",
    "\n",
    "# mse class\n",
    "class MeanSquaredError: # 1/2 * mean((h - y)^2)  --> h - y\n",
    "    def __init__(self):\n",
    "        self.dh = 1\n",
    "        self.last_diff = 1\n",
    "\n",
    "    def __call__(self, h, y):\n",
    "        self.last_diff = h - y\n",
    "        return 1 / 2 * np.mean(np.square(self.last_diff))\n",
    "\n",
    "    def grad(self):\n",
    "        return self.last_diff\n",
    "\n",
    "# DNN\n",
    "class Dense:\n",
    "    def __init__(self, W, b, a_obj):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.a = a_obj()\n",
    "        \n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "        self.dh = np.zeros_like(_t(self.W))\n",
    "        \n",
    "        self.last_x = np.zeros((self.W.shape[0]))\n",
    "        self.last_h = np.zeros((self.W.shape[1]))\n",
    "        \n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.last_x = x\n",
    "        self.last_h = _m(_t(self.W), x) + self.b\n",
    "        return self.a(self.last_h)\n",
    "\n",
    "    def grad(self): # dy/dh = W\n",
    "        return self.W * self.a.grad()\n",
    "\n",
    "    def grad_W(self, dh):\n",
    "        grad = np.ones_like(self.W)\n",
    "        grad_a = self.a.grad()\n",
    "        for j in range(grad.shape[1]): # dy/dw = x\n",
    "            grad[:, j] = dh[j] * grad_a[j] * self.last_x\n",
    "        return grad\n",
    "\n",
    "    def grad_b(self, dh): # dy/db = 1\n",
    "        return dh * self.a.grad()\n",
    "\n",
    "class DNN:\n",
    "    def __init__(self, hidden_depth, num_neuron, input, output, activation=Sigmoid):\n",
    "        def init_var(i, o):\n",
    "            return np.random.normal(0.0, 0.01, (i, o)), np.zeros((o,))\n",
    "\n",
    "        self.sequence = list()\n",
    "        # First hidden layer\n",
    "        W, b = init_var(input, num_neuron)\n",
    "        self.sequence.append(Dense(W, b, activation))\n",
    "\n",
    "        # Hidden Layers\n",
    "        for index in range(hidden_depth):\n",
    "            W, b = init_var(num_neuron, num_neuron)\n",
    "            self.sequence.append(Dense(W, b, activation))\n",
    "\n",
    "        # Output Layer\n",
    "        W, b = init_var(num_neuron, output)\n",
    "        self.sequence.append(Dense(W, b, activation))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.sequence:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def calc_gradient(self, loss_obj):\n",
    "        loss_obj.dh = loss_obj.grad()\n",
    "        self.sequence.append(loss_obj)\n",
    "        \n",
    "        # back-prop loop\n",
    "        for i in range(len(self.sequence) - 1, 0, -1):\n",
    "            l1 = self.sequence[i]\n",
    "            l0 = self.sequence[i - 1]\n",
    "            \n",
    "            l0.dh = _m(l0.grad(), l1.dh)\n",
    "            l0.dW = l0.grad_W(l1.dh)\n",
    "            l0.db = l0.grad_b(l1.dh)\n",
    "        \n",
    "        self.sequence.remove(loss_obj)\n",
    "\n",
    "def gradient_descent(network, x, y, loss_obj, alpha=0.01):\n",
    "    loss = loss_obj(network(x), y)  # Forward inference\n",
    "    network.calc_gradient(loss_obj)  # Back-propagation\n",
    "    for layer in network.sequence:\n",
    "        layer.W += -alpha * layer.dW\n",
    "        layer.b += -alpha * layer.db\n",
    "    return loss\n",
    "\n",
    "x = np.random.normal(0.0, 1.0, (10,))\n",
    "y = np.random.normal(0.0, 1.0, (2,))\n",
    "\n",
    "t = time.time()\n",
    "dnn = DNN(hidden_depth=5, num_neuron=32, input=10, output=2, activation=Sigmoid)\n",
    "loss_obj = MeanSquaredError()\n",
    "for epoch in range(100):\n",
    "    loss = gradient_descent(dnn, x, y, loss_obj, alpha=0.01)\n",
    "    print('Epoch {}: Test loss {}'.format(epoch, loss))\n",
    "print('{} seconds elapsed.'.format(time.time() - t))\n",
    "\n",
    "\n",
    "\n",
    "Image(filename='img/hyper.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6a4bfe",
   "metadata": {},
   "source": [
    "### 하이퍼파라미터 최적화 기법\n",
    "- Grid Search\n",
    "- Random Search\n",
    "- Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c9feb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14d13a73",
   "metadata": {},
   "source": [
    "## 뉴럴 네트워크 학습 알고리즘 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45927dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T16:16:25.375503Z",
     "start_time": "2021-09-18T16:16:25.374001Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# hyperparameters\n",
    "EPOCHS = 10\n",
    "\n",
    "# network structure\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten(input_shape=(28,28))\n",
    "        self.dense1 = tf.keras.layers.Dense(32, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.dense3 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.dense4 = tf.keras.layers.Dense(256, activation='relu')\n",
    "        # 지금 사용할 MNIST 데이터셋은 클래스 레이블이 10개, 그리고 multi-class classification에 쓰이는 activation func는 softmax로 정해져 있음\n",
    "        self.dense5 = tf.keras.layers.Dense(10, activation='softmax')\n",
    "    \n",
    "    def call(self, x, training=None, mask=None):\n",
    "        # __init__ 에서 사용할 input, layers, output을 정의해주고,\n",
    "        # __call__ 에서 어떻게 사용할건지 순서를 정해주면 된다.\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.dense4(x)\n",
    "        return self.dense5(x)\n",
    "\n",
    "# 학습 함수 구현\n",
    "\n",
    "# tf function 데코레이터 활용하면, 아래 연산들이 모두 tf에 최적화된 형태(autograph)로 돌아감\n",
    "@tf.function\n",
    "def train_step(model, images, labels, loss_object, optimizer, train_loss, train_accuracy):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images)\n",
    "        # 예를 들어.. images에 size=32인 batch가 들어왔다면,\n",
    "        # predictions는 32x10 형태가 될 것이다. (batchsize x classes)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "\n",
    "# 테스트 함수 구현\n",
    "@tf.function\n",
    "def test_step(model, images, labels, loss_object, test_loss, test_accuracy):\n",
    "    predictions = model(images)\n",
    "    loss = loss_object(labels, predictions)\n",
    "    \n",
    "    test_loss(loss)\n",
    "    test_accuracy(labels, predictions)\n",
    "\n",
    "# 데이터 불러오기\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(1024).batch(32)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\n",
    "\n",
    "model = MyModel()\n",
    "\n",
    "y_train\n",
    "\n",
    "# 손실함수 및 최적화 알고리즘 정의\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "# 위의 y_train 데이터 형태를 sparse representation이라고도 부른다.\n",
    "# predictions을 하면, 우리는 batch size x classes 의 행렬을 얻는데, 만약 y_train이 똑같이 batch size x classes(one-hot-encoded) 였으면, 그냥 CategoricalCrossentropy를 쓰면 되는거다.\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# 성능 지표\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "# 학습 루프 구현\n",
    "for epoch in range(EPOCHS):\n",
    "    for images, labels in train_ds:\n",
    "        train_step(model, images, labels, loss_object, optimizer, train_loss, train_accuracy)\n",
    "    \n",
    "    for images, labels in test_ds:\n",
    "        test_step(model, images, labels, loss_object, test_loss, test_accuracy)\n",
    "        \n",
    "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "    print(template.format(epoch + 1,\n",
    "                          train_loss.result(),\n",
    "                          train_accuracy.result() * 100,\n",
    "                          test_loss.result(),\n",
    "                          test_accuracy.result() * 100))\n",
    "    \n",
    "    train_loss.reset_state()\n",
    "    train_accuracy.reset_state()\n",
    "    test_loss.reset_state()\n",
    "    test_accuracy.reset_state()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a0c330",
   "metadata": {},
   "source": [
    "## Overfitting 방지 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e24aef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T16:17:45.196916Z",
     "start_time": "2021-09-18T16:17:45.194988Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "EPOCHS = 50\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()   # 28x28 = 784\n",
    "        self.dense1 = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.dense3 = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.dense4 = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.dense5 = tf.keras.layers.Dense(10, activation='softmax')  \n",
    "        # 10\n",
    "\n",
    "    def call(self, x, training=False, mask=None):\n",
    "        h = self.flatten(x)\n",
    "        h = self.dense1(h)\n",
    "        h = self.dense2(h)\n",
    "        h = self.dense3(h)\n",
    "        h = self.dense4(h)\n",
    "        return self.dense5(h)\n",
    "\n",
    "# 데이터 불러오기\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_test = x_test.astype(np.float32)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32).prefetch(2048)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32).prefetch(2048)\n",
    "\n",
    "model = MyModel()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_ds, validation_data=test_ds, epochs=EPOCHS)\n",
    "\n",
    "# visualization\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss, 'ro-')\n",
    "plt.plot(val_loss, 'bo-')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "# Early stopping callback\n",
    "earlystopper = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "\n",
    "history = model.fit(train_ds, validation_data=test_ds, epochs=EPOCHS, callbacks=[earlystopper])\n",
    "\n",
    "# visualization\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss, 'ro-')\n",
    "plt.plot(val_loss, 'bo-')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "# DROPOUT\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()   # 28x28 = 784\n",
    "        self.dense1 = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.dense3 = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.dense4 = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.dropout4 = tf.keras.layers.Dropout(0.5) # dropout은 보통 뒤 쪽 레이어에 적용한다\n",
    "        self.dense5 = tf.keras.layers.Dense(10, activation='softmax')  \n",
    "        # 10\n",
    "\n",
    "    def call(self, x, training=False, mask=None):\n",
    "        h = self.flatten(x)\n",
    "        h = self.dense1(h)\n",
    "        h = self.dense2(h)\n",
    "        h = self.dense3(h)\n",
    "        h = self.dense4(h)\n",
    "        h = self.dropout4(h) # dense4 layer의 출력 중 반을 dropout한다\n",
    "        return self.dense5(h)\n",
    "\n",
    "history = model.fit(train_ds, validation_data=test_ds, epochs=EPOCHS, callbacks=[earlystopper])\n",
    "\n",
    "# visualization\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss, 'ro-')\n",
    "plt.plot(val_loss, 'bo-')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb987f75",
   "metadata": {},
   "source": [
    "# Special Topics in Data Science\n",
    "<a id='Special-Topics-in-Data-Science'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5212257f",
   "metadata": {},
   "source": [
    "## News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bef00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "searching_words = ['파이썬', '데이터공방', '코로나', '강의']\n",
    "for word in searching_words:\n",
    "    print(word)\n",
    "    url = f'https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=0&ie=utf8&query={word}'\n",
    "    print(url)\n",
    "\n",
    "from selenium import webdriver\n",
    "\n",
    "browser = webdriver.Chrome('./chromedriver')\n",
    "\n",
    "url = 'https://n.news.naver.com/article/277/0004834732'\n",
    "browser.get(url)\n",
    "\n",
    "#url에도 구조가 있다. \"주소\" 부분과 \"파라미터\" 부분.\n",
    "#예를 들어... 네이버에서 무언가를 검색할 때..\n",
    "\n",
    "#https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=0&ie=utf8&query=코로나\n",
    "# ? 기호 앞에가 주소이고, 뒤에가 파라미터다.\n",
    "# 파라미터에는..\n",
    "    # ~~?변수=값&변수=값&변수=값.... 이런 식의 진행.\n",
    "    # 저 위에서 query=코로나. query 변수의 값이 검색어라는 뜻\n",
    "\n",
    "# url의 html 정보 가져오기\n",
    "source = browser.page_source #지금 source은 string type\n",
    "\n",
    "from bs4 import BeautifulSoup as bs #source를 html을 읽을 수 있게 도와주는\n",
    "\n",
    "soup = bs(source, 'html.parser') #source를 html.parser에 정의된 기준으로 해석하라\n",
    "soup.select('h2') #soup 데이터 내에서 h3 태그 모두 찾기 => 리스트 형태로 나옴\n",
    "soup.select('.media_end_head_headline') # 마침표를 앞에 붙이면 \"클래스 속성 값\"을 찾는다\n",
    "soup.select('#dic_area') # 샾을 앞에 붙이면 \"id 속성 값\"을 찾는다\n",
    "                         # id 속성은 보통 유니크하게 쓰이기 때문에,\n",
    "                         # 크롤링하고자 하는 데이터에 id 속성이 있으면 '땡큐'하면 된다.\n",
    "\n",
    "soup.select('h2.media_end_head_headline') #태그 명은 h2이면서,\n",
    "                                          #클래스 속성은 media_end_head인 것 찾는다.\n",
    "    \n",
    "soup.select('div > h2') # div 태그를 부모로 갖고 있는 h2 태그 찾는다.\n",
    "soup.select('div.media_end_head_title > h2') # div 태그에 media_end_head_title이라는\n",
    "                                             # 클래스 속성을 부모로 갖는 h2 태그 찾는다.\n",
    "\n",
    "# 태그에서 값 추출하기\n",
    "title = soup.select('.media_end_head_headline')[0]\n",
    "    #soup.select는 항상 list 형태로 나오기 때문에 끝에 [0]을 넣는 것.\n",
    "\n",
    "title.text #앞뒤에 있는 태그 기호를 모두 없애준다.\n",
    "\n",
    "# 만약 text가 아니라 속성 값을 추출하고 싶다면.. ['속성명']\n",
    "title['class']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a2cd41",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4d9a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "import pandas\n",
    "\n",
    "\n",
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:69.0) Gecko/20100101 Firefox/69.0\"\n",
    "\n",
    "fs_list = [\"연결 재무상태표\", \"연결 손익계산서\", \"연결 포괄손익계산서\", \"연결 자본변동표\", \"연결 현금흐름표\", \"재무상태표\"]\n",
    "\n",
    "def download_excel(rcp_no, dcm_no, period, company):\n",
    "    url = \"http://dart.fss.or.kr/pdf/download/excel.do?rcp_no={}&dcm_no={}&lang=ko\".format(rcp_no, dcm_no)\n",
    "    resp = requests.get(url, headers={\"user-agent\": user_agent})\n",
    "    table = BytesIO(resp.content)\n",
    "    for sheet in fs_list:\n",
    "        data = pandas.read_excel(table, sheet_name = sheet, skiprows = 5)\n",
    "        data.to_csv(period + \"_\" + company + \"_\" + sheet + \".csv\", encoding = \"euc-kr\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253cb8f5",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Matplotlib-&amp;-Seaborn\" data-toc-modified-id=\"Matplotlib-&amp;-Seaborn-1\">Matplotlib &amp; Seaborn</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#기본-그래프\" data-toc-modified-id=\"기본-그래프-1.0.1\">기본 그래프</a></span></li><li><span><a href=\"#그래프-주석-추가\" data-toc-modified-id=\"그래프-주석-추가-1.0.2\">그래프 주석 추가</a></span></li><li><span><a href=\"#subplot으로-여러-그래프-출력\" data-toc-modified-id=\"subplot으로-여러-그래프-출력-1.0.3\">subplot으로 여러 그래프 출력</a></span></li><li><span><a href=\"#histogram\" data-toc-modified-id=\"histogram-1.0.4\">histogram</a></span></li></ul></li></ul></li><li><span><a href=\"#Examplars\" data-toc-modified-id=\"Examplars-2\">Examplars</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#이진범주-Target의-분포를-살핀다\" data-toc-modified-id=\"이진범주-Target의-분포를-살핀다-2.0.0.1\">이진범주 Target의 분포를 살핀다</a></span></li></ul></li><li><span><a href=\"#.plot.pie-&amp;-sns.countplot\" data-toc-modified-id=\".plot.pie-&amp;-sns.countplot-2.0.1\">.plot.pie &amp; sns.countplot</a></span></li><li><span><a href=\"#.plot.bar\" data-toc-modified-id=\".plot.bar-2.0.2\">.plot.bar</a></span></li><li><span><a href=\"#sns.factorplot\" data-toc-modified-id=\"sns.factorplot-2.0.3\">sns.factorplot</a></span></li><li><span><a href=\"#sns.kdeplot-&amp;-.plot(kde)\" data-toc-modified-id=\"sns.kdeplot-&amp;-.plot(kde)-2.0.4\">sns.kdeplot &amp; .plot(kde)</a></span></li><li><span><a href=\"#sns.violinplot\" data-toc-modified-id=\"sns.violinplot-2.0.5\">sns.violinplot</a></span></li><li><span><a href=\"#sns.distplot---skewness\" data-toc-modified-id=\"sns.distplot---skewness-2.0.6\">sns.distplot - skewness</a></span></li><li><span><a href=\"#sns.heatmap\" data-toc-modified-id=\"sns.heatmap-2.0.7\">sns.heatmap</a></span></li><li><span><a href=\"#sns.barplot\" data-toc-modified-id=\"sns.barplot-2.0.8\">sns.barplot</a></span></li><li><span><a href=\"#sns.FacetGrid-&amp;-.map\" data-toc-modified-id=\"sns.FacetGrid-&amp;-.map-2.0.9\">sns.FacetGrid &amp; .map</a></span></li><li><span><a href=\"#Two-y-axis\" data-toc-modified-id=\"Two-y-axis-2.0.10\">Two y-axis</a></span></li><li><span><a href=\"#sns.lmplot\" data-toc-modified-id=\"sns.lmplot-2.0.11\">sns.lmplot</a></span></li><li><span><a href=\"#Plot-Decision-Grid\" data-toc-modified-id=\"Plot-Decision-Grid-2.0.12\">Plot Decision Grid</a></span></li><li><span><a href=\"#Plot-Learning-Curve\" data-toc-modified-id=\"Plot-Learning-Curve-2.0.13\">Plot Learning Curve</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcf251f",
   "metadata": {},
   "source": [
    "# Matplotlib & Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6741d349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rc('font', family='AppleGothic')\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_context(\"notebook\", font_scale=1.5)\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ece98e",
   "metadata": {},
   "source": [
    "### 기본 그래프\n",
    "- plt.plot(x, y)  - 선 그래프\n",
    "- plt.scatter(x, y)  - 점 그래프\n",
    "- plt.hist(x, y)  - 히스토그램"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d51a265",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 10, 11)\n",
    "y = x ** 2 + x + 2 + np.random.randn(11)\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5557ddb0",
   "metadata": {},
   "source": [
    "### 그래프 주석 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b249bab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('X values')\n",
    "plt.ylabel('Y values')\n",
    "plt.title('X-Y relation')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.xlim(0, 20)\n",
    "plt.ylim(0, 200)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356f0c4e",
   "metadata": {},
   "source": [
    "### subplot으로 여러 그래프 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b390fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(x, y, 'r')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(x, y, 'g')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(y, x, 'k')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(x, np.exp(x), 'b')\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b0977f",
   "metadata": {},
   "source": [
    "### histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84c78ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = np.random.randint(1, 100, size=200)\n",
    "plt.hist(data, bins=20, alpha=0.3)\n",
    "plt.xlabel('value')\n",
    "plt.ylabel('number')\n",
    "plt.grid(True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fb5708",
   "metadata": {},
   "source": [
    "# Examplars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bf0e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../data/titanic/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaf0bce",
   "metadata": {},
   "source": [
    "#### 이진범주 Target의 분포를 살핀다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7ea8fa",
   "metadata": {},
   "source": [
    "### .plot.pie & sns.countplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6af93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "data['Survived'].value_counts().plot.pie(explode=[0, 0.1], autopct='%1.1f%%', ax=ax[0], shadow=True)\n",
    "    # explode 는 pie 차트의 fraction을 키워주는 역할\n",
    "    # autopct는 numerical value의 format\n",
    "ax[0].set_title('Pie plot - Survived')\n",
    "ax[0].set_ylabel('')\n",
    "\n",
    "sns.countplot('Survived', data=data, ax=ax[1])\n",
    "ax[1].set_title('Count plot - Survived')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faf94d6",
   "metadata": {},
   "source": [
    "### .plot.bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be5afab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['Pclass', 'Survived']].groupby(['Pclass'], as_index=True).mean().sort_values(by='Survived', ascending=False).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd14e4fb",
   "metadata": {},
   "source": [
    "### sns.factorplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4e16fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot('Pclass', 'Survived', hue='Sex',\n",
    "               data=data, size=6, aspect=1.5)\n",
    "\n",
    "sns.factorplot('Pclass', 'Survived', hue='Sex',\n",
    "               data=data, kind='bar')\n",
    "\n",
    "sns.factorplot(x='Sex', y='Survived', col='Pclass',\n",
    "               kind='bar', data=data)\n",
    "\n",
    "\n",
    "g = sns.factorplot(x='Sex', y='Age', data=data, kind='box')\n",
    "g = sns.factorplot(x='Sex', y='Age', hue='Pclass',\n",
    "                   data=data, kind='box')\n",
    "g = sns.factorplot(x='Parch', y='Age', data=data, kind='box')\n",
    "g = sns.factorplot(x='SibSp', y='Age', data=data, kind='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099c8e6f",
   "metadata": {},
   "source": [
    "### sns.kdeplot & .plot(kde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47312327",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "sns.kdeplot(data[data['Survived'] == 1]['Age'], ax=ax)\n",
    "sns.kdeplot(data[data['Survived'] == 0]['Age'], ax=ax)\n",
    "plt.legend(['Survived == 1', 'Survived == 0'])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "data['Age'][data['Pclass'] == 1].plot(kind='kde')\n",
    "data['Age'][data['Pclass'] == 2].plot(kind='kde')\n",
    "data['Age'][data['Pclass'] == 3].plot(kind='kde')\n",
    "\n",
    "plt.xlabel('Age')\n",
    "plt.title('Age Distribution within Classes')\n",
    "plt.legend(['1st Class', '2nd Class', '3rd Class'])\n",
    "\n",
    "# explore age distribution\n",
    "g = sns.kdeplot(data['Age'][(data['Survived']==0) &\n",
    "                             (data['Age'].notnull())],\n",
    "                color='Red', shade=True)\n",
    "g = sns.kdeplot(data['Age'][(data['Survived']==1) &\n",
    "                             (data['Age'].notnull())],\n",
    "                ax=g, color='Blue', shade=True)\n",
    "g.set_xlabel('Age')\n",
    "g.set_ylabel('Frequency')\n",
    "g.legend(['Not Survived', 'Survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b42624f",
   "metadata": {},
   "source": [
    "### sns.violinplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab61fe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 2, figsize=(18, 8))\n",
    "sns.violinplot('Pclass', 'Age', hue='Survived', data=data, scale='count', split=True, ax=ax[0])\n",
    "ax[0].set_title('Pclass and Age vs Survived')\n",
    "ax[0].set_yticks(range(0,110,10))\n",
    "sns.violinplot('Sex', 'Age', hue='Survived', data=data, scale='count', split=True, ax=ax[1])\n",
    "ax[1].set_title('Sex and Age vs Survived')\n",
    "ax[1].set_yticks(range(0,110,10))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b53e446",
   "metadata": {},
   "source": [
    "### sns.distplot - skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a6c2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "g = sns.distplot(data['Fare'], color='b', label='Skewness : {:.2f}'.format(data['Fare'].skew()), ax=ax)\n",
    "g = g.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745cde0a",
   "metadata": {},
   "source": [
    "### sns.heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf65945f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data.corr(), annot=True, cmap='RdYlGn', linewidths=0.2)\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(10,8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb89521e",
   "metadata": {},
   "source": [
    "### sns.barplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fecfb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g = sns.barplot('Age', 'Pclass', data=data, orient='h')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55660a25",
   "metadata": {},
   "source": [
    "### sns.FacetGrid & .map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c0039c",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(data, col='Survived', size=6)\n",
    "g = g.map(sns.distplot, 'Age')\n",
    "plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baa261e",
   "metadata": {},
   "source": [
    "### Two y-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7769920f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_url = 'http://bit.ly/2cLzoxH'\n",
    "gapminder = pd.read_csv(data_url)\n",
    "print(gapminder.head(3))\n",
    "\n",
    "gapminder_us = gapminder[gapminder.country=='United States']\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.plot(gapminder_us.year, gapminder_us.lifeExp, marker='o', color='red')\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('LifeExp', color='red')\n",
    "plt.grid(False)\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(gapminder_us.year, gapminder_us.gdpPercap, color='blue', marker='o')\n",
    "ax2.set_ylabel('gdpPercap', color='blue')\n",
    "\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaac1a08",
   "metadata": {},
   "source": [
    "### sns.lmplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29c52d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the example dataset for Anscombe's quartet\n",
    "df = sns.load_dataset(\"anscombe\")\n",
    "\n",
    "# Show the results of a linear regression within each dataset\n",
    "sns.lmplot(x=\"x\", y=\"y\", col=\"dataset\", hue=\"dataset\", data=df,\n",
    "           col_wrap=2, ci=None, palette=\"muted\", height=4,\n",
    "           scatter_kws={\"s\": 50, \"alpha\": 1})\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7942c45d",
   "metadata": {},
   "source": [
    "### Plot Decision Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1e25d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d44e03",
   "metadata": {},
   "source": [
    "### Plot Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5a7b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3995b8be",
   "metadata": {},
   "source": [
    "# Best Practices and Advanced Techniques\n",
    "<a id='Best-Practices-and-Advanced-Techniques'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c636b55",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "<a id='Conclusion'></a>"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
